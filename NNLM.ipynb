{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Copy of NNLM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4aQIHYYJVZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'brown_train.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq9WIOPwAnKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no228r1oJVZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "    \n",
        "def load(filepath, window_size, vocab_size=None):\n",
        "\n",
        "    words = []\n",
        "    with open(filepath, 'r', encoding='utf8') as file:\n",
        "        words = word_tokenize(file.readline())    \n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(len(words) - window_size + 1):\n",
        "        x_train.append(words[i: i + window_size - 1])\n",
        "        y_train.append(words[i +  window_size - 1])\n",
        "    \n",
        "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
        "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
        "    \n",
        "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
        "\n",
        "def load_zh(filepath, window_size, vocab_size=None):\n",
        "\n",
        "    words = []\n",
        "    with open(filepath, 'r', encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            words += word_tokenize(line.strip())\n",
        "        \n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(len(words) - window_size + 1):\n",
        "        x_train.append(words[i: i + window_size - 1])\n",
        "        y_train.append(words[i +  window_size - 1])\n",
        "    \n",
        "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
        "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
        "    \n",
        "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
        "            \n",
        "def convert_to_id(x_train, y_train, vocab):\n",
        "    \n",
        "    word_to_id = {}\n",
        "    for i, vocab in enumerate(vocab):\n",
        "        word_to_id[vocab] = i\n",
        "        \n",
        "    for i in range(len(x_train)):\n",
        "        x_train[i] = [word_to_id[word] for word in x_train[i]]\n",
        "        y_train[i] = word_to_id[y_train[i][0]]\n",
        "        \n",
        "    return x_train.astype(int), y_train.astype(int)\n",
        "\n",
        "\n",
        "def next_batch(x_train, y_train, batch_size):\n",
        "    \n",
        "    num_batch = len(x_train) // batch_size + 1\n",
        "    for n in range(num_batch):        \n",
        "        offset = n * batch_size\n",
        "        x_batch = x_train[offset: offset + batch_size]\n",
        "        y_batch = y_train[offset: offset + batch_size]\n",
        "        \n",
        "        yield x_batch, y_batch\n",
        "        \n",
        "# def convert_to_word(x_train, y_train, id_to_word):\n",
        "#     for i in range(len(x_train)):\n",
        "#         print(x_train[i])\n",
        "#         x_train[i] = id_to_word[x_train[i]]\n",
        "#         y_train[i] = id_to_word[y_train[i]]\n",
        "#     return x_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4WGvLpMJVZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameter\n",
        "# TODO: change to number of batches \n",
        "batch_size = 30\n",
        "# TODO: edit to be less hacky\n",
        "window_size = 6\n",
        "vocab_size = None\n",
        "hidden_size = 50\n",
        "emb_dim = 60\n",
        "learning_rate = 0.5\n",
        "epoch_size = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Had-f-TQJwvI",
        "colab_type": "code",
        "outputId": "08d96de6-2f2b-41df-cc2b-307299cba00d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weiDCYWiL2Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: split into train, valid, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEW5yqMxJVZu",
        "colab_type": "code",
        "outputId": "6e08f5ea-72eb-4aae-edcd-4d71d816dfbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_raw, y_raw, vocab, word2id = load_zh(filepath, window_size, vocab_size)\n",
        "vocab_size = len(vocab)\n",
        "print('vocab_size: {}'.format(vocab_size))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 52945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbWHEqDRJjr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_v = 'brown_valid.txt'\n",
        "x_raw_v, y_raw_v, vocab_v, word2id_v = load_zh(filepath_v, window_size, vocab_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqZ8qrbSJbon",
        "colab_type": "code",
        "outputId": "97799d2f-66ae-470c-a314-7ec061efde94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['<', '>', 'the', ..., 'lurched', 'muddied', 'dogtrot'],\n",
              "      dtype='<U38')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdynygmUJVZx",
        "colab_type": "code",
        "outputId": "211e4f5a-e4c8-4733-f10e-bc3f9c4a8135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# integer representations of vocab\n",
        "x_train, y_train = convert_to_id(x_raw, y_raw, vocab)\n",
        "print('Length: {}'.format(len(x_train)))\n",
        "print('Number of batch: {}'.format(len(x_train) / batch_size))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length: 1321189\n",
            "Number of batch: 44039.63333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfo81jKZJsSw",
        "colab_type": "code",
        "outputId": "99aad571-a5b6-45a1-e480-43a570b3ea63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_valid, y_valid = convert_to_id(x_raw_v, y_raw_v, vocab_v)\n",
        "print('Length: {}'.format(len(x_valid)))\n",
        "print('Number of batch: {}'.format(len(x_valid) / batch_size))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length: 175397\n",
            "Number of batch: 5846.566666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mP2MomyLbD0",
        "colab_type": "code",
        "outputId": "8afc9512-63cb-493b-874d-db398ca3eeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1321189, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfZ3NR_AJVZ0",
        "colab_type": "code",
        "outputId": "11bffb75-6309-4bb4-c9c7-68d49055e899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# import tensorflow as tf\n",
        "# %tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQkPaB6VJVZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Parameter Definition\n",
        "\n",
        "\n",
        "# Input && Output\n",
        "input_words = tf.placeholder(dtype=tf.int32, shape=(batch_size, window_size-1))\n",
        "output_word = tf.placeholder(dtype=tf.int32, shape=(batch_size, 1))\n",
        "\n",
        "\n",
        "# Word Features\n",
        "# word embedding matrix\n",
        "# truncated_normal randomly initializes a matrix of the given shape with values from the normal distribution\n",
        "C = tf.Variable(tf.truncated_normal(shape=(vocab_size, emb_dim), mean=-1, stddev=-1), name='word_embedding')\n",
        "\n",
        "\n",
        "# Hidden Layer Weight && Bias\n",
        "H = tf.Variable(tf.random_normal(shape=(hidden_size, (window_size - 1 ) * emb_dim)))\n",
        "d = tf.Variable(tf.random_normal(shape=(hidden_size, )))\n",
        "\n",
        "# Hidden-to-Output Weight && Bias\n",
        "U = tf.Variable(tf.random_normal(shape=(vocab_size, hidden_size)))\n",
        "b = tf.Variable(tf.random_normal(shape=(vocab_size, )))\n",
        "\n",
        "# Projection-to-Output Weight\n",
        "W = tf.Variable(tf.random_normal(shape=(vocab_size, (window_size - 1) * emb_dim)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSbKQ-12JVZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "64617f09-3b09-4a1a-b8fc-3a8583181ae7"
      },
      "source": [
        "# y = b + Wx + Utanh(d + Hx)\n",
        "\n",
        "# x = (C(w(t-1)), C(w(t-2), ..., C(w(t-n+1))), n == window_size\n",
        "with tf.name_scope('Projection_Layer'):\n",
        "  # get the actual embedding vectors from our batch inputs\n",
        "    x  = tf.nn.embedding_lookup(C, input_words) # (batch_size, window_size-1, emb_dim)\n",
        "    x  = tf.reshape(x, shape=(batch_size, (window_size - 1) * emb_dim))\n",
        "    \n",
        "with tf.name_scope('Hidden_Layer'):\n",
        "    Hx = tf.matmul(x, tf.transpose(H)) # (batch_size, hidden_size)\n",
        "    o  = tf.add(d, Hx) # (batch_size, hidden_size)\n",
        "    a  = tf.nn.tanh(o)  # (batch_size, hidden_size)\n",
        "     \n",
        "with tf.name_scope('Output_Layer'):\n",
        "    Ua = tf.matmul(a, tf.transpose(U)) # (batch_size, vocab_size)\n",
        "    Wx = tf.matmul(x, tf.transpose(W)) # (batch_size, vocab_size)\n",
        "    y  = tf.nn.softmax(tf.clip_by_value(tf.add(b, tf.add(Wx, Ua)), 0.0, 10)) # (batch_size, vocab_size)\n",
        "    #ppl = -1*tf.log(y)\n",
        "\n",
        "with tf.name_scope('Loss'):\n",
        "    onehot_tgt = tf.one_hot(tf.squeeze(output_word), vocab_size)  # (batch_size, vocab_size)\n",
        "    loss = -1 * tf.reduce_mean(tf.reduce_sum(tf.log(y) * onehot_tgt, 1)) # 乘 -1 -> maximize loss\n",
        "   \n",
        "with tf.name_scope('Perplexity'):\n",
        "    ppl = tf.math.exp(loss)\n",
        "    #print(ppl)\n",
        "    \n",
        "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss) \n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ0x6Zu7bYRe",
        "colab_type": "code",
        "outputId": "d185b37a-9808-4fcb-c992-dcf5c5e75d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "saver = tf.train.Saver()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMHizXWCJVZ-",
        "colab_type": "code",
        "outputId": "bde96b47-4c1c-406d-d28d-434065b7e1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)) as sess:\n",
        "    # initializes all of those variables we declared in earlier cells!\n",
        "    initializer = tf.global_variables_initializer()\n",
        "    initializer.run()\n",
        "    \n",
        "    step = 0\n",
        "    avg_loss_t = 0\n",
        "    avg_loss_v = 0\n",
        "    loss_t = []\n",
        "    loss_v = []\n",
        "    saver.restore(sess, \"/content/gdrive/My Drive/nnlm/final_model.ckpt\")\n",
        "    for epoch in range(epoch_size):\n",
        "        print('epoch no ', epoch)\n",
        "        save_path = saver.save(sess, \"/content/gdrive/My Drive/nnlm/final_model.ckpt\")\n",
        "\n",
        "        for x_batch, y_batch in next_batch(x_train, y_train, batch_size):\n",
        "            # if the batch is smaller than it's supposed to be (i.e. at end of vocab), skip it\n",
        "            # TODO: change this to account for num_batches, not batch_size\n",
        "\n",
        "            if len(x_batch) != batch_size:\n",
        "                continue\n",
        "            # give TF the data to use for all of the calcs in previous cells\n",
        "            feed_dict = {input_words: x_batch, output_word: y_batch}\n",
        "            # here we tell TF to return the loss to us \n",
        "            fetches = [loss, optimizer]\n",
        "            # where the magic happens \n",
        "            Loss, _ = sess.run(fetches, feed_dict)\n",
        "            avg_loss_t += Loss\n",
        "            #ppl = Perplexity\n",
        "            if step % 1000 == 0:\n",
        "                print('Step {}, Loss: {}'.format(step, avg_loss_t / 1000))\n",
        "                #print('Perplexity: {}'.format(ppl))\n",
        "\n",
        "                for valid_x, valid_y in next_batch(x_valid, y_valid, batch_size):\n",
        "                  if len(valid_x) != batch_size:\n",
        "                    continue\n",
        "                  feed_dict = {input_words: valid_x, output_word: valid_y}\n",
        "                  fetches = [loss, optimizer]\n",
        "                  Loss, _ = sess.run(fetches, feed_dict)\n",
        "                  avg_loss_v += Loss\n",
        "                print('Step {}, Loss: {}'.format(step, avg_loss_v / 1000))\n",
        "                loss_t.append(avg_loss_t)\n",
        "                loss_v.append(avg_loss_v)\n",
        "                avg_loss_t = 0\n",
        "                avg_loss_v = 0\n",
        "                \n",
        "            step += 1\n",
        "        \n",
        "    print('Training Done.')\n",
        "    word_embedding = C.eval()\n",
        "    # # TODO: this fails because it's a placeholder. Figure out how to visualize\n",
        "    # # y \n",
        "    # y_vals = y.eval()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/nnlm/final_model.ckpt\n",
            "epoch no  0\n",
            "Step 0, Loss: 0.009127057075500489\n",
            "Step 0, Loss: 16.13023182606697\n",
            "Step 1000, Loss: 6.744821178913116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEY_7Zk8SIxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: # y are our logits from the Bengio equation. those logits we must then convert to pseudo probabilities, then normalize via softmax to produce our y_hat.\n",
        "#       y_hat should be (vocabulary length) X (1) in size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt9utADwkSL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_t = np.array(loss_t)\n",
        "plt.plot(range(len(loss_t)), np.exp(loss_t/1000))\n",
        "loss_v = np.array(loss_v)\n",
        "plt.plot(range(len(loss_v)), np.exp(loss_v/1000))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeabkDBJiS3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}