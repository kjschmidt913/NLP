{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "NNLM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4aQIHYYJVZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'brown_tokenized.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no228r1oJVZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "    \n",
        "def load(filepath, window_size, vocab_size=None):\n",
        "\n",
        "    words = []\n",
        "    with open(filepath, 'r', encoding='utf8') as file:\n",
        "        words = word_tokenize(file.readline())    \n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(len(words) - window_size + 1):\n",
        "        x_train.append(words[i: i + window_size - 1])\n",
        "        y_train.append(words[i +  window_size - 1])\n",
        "    \n",
        "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
        "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
        "    \n",
        "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
        "\n",
        "def load_zh(filepath, window_size, vocab_size=None):\n",
        "\n",
        "    words = []\n",
        "    with open(filepath, 'r', encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            words += word_tokenize(line.strip())\n",
        "        \n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(len(words) - window_size + 1):\n",
        "        x_train.append(words[i: i + window_size - 1])\n",
        "        y_train.append(words[i +  window_size - 1])\n",
        "    \n",
        "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
        "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
        "    \n",
        "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
        "            \n",
        "def convert_to_id(x_train, y_train, vocab):\n",
        "    \n",
        "    word_to_id = {}\n",
        "    for i, vocab in enumerate(vocab):\n",
        "        word_to_id[vocab] = i\n",
        "        \n",
        "    for i in range(len(x_train)):\n",
        "        x_train[i] = [word_to_id[word] for word in x_train[i]]\n",
        "        y_train[i] = word_to_id[y_train[i][0]]\n",
        "        \n",
        "    return x_train.astype(int), y_train.astype(int)\n",
        "\n",
        "\n",
        "def next_batch(x_train, y_train, batch_size):\n",
        "    \n",
        "    num_batch = len(x_train) // batch_size + 1\n",
        "    for n in range(num_batch):        \n",
        "        offset = n * batch_size\n",
        "        x_batch = x_train[offset: offset + batch_size]\n",
        "        y_batch = y_train[offset: offset + batch_size]\n",
        "        \n",
        "        yield x_batch, y_batch\n",
        "        \n",
        "# def convert_to_word(x_train, y_train, id_to_word):\n",
        "#     for i in range(len(x_train)):\n",
        "#         print(x_train[i])\n",
        "#         x_train[i] = id_to_word[x_train[i]]\n",
        "#         y_train[i] = id_to_word[y_train[i]]\n",
        "#     return x_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4WGvLpMJVZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameter\n",
        "# TODO: change to number of batches \n",
        "batch_size = 30\n",
        "# TODO: edit to be less hacky\n",
        "window_size = 6\n",
        "vocab_size = None\n",
        "hidden_size = 50\n",
        "emb_dim = 60\n",
        "learning_rate = 0.8\n",
        "epoch_size = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Had-f-TQJwvI",
        "colab_type": "code",
        "outputId": "8729cbb6-2ff4-406a-8efd-e0b5a603de54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weiDCYWiL2Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: split into train, valid, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEW5yqMxJVZu",
        "colab_type": "code",
        "outputId": "77ed90c4-a44d-45c9-ca4f-bc4278ccdf6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_raw, y_raw, vocab, word2id = load_zh(filepath, window_size, vocab_size)\n",
        "vocab_size = len(vocab)\n",
        "print('vocab_size: {}'.format(vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 60500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqZ8qrbSJbon",
        "colab_type": "code",
        "outputId": "a954dcf6-5d54-4a71-9f84-1511fa0131fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['the', ',', '.', ..., 'Came', 'Williams-', '.m'], dtype='<U43')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdynygmUJVZx",
        "colab_type": "code",
        "outputId": "a3959a8f-9054-440f-a0f4-0b171ae951de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# integer representations of vocab\n",
        "x_train, y_train = convert_to_id(x_raw, y_raw, vocab)\n",
        "print('Length: {}'.format(len(x_train)))\n",
        "print('Number of batch: {}'.format(len(x_train) / batch_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length: 1365623\n",
            "Number of batch: 45520.76666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mP2MomyLbD0",
        "colab_type": "code",
        "outputId": "8c6059a7-88fb-45e4-a777-84c728d7b414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1365623, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfZ3NR_AJVZ0",
        "colab_type": "code",
        "outputId": "ebdb6890-9b7a-4bfc-956d-d471667dfa12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# import tensorflow as tf\n",
        "# %tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQkPaB6VJVZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Parameter Definition\n",
        "\n",
        "\n",
        "# Input && Output\n",
        "input_words = tf.placeholder(dtype=tf.int32, shape=(batch_size, window_size-1))\n",
        "output_word = tf.placeholder(dtype=tf.int32, shape=(batch_size, 1))\n",
        "\n",
        "\n",
        "# Word Features\n",
        "# word embedding matrix\n",
        "# truncated_normal randomly initializes a matrix of the given shape with values from the normal distribution\n",
        "C = tf.Variable(tf.truncated_normal(shape=(vocab_size, emb_dim), mean=-1, stddev=-1), name='word_embedding')\n",
        "\n",
        "\n",
        "# Hidden Layer Weight && Bias\n",
        "H = tf.Variable(tf.random_normal(shape=(hidden_size, (window_size - 1 ) * emb_dim)))\n",
        "d = tf.Variable(tf.random_normal(shape=(hidden_size, )))\n",
        "\n",
        "# Hidden-to-Output Weight && Bias\n",
        "U = tf.Variable(tf.random_normal(shape=(vocab_size, hidden_size)))\n",
        "b = tf.Variable(tf.random_normal(shape=(vocab_size, )))\n",
        "\n",
        "# Projection-to-Output Weight\n",
        "W = tf.Variable(tf.random_normal(shape=(vocab_size, (window_size - 1) * emb_dim)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSbKQ-12JVZ7",
        "colab_type": "code",
        "outputId": "a8257134-1763-4900-eeab-4ff949f2c5fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "# y = b + Wx + Utanh(d + Hx)\n",
        "\n",
        "# x = (C(w(t-1)), C(w(t-2), ..., C(w(t-n+1))), n == window_size\n",
        "with tf.name_scope('Projection_Layer'):\n",
        "  # get the actual embedding vectors from our batch inputs\n",
        "    x  = tf.nn.embedding_lookup(C, input_words) # (batch_size, window_size-1, emb_dim)\n",
        "    x  = tf.reshape(x, shape=(batch_size, (window_size - 1) * emb_dim))\n",
        "    \n",
        "with tf.name_scope('Hidden_Layer'):\n",
        "    Hx = tf.matmul(x, tf.transpose(H)) # (batch_size, hidden_size)\n",
        "    o  = tf.add(d, Hx) # (batch_size, hidden_size)\n",
        "    a  = tf.nn.tanh(o)  # (batch_size, hidden_size)\n",
        "     \n",
        "with tf.name_scope('Output_Layer'):\n",
        "    Ua = tf.matmul(a, tf.transpose(U)) # (batch_size, vocab_size)\n",
        "    Wx = tf.matmul(x, tf.transpose(W)) # (batch_size, vocab_size)\n",
        "    y  = tf.nn.softmax(tf.clip_by_value(tf.add(b, tf.add(Wx, Ua)), 0.0, 10)) # (batch_size, vocab_size)\n",
        "    \n",
        "\n",
        "with tf.name_scope('Loss'):\n",
        "    onehot_tgt = tf.one_hot(tf.squeeze(output_word), vocab_size)  # (batch_size, vocab_size)\n",
        "    loss = -1 * tf.reduce_mean(tf.reduce_sum(tf.log(y) * onehot_tgt, 1)) # ä¹˜ -1 -> maximize loss\n",
        "    print(loss)\n",
        "    \n",
        "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Loss/mul_1:0\", shape=(), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMHizXWCJVZ-",
        "colab_type": "code",
        "outputId": "88f5150a-744d-460e-ce1e-8d4a120d46c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)) as sess:\n",
        "    # initializes all of those variables we declared in earlier cells!\n",
        "    initializer = tf.global_variables_initializer()\n",
        "    initializer.run()\n",
        "    \n",
        "    step = 0\n",
        "    avg_loss = 0\n",
        "    for epoch in range(epoch_size):\n",
        "        print('epoch no ', epoch)\n",
        "        for x_batch, y_batch in next_batch(x_train, y_train, batch_size):\n",
        "            # if the batch is smaller than it's supposed to be (i.e. at end of vocab), skip it\n",
        "            # TODO: change this to account for num_batches, not batch_size\n",
        "            if len(x_batch) != batch_size:\n",
        "                continue\n",
        "            # give TF the data to use for all of the calcs in previous cells\n",
        "            feed_dict = {input_words: x_batch, output_word: y_batch}\n",
        "            # here we tell TF to return the loss to us \n",
        "            fetches = [loss, optimizer]\n",
        "            # where the magic happens \n",
        "            Loss, _ = sess.run(fetches, feed_dict)\n",
        "            avg_loss += Loss\n",
        "            if step % 100 == 0:\n",
        "                print('Step {}, Loss: {}'.format(step, avg_loss / 100))\n",
        "                avg_loss = 0\n",
        "            \n",
        "            step += 1\n",
        "    \n",
        "    print('Training Done.')\n",
        "    word_embedding = C.eval()\n",
        "    # # TODO: this fails because it's a placeholder. Figure out how to visualize\n",
        "    # # y \n",
        "    # y_vals = y.eval()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 446600, Loss: 9.860878763198853\n",
            "Step 446700, Loss: 9.922804136276245\n",
            "Step 446800, Loss: 9.75646375656128\n",
            "Step 446900, Loss: 9.66767149925232\n",
            "Step 447000, Loss: 9.429053297042847\n",
            "Step 447100, Loss: 9.678875703811645\n",
            "Step 447200, Loss: 9.968291902542115\n",
            "Step 447300, Loss: 9.729340867996216\n",
            "Step 447400, Loss: 9.610073804855347\n",
            "Step 447500, Loss: 9.842499752044677\n",
            "Step 447600, Loss: 9.767905788421631\n",
            "Step 447700, Loss: 9.533610849380493\n",
            "Step 447800, Loss: 9.617179107666015\n",
            "Step 447900, Loss: 9.670320348739624\n",
            "Step 448000, Loss: 9.586722431182862\n",
            "Step 448100, Loss: 9.787422122955322\n",
            "Step 448200, Loss: 9.83362551689148\n",
            "Step 448300, Loss: 9.67626202583313\n",
            "Step 448400, Loss: 9.85460705757141\n",
            "Step 448500, Loss: 9.754657487869263\n",
            "Step 448600, Loss: 9.554422235488891\n",
            "Step 448700, Loss: 9.674032402038574\n",
            "Step 448800, Loss: 9.545900869369508\n",
            "Step 448900, Loss: 9.475338230133056\n",
            "Step 449000, Loss: 9.688164825439452\n",
            "Step 449100, Loss: 9.663514165878295\n",
            "Step 449200, Loss: 9.700543899536132\n",
            "Step 449300, Loss: 9.466573333740234\n",
            "Step 449400, Loss: 9.499499464035035\n",
            "Step 449500, Loss: 9.446941804885864\n",
            "Step 449600, Loss: 9.51892297744751\n",
            "Step 449700, Loss: 9.587748079299926\n",
            "Step 449800, Loss: 9.638339023590088\n",
            "Step 449900, Loss: 9.589436960220336\n",
            "Step 450000, Loss: 10.000953855514526\n",
            "Step 450100, Loss: 9.871466588973998\n",
            "Step 450200, Loss: 9.69199462890625\n",
            "Step 450300, Loss: 9.33432186126709\n",
            "Step 450400, Loss: 9.474212942123414\n",
            "Step 450500, Loss: 9.67756238937378\n",
            "Step 450600, Loss: 9.741589689254761\n",
            "Step 450700, Loss: 9.880445804595947\n",
            "Step 450800, Loss: 9.843299112319947\n",
            "Step 450900, Loss: 9.837802019119263\n",
            "Step 451000, Loss: 10.098132066726684\n",
            "Step 451100, Loss: 10.133295640945434\n",
            "Step 451200, Loss: 9.685331239700318\n",
            "Step 451300, Loss: 9.586931447982789\n",
            "Step 451400, Loss: 9.813369092941285\n",
            "Step 451500, Loss: 9.67069875717163\n",
            "Step 451600, Loss: 9.537127084732056\n",
            "Step 451700, Loss: 9.582795114517213\n",
            "Step 451800, Loss: 9.587766637802124\n",
            "Step 451900, Loss: 9.654280910491943\n",
            "Step 452000, Loss: 9.742278671264648\n",
            "Step 452100, Loss: 9.610477991104126\n",
            "Step 452200, Loss: 9.712271223068237\n",
            "Step 452300, Loss: 9.577650451660157\n",
            "Step 452400, Loss: 9.815722007751464\n",
            "Step 452500, Loss: 9.70093653678894\n",
            "Step 452600, Loss: 9.830131149291992\n",
            "Step 452700, Loss: 9.8695858669281\n",
            "Step 452800, Loss: 10.150020771026611\n",
            "Step 452900, Loss: 9.515377340316773\n",
            "Step 453000, Loss: 9.850766763687133\n",
            "Step 453100, Loss: 9.64893991470337\n",
            "Step 453200, Loss: 9.61541893005371\n",
            "Step 453300, Loss: 9.60387583732605\n",
            "Step 453400, Loss: 9.345466175079345\n",
            "Step 453500, Loss: 9.866131477355957\n",
            "Step 453600, Loss: 9.670219116210937\n",
            "Step 453700, Loss: 9.736424493789674\n",
            "Step 453800, Loss: 9.443324089050293\n",
            "Step 453900, Loss: 9.858034658432008\n",
            "Step 454000, Loss: 9.70622757911682\n",
            "Step 454100, Loss: 9.937287483215332\n",
            "Step 454200, Loss: 9.697133083343505\n",
            "Step 454300, Loss: 9.69814317703247\n",
            "Step 454400, Loss: 9.974135522842408\n",
            "Step 454500, Loss: 9.590334959030152\n",
            "Step 454600, Loss: 9.86857624053955\n",
            "Step 454700, Loss: 10.14689281463623\n",
            "Step 454800, Loss: 10.06310396194458\n",
            "Step 454900, Loss: 9.96746187210083\n",
            "Step 455000, Loss: 10.041604776382446\n",
            "Step 455100, Loss: 9.79074688911438\n",
            "Step 455200, Loss: 10.08992645263672\n",
            "Step 455300, Loss: 9.926853723526001\n",
            "Step 455400, Loss: 10.534635791778564\n",
            "Step 455500, Loss: 10.348243198394776\n",
            "Step 455600, Loss: 10.184210357666016\n",
            "Step 455700, Loss: 9.522710514068603\n",
            "Step 455800, Loss: 9.54984718322754\n",
            "Step 455900, Loss: 9.613957567214966\n",
            "Step 456000, Loss: 9.435300159454346\n",
            "Step 456100, Loss: 9.744677963256835\n",
            "Step 456200, Loss: 9.7828164768219\n",
            "Step 456300, Loss: 9.796285638809204\n",
            "Step 456400, Loss: 9.471402559280396\n",
            "Step 456500, Loss: 9.644841632843018\n",
            "Step 456600, Loss: 9.542585678100586\n",
            "Step 456700, Loss: 9.666376752853393\n",
            "Step 456800, Loss: 9.277194957733155\n",
            "Step 456900, Loss: 9.568730516433716\n",
            "Step 457000, Loss: 9.82545392036438\n",
            "Step 457100, Loss: 9.668464670181274\n",
            "Step 457200, Loss: 9.518262939453125\n",
            "Step 457300, Loss: 9.622910699844361\n",
            "Step 457400, Loss: 9.445945920944213\n",
            "Step 457500, Loss: 9.544232416152955\n",
            "Step 457600, Loss: 9.730021305084229\n",
            "Step 457700, Loss: 9.727197065353394\n",
            "Step 457800, Loss: 9.767632312774658\n",
            "Step 457900, Loss: 9.510702056884766\n",
            "Step 458000, Loss: 9.509800519943237\n",
            "Step 458100, Loss: 9.498756942749024\n",
            "Step 458200, Loss: 9.754532871246338\n",
            "Step 458300, Loss: 9.353535614013673\n",
            "Step 458400, Loss: 9.55585355758667\n",
            "Step 458500, Loss: 9.988013429641724\n",
            "Step 458600, Loss: 9.998368453979491\n",
            "Step 458700, Loss: 9.780550374984742\n",
            "Step 458800, Loss: 9.81863733291626\n",
            "Step 458900, Loss: 9.851503992080689\n",
            "Step 459000, Loss: 9.725580158233642\n",
            "Step 459100, Loss: 9.568582153320312\n",
            "Step 459200, Loss: 9.998833932876586\n",
            "Step 459300, Loss: 9.670440044403076\n",
            "Step 459400, Loss: 10.142621030807495\n",
            "Step 459500, Loss: 10.115216856002808\n",
            "Step 459600, Loss: 10.160183010101319\n",
            "Step 459700, Loss: 10.083383359909057\n",
            "Step 459800, Loss: 10.199857063293457\n",
            "Step 459900, Loss: 10.166496629714965\n",
            "Step 460000, Loss: 10.023394203186035\n",
            "Step 460100, Loss: 9.897233772277833\n",
            "Step 460200, Loss: 10.53453556060791\n",
            "Step 460300, Loss: 10.070359544754028\n",
            "Step 460400, Loss: 9.574372806549071\n",
            "Step 460500, Loss: 9.851886234283448\n",
            "Step 460600, Loss: 9.678518562316894\n",
            "Step 460700, Loss: 9.688063297271729\n",
            "Step 460800, Loss: 9.584320697784424\n",
            "Step 460900, Loss: 9.868509140014648\n",
            "Step 461000, Loss: 9.980717267990112\n",
            "Step 461100, Loss: 9.631770334243775\n",
            "Step 461200, Loss: 9.739597263336181\n",
            "Step 461300, Loss: 9.972986211776734\n",
            "Step 461400, Loss: 10.066890325546265\n",
            "Step 461500, Loss: 9.694382677078247\n",
            "Step 461600, Loss: 9.77927363395691\n",
            "Step 461700, Loss: 9.975019283294678\n",
            "Step 461800, Loss: 9.9961119556427\n",
            "Step 461900, Loss: 9.515969371795654\n",
            "Step 462000, Loss: 9.651664333343506\n",
            "Step 462100, Loss: 9.877545042037964\n",
            "Step 462200, Loss: 9.690024318695068\n",
            "Step 462300, Loss: 9.739747076034545\n",
            "Step 462400, Loss: 9.483302984237671\n",
            "Step 462500, Loss: 9.560881900787354\n",
            "Step 462600, Loss: 9.582394332885743\n",
            "Step 462700, Loss: 9.491348905563354\n",
            "Step 462800, Loss: 9.542389469146729\n",
            "Step 462900, Loss: 9.360825986862183\n",
            "Step 463000, Loss: 9.506316070556641\n",
            "Step 463100, Loss: 9.691724491119384\n",
            "Step 463200, Loss: 9.622167491912842\n",
            "Step 463300, Loss: 9.548776483535766\n",
            "Step 463400, Loss: 9.68368037223816\n",
            "Step 463500, Loss: 9.50788007736206\n",
            "Step 463600, Loss: 9.46982395172119\n",
            "Step 463700, Loss: 9.727178869247437\n",
            "Step 463800, Loss: 9.578694400787354\n",
            "Step 463900, Loss: 9.828747901916504\n",
            "Step 464000, Loss: 9.65725040435791\n",
            "Step 464100, Loss: 9.781789360046387\n",
            "Step 464200, Loss: 9.405380821228027\n",
            "Step 464300, Loss: 9.87986053466797\n",
            "Step 464400, Loss: 9.628194389343262\n",
            "Step 464500, Loss: 9.767272720336914\n",
            "Step 464600, Loss: 9.613766927719116\n",
            "Step 464700, Loss: 9.477618627548217\n",
            "Step 464800, Loss: 9.778043174743653\n",
            "Step 464900, Loss: 9.633927421569824\n",
            "Step 465000, Loss: 9.72716856956482\n",
            "Step 465100, Loss: 9.81788332939148\n",
            "Step 465200, Loss: 9.516261730194092\n",
            "Step 465300, Loss: 9.381369962692261\n",
            "Step 465400, Loss: 9.735769243240357\n",
            "Step 465500, Loss: 9.844919958114623\n",
            "Step 465600, Loss: 9.351608276367188\n",
            "Step 465700, Loss: 9.59631552696228\n",
            "Step 465800, Loss: 10.009752292633056\n",
            "Step 465900, Loss: 9.781505556106568\n",
            "Step 466000, Loss: 9.64375638961792\n",
            "Step 466100, Loss: 9.750352201461792\n",
            "Step 466200, Loss: 9.80442120552063\n",
            "Step 466300, Loss: 9.988551206588745\n",
            "Step 466400, Loss: 9.906741590499879\n",
            "Step 466500, Loss: 9.90886350631714\n",
            "Step 466600, Loss: 9.443374557495117\n",
            "Step 466700, Loss: 9.546277828216553\n",
            "Step 466800, Loss: 9.95541434288025\n",
            "Step 466900, Loss: 9.85385666847229\n",
            "Step 467000, Loss: 9.720747804641723\n",
            "Step 467100, Loss: 9.685238285064697\n",
            "Step 467200, Loss: 10.01996989250183\n",
            "Step 467300, Loss: 9.805151710510254\n",
            "Step 467400, Loss: 9.979605159759522\n",
            "Step 467500, Loss: 10.03052412033081\n",
            "Step 467600, Loss: 9.814536762237548\n",
            "Step 467700, Loss: 9.627940301895142\n",
            "Step 467800, Loss: 9.869999074935913\n",
            "Step 467900, Loss: 9.742127170562744\n",
            "Step 468000, Loss: 9.713542585372926\n",
            "Step 468100, Loss: 9.682746486663818\n",
            "Step 468200, Loss: 9.373446054458618\n",
            "Step 468300, Loss: 9.604267339706421\n",
            "Step 468400, Loss: 9.364724388122559\n",
            "Step 468500, Loss: 9.400865249633789\n",
            "Step 468600, Loss: 9.735235061645508\n",
            "Step 468700, Loss: 9.477941761016845\n",
            "Step 468800, Loss: 9.852023572921754\n",
            "Step 468900, Loss: 9.547095651626586\n",
            "Step 469000, Loss: 9.38468560218811\n",
            "Step 469100, Loss: 9.475814895629883\n",
            "Step 469200, Loss: 9.736609601974488\n",
            "Step 469300, Loss: 9.447700481414795\n",
            "Step 469400, Loss: 9.854421501159669\n",
            "Step 469500, Loss: 9.409028759002686\n",
            "Step 469600, Loss: 9.473261289596557\n",
            "Step 469700, Loss: 9.524636430740356\n",
            "Step 469800, Loss: 9.61593373298645\n",
            "Step 469900, Loss: 9.85641167640686\n",
            "Step 470000, Loss: 9.751697006225585\n",
            "Step 470100, Loss: 10.010948648452759\n",
            "Step 470200, Loss: 9.614936962127686\n",
            "Step 470300, Loss: 9.770802145004273\n",
            "Step 470400, Loss: 9.739807767868042\n",
            "Step 470500, Loss: 10.145876483917236\n",
            "Step 470600, Loss: 10.421176681518554\n",
            "epoch no  12\n",
            "Step 470700, Loss: 9.734594278335571\n",
            "Step 470800, Loss: 9.854687213897705\n",
            "Step 470900, Loss: 9.689263505935669\n",
            "Step 471000, Loss: 9.508412828445435\n",
            "Step 471100, Loss: 9.745946264266967\n",
            "Step 471200, Loss: 9.735164394378662\n",
            "Step 471300, Loss: 9.858441514968872\n",
            "Step 471400, Loss: 9.846954851150512\n",
            "Step 471500, Loss: 10.005092506408692\n",
            "Step 471600, Loss: 10.170772943496704\n",
            "Step 471700, Loss: 10.006834306716918\n",
            "Step 471800, Loss: 9.892466983795167\n",
            "Step 471900, Loss: 10.063549175262452\n",
            "Step 472000, Loss: 10.082405443191528\n",
            "Step 472100, Loss: 9.95820276260376\n",
            "Step 472200, Loss: 9.918141794204711\n",
            "Step 472300, Loss: 9.843687191009522\n",
            "Step 472400, Loss: 9.927124843597412\n",
            "Step 472500, Loss: 10.121927347183227\n",
            "Step 472600, Loss: 9.776232919692994\n",
            "Step 472700, Loss: 9.915639247894287\n",
            "Step 472800, Loss: 9.667864561080933\n",
            "Step 472900, Loss: 10.306909265518188\n",
            "Step 473000, Loss: 10.048022947311402\n",
            "Step 473100, Loss: 9.882462482452393\n",
            "Step 473200, Loss: 9.803102617263795\n",
            "Step 473300, Loss: 9.466248254776001\n",
            "Step 473400, Loss: 9.57792737007141\n",
            "Step 473500, Loss: 9.840164623260499\n",
            "Step 473600, Loss: 9.830625762939453\n",
            "Step 473700, Loss: 9.71414493560791\n",
            "Step 473800, Loss: 9.807006549835204\n",
            "Step 473900, Loss: 9.826473340988159\n",
            "Step 474000, Loss: 9.592399587631226\n",
            "Step 474100, Loss: 9.702718315124512\n",
            "Step 474200, Loss: 9.799612340927125\n",
            "Step 474300, Loss: 9.68191291809082\n",
            "Step 474400, Loss: 9.697093572616577\n",
            "Step 474500, Loss: 9.761492509841919\n",
            "Step 474600, Loss: 9.609503717422486\n",
            "Step 474700, Loss: 9.524913902282714\n",
            "Step 474800, Loss: 9.308927745819092\n",
            "Step 474900, Loss: 9.844553890228271\n",
            "Step 475000, Loss: 9.887957305908204\n",
            "Step 475100, Loss: 9.743336515426636\n",
            "Step 475200, Loss: 9.435813102722168\n",
            "Step 475300, Loss: 9.559681043624877\n",
            "Step 475400, Loss: 9.724803733825684\n",
            "Step 475500, Loss: 9.812651109695434\n",
            "Step 475600, Loss: 9.665595788955688\n",
            "Step 475700, Loss: 9.621820316314697\n",
            "Step 475800, Loss: 9.567554655075073\n",
            "Step 475900, Loss: 9.967899494171142\n",
            "Step 476000, Loss: 9.595356245040893\n",
            "Step 476100, Loss: 9.712025785446167\n",
            "Step 476200, Loss: 9.827020196914672\n",
            "Step 476300, Loss: 9.965821132659912\n",
            "Step 476400, Loss: 9.86813286781311\n",
            "Step 476500, Loss: 9.914014453887939\n",
            "Step 476600, Loss: 9.814073543548584\n",
            "Step 476700, Loss: 10.150718097686768\n",
            "Step 476800, Loss: 9.972400760650634\n",
            "Step 476900, Loss: 9.901069993972778\n",
            "Step 477000, Loss: 10.120382118225098\n",
            "Step 477100, Loss: 9.691986026763916\n",
            "Step 477200, Loss: 9.823591032028197\n",
            "Step 477300, Loss: 10.097083215713502\n",
            "Step 477400, Loss: 10.26446961402893\n",
            "Step 477500, Loss: 9.924859657287598\n",
            "Step 477600, Loss: 9.56429482460022\n",
            "Step 477700, Loss: 9.535609674453735\n",
            "Step 477800, Loss: 9.526558923721314\n",
            "Step 477900, Loss: 9.794567537307739\n",
            "Step 478000, Loss: 9.453625802993775\n",
            "Step 478100, Loss: 9.413952922821045\n",
            "Step 478200, Loss: 9.683171005249024\n",
            "Step 478300, Loss: 9.652708587646485\n",
            "Step 478400, Loss: 9.456484212875367\n",
            "Step 478500, Loss: 9.441002225875854\n",
            "Step 478600, Loss: 9.69482666015625\n",
            "Step 478700, Loss: 9.899577083587646\n",
            "Step 478800, Loss: 9.650766553878784\n",
            "Step 478900, Loss: 9.676944160461426\n",
            "Step 479000, Loss: 9.709364767074584\n",
            "Step 479100, Loss: 9.83603045463562\n",
            "Step 479200, Loss: 9.79429880142212\n",
            "Step 479300, Loss: 9.717937784194946\n",
            "Step 479400, Loss: 9.580923595428466\n",
            "Step 479500, Loss: 10.037009229660034\n",
            "Step 479600, Loss: 10.23286003112793\n",
            "Step 479700, Loss: 10.253468198776245\n",
            "Step 479800, Loss: 9.995027923583985\n",
            "Step 479900, Loss: 10.050257978439332\n",
            "Step 480000, Loss: 10.349936866760254\n",
            "Step 480100, Loss: 9.979333658218383\n",
            "Step 480200, Loss: 9.824665412902831\n",
            "Step 480300, Loss: 9.768951511383056\n",
            "Step 480400, Loss: 9.601042709350587\n",
            "Step 480500, Loss: 9.609093189239502\n",
            "Step 480600, Loss: 9.80691876411438\n",
            "Step 480700, Loss: 9.541588697433472\n",
            "Step 480800, Loss: 9.862860040664673\n",
            "Step 480900, Loss: 10.082359409332275\n",
            "Step 481000, Loss: 9.846210241317749\n",
            "Step 481100, Loss: 9.519786224365234\n",
            "Step 481200, Loss: 9.798885688781738\n",
            "Step 481300, Loss: 9.51695707321167\n",
            "Step 481400, Loss: 9.709877529144286\n",
            "Step 481500, Loss: 10.036875476837158\n",
            "Step 481600, Loss: 9.617218008041382\n",
            "Step 481700, Loss: 9.72153733253479\n",
            "Step 481800, Loss: 9.737424812316895\n",
            "Step 481900, Loss: 9.854394445419311\n",
            "Step 482000, Loss: 9.820161809921265\n",
            "Step 482100, Loss: 9.497298755645751\n",
            "Step 482200, Loss: 9.697544555664063\n",
            "Step 482300, Loss: 9.650235652923584\n",
            "Step 482400, Loss: 9.856934595108033\n",
            "Step 482500, Loss: 9.564551229476928\n",
            "Step 482600, Loss: 9.406213207244873\n",
            "Step 482700, Loss: 9.670962123870849\n",
            "Step 482800, Loss: 9.620118741989137\n",
            "Step 482900, Loss: 9.896457586288452\n",
            "Step 483000, Loss: 9.852838134765625\n",
            "Step 483100, Loss: 9.807987880706786\n",
            "Step 483200, Loss: 9.999542512893676\n",
            "Step 483300, Loss: 9.585333013534546\n",
            "Step 483400, Loss: 9.598774118423462\n",
            "Step 483500, Loss: 9.85638825416565\n",
            "Step 483600, Loss: 9.948109760284424\n",
            "Step 483700, Loss: 9.748577642440797\n",
            "Step 483800, Loss: 9.676077280044556\n",
            "Step 483900, Loss: 9.92142255783081\n",
            "Step 484000, Loss: 9.491945686340332\n",
            "Step 484100, Loss: 9.39595332145691\n",
            "Step 484200, Loss: 9.744525175094605\n",
            "Step 484300, Loss: 9.957498664855956\n",
            "Step 484400, Loss: 9.570642642974853\n",
            "Step 484500, Loss: 9.519303255081176\n",
            "Step 484600, Loss: 9.699815168380738\n",
            "Step 484700, Loss: 9.70751898765564\n",
            "Step 484800, Loss: 9.87704755783081\n",
            "Step 484900, Loss: 9.547923049926759\n",
            "Step 485000, Loss: 9.655613603591918\n",
            "Step 485100, Loss: 9.694729232788086\n",
            "Step 485200, Loss: 9.736120319366455\n",
            "Step 485300, Loss: 9.450181245803833\n",
            "Step 485400, Loss: 9.577987184524536\n",
            "Step 485500, Loss: 9.703168640136719\n",
            "Step 485600, Loss: 9.76968855857849\n",
            "Step 485700, Loss: 9.788805904388427\n",
            "Step 485800, Loss: 9.88090908050537\n",
            "Step 485900, Loss: 9.842260646820069\n",
            "Step 486000, Loss: 9.792588500976562\n",
            "Step 486100, Loss: 9.623888273239135\n",
            "Step 486200, Loss: 9.41832709312439\n",
            "Step 486300, Loss: 9.61471209526062\n",
            "Step 486400, Loss: 9.898010635375977\n",
            "Step 486500, Loss: 9.711400947570802\n",
            "Step 486600, Loss: 9.599005060195923\n",
            "Step 486700, Loss: 9.71517819404602\n",
            "Step 486800, Loss: 9.756042375564576\n",
            "Step 486900, Loss: 9.430866298675538\n",
            "Step 487000, Loss: 9.672509212493896\n",
            "Step 487100, Loss: 9.625819778442382\n",
            "Step 487200, Loss: 9.491601667404176\n",
            "Step 487300, Loss: 9.687795162200928\n",
            "Step 487400, Loss: 9.792122011184693\n",
            "Step 487500, Loss: 9.670299396514892\n",
            "Step 487600, Loss: 9.768596363067626\n",
            "Step 487700, Loss: 9.784317836761474\n",
            "Step 487800, Loss: 9.533411960601807\n",
            "Step 487900, Loss: 9.630712728500367\n",
            "Step 488000, Loss: 9.486657018661498\n",
            "Step 488100, Loss: 9.426520566940308\n",
            "Step 488200, Loss: 9.651979122161865\n",
            "Step 488300, Loss: 9.598918714523315\n",
            "Step 488400, Loss: 9.744778308868408\n",
            "Step 488500, Loss: 9.42067603111267\n",
            "Step 488600, Loss: 9.529187126159668\n",
            "Step 488700, Loss: 9.433515338897704\n",
            "Step 488800, Loss: 9.420759782791137\n",
            "Step 488900, Loss: 9.55946346282959\n",
            "Step 489000, Loss: 9.54460078239441\n",
            "Step 489100, Loss: 9.562620077133179\n",
            "Step 489200, Loss: 9.935042963027954\n",
            "Step 489300, Loss: 9.873896408081055\n",
            "Step 489400, Loss: 9.571711387634277\n",
            "Step 489500, Loss: 9.398053331375122\n",
            "Step 489600, Loss: 9.424448356628417\n",
            "Step 489700, Loss: 9.519519290924073\n",
            "Step 489800, Loss: 9.68633755683899\n",
            "Step 489900, Loss: 9.901399812698365\n",
            "Step 490000, Loss: 9.679552793502808\n",
            "Step 490100, Loss: 9.812905836105347\n",
            "Step 490200, Loss: 9.99304672241211\n",
            "Step 490300, Loss: 10.185621757507324\n",
            "Step 490400, Loss: 9.68822364807129\n",
            "Step 490500, Loss: 9.507705812454224\n",
            "Step 490600, Loss: 9.75510287284851\n",
            "Step 490700, Loss: 9.638643026351929\n",
            "Step 490800, Loss: 9.526932182312011\n",
            "Step 490900, Loss: 9.515668563842773\n",
            "Step 491000, Loss: 9.49000247001648\n",
            "Step 491100, Loss: 9.711819839477538\n",
            "Step 491200, Loss: 9.58829358100891\n",
            "Step 491300, Loss: 9.668892879486084\n",
            "Step 491400, Loss: 9.613378887176514\n",
            "Step 491500, Loss: 9.597595901489258\n",
            "Step 491600, Loss: 9.721064510345458\n",
            "Step 491700, Loss: 9.627371139526367\n",
            "Step 491800, Loss: 9.818249502182006\n",
            "Step 491900, Loss: 9.772414789199829\n",
            "Step 492000, Loss: 10.146611680984497\n",
            "Step 492100, Loss: 9.508011741638184\n",
            "Step 492200, Loss: 9.741647787094116\n",
            "Step 492300, Loss: 9.664658412933349\n",
            "Step 492400, Loss: 9.576373987197876\n",
            "Step 492500, Loss: 9.613687582015991\n",
            "Step 492600, Loss: 9.278335552215577\n",
            "Step 492700, Loss: 9.701482973098756\n",
            "Step 492800, Loss: 9.691247119903565\n",
            "Step 492900, Loss: 9.703338775634766\n",
            "Step 493000, Loss: 9.388597354888915\n",
            "Step 493100, Loss: 9.902792644500732\n",
            "Step 493200, Loss: 9.551988639831542\n",
            "Step 493300, Loss: 9.816400661468506\n",
            "Step 493400, Loss: 9.771570978164673\n",
            "Step 493500, Loss: 9.639483613967895\n",
            "Step 493600, Loss: 9.881495122909547\n",
            "Step 493700, Loss: 9.614584856033325\n",
            "Step 493800, Loss: 9.780825567245483\n",
            "Step 493900, Loss: 10.06030574798584\n",
            "Step 494000, Loss: 10.103597593307494\n",
            "Step 494100, Loss: 9.884187965393066\n",
            "Step 494200, Loss: 9.986215047836303\n",
            "Step 494300, Loss: 9.7832772731781\n",
            "Step 494400, Loss: 10.014562664031983\n",
            "Step 494500, Loss: 9.749225912094117\n",
            "Step 494600, Loss: 10.471341466903686\n",
            "Step 494700, Loss: 10.386401090621948\n",
            "Step 494800, Loss: 10.228982772827148\n",
            "Step 494900, Loss: 9.434464454650879\n",
            "Step 495000, Loss: 9.477885255813598\n",
            "Step 495100, Loss: 9.684816217422485\n",
            "Step 495200, Loss: 9.390788793563843\n",
            "Step 495300, Loss: 9.608698854446411\n",
            "Step 495400, Loss: 9.795951204299927\n",
            "Step 495500, Loss: 9.751675481796264\n",
            "Step 495600, Loss: 9.341096429824828\n",
            "Step 495700, Loss: 9.571906871795655\n",
            "Step 495800, Loss: 9.57394630432129\n",
            "Step 495900, Loss: 9.608045873641968\n",
            "Step 496000, Loss: 9.313985376358032\n",
            "Step 496100, Loss: 9.407320337295532\n",
            "Step 496200, Loss: 9.71429202079773\n",
            "Step 496300, Loss: 9.719096784591676\n",
            "Step 496400, Loss: 9.491599245071411\n",
            "Step 496500, Loss: 9.54282169342041\n",
            "Step 496600, Loss: 9.418963708877563\n",
            "Step 496700, Loss: 9.488887538909912\n",
            "Step 496800, Loss: 9.663690547943116\n",
            "Step 496900, Loss: 9.758957605361939\n",
            "Step 497000, Loss: 9.705638341903686\n",
            "Step 497100, Loss: 9.476238756179809\n",
            "Step 497200, Loss: 9.560731582641601\n",
            "Step 497300, Loss: 9.479442205429077\n",
            "Step 497400, Loss: 9.6367689037323\n",
            "Step 497500, Loss: 9.334523363113403\n",
            "Step 497600, Loss: 9.501868324279785\n",
            "Step 497700, Loss: 9.939265766143798\n",
            "Step 497800, Loss: 9.911738529205323\n",
            "Step 497900, Loss: 9.74449052810669\n",
            "Step 498000, Loss: 9.748589963912965\n",
            "Step 498100, Loss: 9.787652025222778\n",
            "Step 498200, Loss: 9.725402431488037\n",
            "Step 498300, Loss: 9.550659914016723\n",
            "Step 498400, Loss: 9.862646923065185\n",
            "Step 498500, Loss: 9.658510618209839\n",
            "Step 498600, Loss: 10.061855630874634\n",
            "Step 498700, Loss: 10.050513763427734\n",
            "Step 498800, Loss: 10.117461719512939\n",
            "Step 498900, Loss: 10.175138444900513\n",
            "Step 499000, Loss: 10.00582160949707\n",
            "Step 499100, Loss: 10.254504842758179\n",
            "Step 499200, Loss: 9.99995388031006\n",
            "Step 499300, Loss: 9.70768922805786\n",
            "Step 499400, Loss: 10.41364179611206\n",
            "Step 499500, Loss: 10.112357387542724\n",
            "Step 499600, Loss: 9.584711198806763\n",
            "Step 499700, Loss: 9.889622974395753\n",
            "Step 499800, Loss: 9.54062879562378\n",
            "Step 499900, Loss: 9.758817825317383\n",
            "Step 500000, Loss: 9.508144283294678\n",
            "Step 500100, Loss: 9.742884111404418\n",
            "Step 500200, Loss: 9.963096475601196\n",
            "Step 500300, Loss: 9.705653486251832\n",
            "Step 500400, Loss: 9.591786117553712\n",
            "Step 500500, Loss: 9.928055686950684\n",
            "Step 500600, Loss: 9.990401554107667\n",
            "Step 500700, Loss: 9.766911125183105\n",
            "Step 500800, Loss: 9.663544836044311\n",
            "Step 500900, Loss: 9.79997371673584\n",
            "Step 501000, Loss: 10.10182053565979\n",
            "Step 501100, Loss: 9.463818845748902\n",
            "Step 501200, Loss: 9.447655506134033\n",
            "Step 501300, Loss: 9.850583696365357\n",
            "Step 501400, Loss: 9.78390640258789\n",
            "Step 501500, Loss: 9.662180557250977\n",
            "Step 501600, Loss: 9.432703189849853\n",
            "Step 501700, Loss: 9.512839603424073\n",
            "Step 501800, Loss: 9.61390640258789\n",
            "Step 501900, Loss: 9.426812705993653\n",
            "Step 502000, Loss: 9.583691215515136\n",
            "Step 502100, Loss: 9.283965244293213\n",
            "Step 502200, Loss: 9.433392019271851\n",
            "Step 502300, Loss: 9.583188734054566\n",
            "Step 502400, Loss: 9.584230728149414\n",
            "Step 502500, Loss: 9.547100820541381\n",
            "Step 502600, Loss: 9.589100551605224\n",
            "Step 502700, Loss: 9.510440521240234\n",
            "Step 502800, Loss: 9.423143014907836\n",
            "Step 502900, Loss: 9.641931829452515\n",
            "Step 503000, Loss: 9.548753519058227\n",
            "Step 503100, Loss: 9.73370530128479\n",
            "Step 503200, Loss: 9.648775253295899\n",
            "Step 503300, Loss: 9.6734414768219\n",
            "Step 503400, Loss: 9.5124556350708\n",
            "Step 503500, Loss: 9.684378538131714\n",
            "Step 503600, Loss: 9.541590433120728\n",
            "Step 503700, Loss: 9.765713043212891\n",
            "Step 503800, Loss: 9.592593631744386\n",
            "Step 503900, Loss: 9.365824642181396\n",
            "Step 504000, Loss: 9.779367246627807\n",
            "Step 504100, Loss: 9.61134656906128\n",
            "Step 504200, Loss: 9.615854015350342\n",
            "Step 504300, Loss: 9.823150434494018\n",
            "Step 504400, Loss: 9.486609525680542\n",
            "Step 504500, Loss: 9.298039608001709\n",
            "Step 504600, Loss: 9.633429651260377\n",
            "Step 504700, Loss: 9.791602392196655\n",
            "Step 504800, Loss: 9.354146614074708\n",
            "Step 504900, Loss: 9.570185813903809\n",
            "Step 505000, Loss: 9.87517409324646\n",
            "Step 505100, Loss: 9.765864877700805\n",
            "Step 505200, Loss: 9.656718444824218\n",
            "Step 505300, Loss: 9.738144521713258\n",
            "Step 505400, Loss: 9.709531469345093\n",
            "Step 505500, Loss: 9.915942602157592\n",
            "Step 505600, Loss: 9.726765241622925\n",
            "Step 505700, Loss: 10.033610363006591\n",
            "Step 505800, Loss: 9.445629367828369\n",
            "Step 505900, Loss: 9.466223688125611\n",
            "Step 506000, Loss: 9.798533477783202\n",
            "Step 506100, Loss: 9.963819046020507\n",
            "Step 506200, Loss: 9.626756820678711\n",
            "Step 506300, Loss: 9.640967397689819\n",
            "Step 506400, Loss: 9.934605207443237\n",
            "Step 506500, Loss: 9.816604375839233\n",
            "Step 506600, Loss: 9.757961101531983\n",
            "Step 506700, Loss: 10.082470445632934\n",
            "Step 506800, Loss: 9.780553979873657\n",
            "Step 506900, Loss: 9.543916530609131\n",
            "Step 507000, Loss: 9.86249800682068\n",
            "Step 507100, Loss: 9.656105966567994\n",
            "Step 507200, Loss: 9.65757088661194\n",
            "Step 507300, Loss: 9.667996530532838\n",
            "Step 507400, Loss: 9.393731756210327\n",
            "Step 507500, Loss: 9.470751075744628\n",
            "Step 507600, Loss: 9.379252109527588\n",
            "Step 507700, Loss: 9.317289714813233\n",
            "Step 507800, Loss: 9.726492319107056\n",
            "Step 507900, Loss: 9.378660612106323\n",
            "Step 508000, Loss: 9.721440048217774\n",
            "Step 508100, Loss: 9.590925817489625\n",
            "Step 508200, Loss: 9.288831415176391\n",
            "Step 508300, Loss: 9.411605739593506\n",
            "Step 508400, Loss: 9.683941278457642\n",
            "Step 508500, Loss: 9.392246046066283\n",
            "Step 508600, Loss: 9.74608654975891\n",
            "Step 508700, Loss: 9.441181259155274\n",
            "Step 508800, Loss: 9.415689401626587\n",
            "Step 508900, Loss: 9.459929294586182\n",
            "Step 509000, Loss: 9.483176193237306\n",
            "Step 509100, Loss: 9.860657873153686\n",
            "Step 509200, Loss: 9.706243648529053\n",
            "Step 509300, Loss: 9.935250434875488\n",
            "Step 509400, Loss: 9.621591396331787\n",
            "Step 509500, Loss: 9.712521619796753\n",
            "Step 509600, Loss: 9.728831272125245\n",
            "Step 509700, Loss: 9.945065670013427\n",
            "Step 509800, Loss: 10.42648232460022\n",
            "epoch no  13\n",
            "Step 509900, Loss: 9.698722820281983\n",
            "Step 510000, Loss: 9.805045404434203\n",
            "Step 510100, Loss: 9.701728982925415\n",
            "Step 510200, Loss: 9.465546607971191\n",
            "Step 510300, Loss: 9.682360038757324\n",
            "Step 510400, Loss: 9.715650396347046\n",
            "Step 510500, Loss: 9.777931299209595\n",
            "Step 510600, Loss: 9.741334409713746\n",
            "Step 510700, Loss: 10.017417278289795\n",
            "Step 510800, Loss: 10.060176124572754\n",
            "Step 510900, Loss: 9.928842477798462\n",
            "Step 511000, Loss: 9.831146249771118\n",
            "Step 511100, Loss: 10.07718892097473\n",
            "Step 511200, Loss: 9.969892950057984\n",
            "Step 511300, Loss: 9.921295251846313\n",
            "Step 511400, Loss: 9.922463788986207\n",
            "Step 511500, Loss: 9.760682439804077\n",
            "Step 511600, Loss: 9.8938627243042\n",
            "Step 511700, Loss: 10.068512411117554\n",
            "Step 511800, Loss: 9.824302806854249\n",
            "Step 511900, Loss: 9.750595874786377\n",
            "Step 512000, Loss: 9.71149118423462\n",
            "Step 512100, Loss: 10.166287860870362\n",
            "Step 512200, Loss: 9.96177104949951\n",
            "Step 512300, Loss: 9.891996402740478\n",
            "Step 512400, Loss: 9.871184549331666\n",
            "Step 512500, Loss: 9.372888622283936\n",
            "Step 512600, Loss: 9.499658489227295\n",
            "Step 512700, Loss: 9.762552309036256\n",
            "Step 512800, Loss: 9.761404390335082\n",
            "Step 512900, Loss: 9.690033683776855\n",
            "Step 513000, Loss: 9.758639011383057\n",
            "Step 513100, Loss: 9.787004814147949\n",
            "Step 513200, Loss: 9.686112928390504\n",
            "Step 513300, Loss: 9.565688714981079\n",
            "Step 513400, Loss: 9.779470825195313\n",
            "Step 513500, Loss: 9.622711582183838\n",
            "Step 513600, Loss: 9.670545949935914\n",
            "Step 513700, Loss: 9.652215175628662\n",
            "Step 513800, Loss: 9.610492887496948\n",
            "Step 513900, Loss: 9.509122276306153\n",
            "Step 514000, Loss: 9.411855449676514\n",
            "Step 514100, Loss: 9.632382612228394\n",
            "Step 514200, Loss: 9.791224966049194\n",
            "Step 514300, Loss: 9.840405044555665\n",
            "Step 514400, Loss: 9.395227575302124\n",
            "Step 514500, Loss: 9.501408824920654\n",
            "Step 514600, Loss: 9.663433694839478\n",
            "Step 514700, Loss: 9.743912649154662\n",
            "Step 514800, Loss: 9.629599123001098\n",
            "Step 514900, Loss: 9.617825574874878\n",
            "Step 515000, Loss: 9.52360273361206\n",
            "Step 515100, Loss: 9.861578693389893\n",
            "Step 515200, Loss: 9.625268850326538\n",
            "Step 515300, Loss: 9.598100624084474\n",
            "Step 515400, Loss: 9.696870307922364\n",
            "Step 515500, Loss: 10.02129397392273\n",
            "Step 515600, Loss: 9.79318055152893\n",
            "Step 515700, Loss: 9.791513004302978\n",
            "Step 515800, Loss: 9.869397563934326\n",
            "Step 515900, Loss: 10.029506988525391\n",
            "Step 516000, Loss: 9.96954174041748\n",
            "Step 516100, Loss: 9.887970962524413\n",
            "Step 516200, Loss: 10.011417217254639\n",
            "Step 516300, Loss: 9.743216619491577\n",
            "Step 516400, Loss: 9.74394495010376\n",
            "Step 516500, Loss: 9.950427227020263\n",
            "Step 516600, Loss: 10.34994709968567\n",
            "Step 516700, Loss: 9.877695074081421\n",
            "Step 516800, Loss: 9.631772775650024\n",
            "Step 516900, Loss: 9.469772720336914\n",
            "Step 517000, Loss: 9.52375054359436\n",
            "Step 517100, Loss: 9.693987512588501\n",
            "Step 517200, Loss: 9.522153749465943\n",
            "Step 517300, Loss: 9.292044343948364\n",
            "Step 517400, Loss: 9.65513171195984\n",
            "Step 517500, Loss: 9.597863578796387\n",
            "Step 517600, Loss: 9.381407766342162\n",
            "Step 517700, Loss: 9.381833734512329\n",
            "Step 517800, Loss: 9.564024391174316\n",
            "Step 517900, Loss: 9.932011795043945\n",
            "Step 518000, Loss: 9.589853496551514\n",
            "Step 518100, Loss: 9.635962524414062\n",
            "Step 518200, Loss: 9.657523097991943\n",
            "Step 518300, Loss: 9.758705797195434\n",
            "Step 518400, Loss: 9.741319417953491\n",
            "Step 518500, Loss: 9.684181680679322\n",
            "Step 518600, Loss: 9.649928007125855\n",
            "Step 518700, Loss: 9.743848600387572\n",
            "Step 518800, Loss: 10.330222778320312\n",
            "Step 518900, Loss: 10.200455207824707\n",
            "Step 519000, Loss: 9.98686562538147\n",
            "Step 519100, Loss: 9.922360172271729\n",
            "Step 519200, Loss: 10.275287141799927\n",
            "Step 519300, Loss: 10.056083068847656\n",
            "Step 519400, Loss: 9.757250881195068\n",
            "Step 519500, Loss: 9.792672853469849\n",
            "Step 519600, Loss: 9.604116497039795\n",
            "Step 519700, Loss: 9.503312759399414\n",
            "Step 519800, Loss: 9.719663162231445\n",
            "Step 519900, Loss: 9.537853212356568\n",
            "Step 520000, Loss: 9.739798021316528\n",
            "Step 520100, Loss: 9.9589412689209\n",
            "Step 520200, Loss: 9.966442613601684\n",
            "Step 520300, Loss: 9.42838761329651\n",
            "Step 520400, Loss: 9.724380111694336\n",
            "Step 520500, Loss: 9.541319875717162\n",
            "Step 520600, Loss: 9.573356513977052\n",
            "Step 520700, Loss: 9.964413633346558\n",
            "Step 520800, Loss: 9.700130348205567\n",
            "Step 520900, Loss: 9.63726523399353\n",
            "Step 521000, Loss: 9.761900634765626\n",
            "Step 521100, Loss: 9.691221799850464\n",
            "Step 521200, Loss: 9.956154050827026\n",
            "Step 521300, Loss: 9.43532753944397\n",
            "Step 521400, Loss: 9.603756465911864\n",
            "Step 521500, Loss: 9.65021884918213\n",
            "Step 521600, Loss: 9.799828758239746\n",
            "Step 521700, Loss: 9.596166028976441\n",
            "Step 521800, Loss: 9.345445928573609\n",
            "Step 521900, Loss: 9.633082389831543\n",
            "Step 522000, Loss: 9.553505268096924\n",
            "Step 522100, Loss: 9.841237306594849\n",
            "Step 522200, Loss: 9.83109546661377\n",
            "Step 522300, Loss: 9.68065752029419\n",
            "Step 522400, Loss: 10.00613763809204\n",
            "Step 522500, Loss: 9.650676965713501\n",
            "Step 522600, Loss: 9.502563877105713\n",
            "Step 522700, Loss: 9.821791915893554\n",
            "Step 522800, Loss: 9.823407325744629\n",
            "Step 522900, Loss: 9.826694574356079\n",
            "Step 523000, Loss: 9.592432098388672\n",
            "Step 523100, Loss: 9.871481809616089\n",
            "Step 523200, Loss: 9.52778657913208\n",
            "Step 523300, Loss: 9.371650371551514\n",
            "Step 523400, Loss: 9.500565614700317\n",
            "Step 523500, Loss: 10.034375257492066\n",
            "Step 523600, Loss: 9.624833364486694\n",
            "Step 523700, Loss: 9.43577610015869\n",
            "Step 523800, Loss: 9.703946027755737\n",
            "Step 523900, Loss: 9.63586046218872\n",
            "Step 524000, Loss: 9.915302181243897\n",
            "Step 524100, Loss: 9.49813980102539\n",
            "Step 524200, Loss: 9.522497768402099\n",
            "Step 524300, Loss: 9.729338665008544\n",
            "Step 524400, Loss: 9.694541387557983\n",
            "Step 524500, Loss: 9.45077615737915\n",
            "Step 524600, Loss: 9.515926971435547\n",
            "Step 524700, Loss: 9.584646577835082\n",
            "Step 524800, Loss: 9.81571499824524\n",
            "Step 524900, Loss: 9.689660272598267\n",
            "Step 525000, Loss: 9.809014711380005\n",
            "Step 525100, Loss: 9.835452470779419\n",
            "Step 525200, Loss: 9.729008207321167\n",
            "Step 525300, Loss: 9.572092552185058\n",
            "Step 525400, Loss: 9.395259475708007\n",
            "Step 525500, Loss: 9.46147873878479\n",
            "Step 525600, Loss: 9.910741224288941\n",
            "Step 525700, Loss: 9.744935455322265\n",
            "Step 525800, Loss: 9.601781759262085\n",
            "Step 525900, Loss: 9.647675905227661\n",
            "Step 526000, Loss: 9.766383533477784\n",
            "Step 526100, Loss: 9.393259325027465\n",
            "Step 526200, Loss: 9.620456676483155\n",
            "Step 526300, Loss: 9.598945941925049\n",
            "Step 526400, Loss: 9.493504209518433\n",
            "Step 526500, Loss: 9.62048625946045\n",
            "Step 526600, Loss: 9.768863563537598\n",
            "Step 526700, Loss: 9.719048862457276\n",
            "Step 526800, Loss: 9.608327751159669\n",
            "Step 526900, Loss: 9.740718603134155\n",
            "Step 527000, Loss: 9.55542360305786\n",
            "Step 527100, Loss: 9.619484405517579\n",
            "Step 527200, Loss: 9.480469675064088\n",
            "Step 527300, Loss: 9.39041983604431\n",
            "Step 527400, Loss: 9.508350467681884\n",
            "Step 527500, Loss: 9.588728580474854\n",
            "Step 527600, Loss: 9.684650888442993\n",
            "Step 527700, Loss: 9.412125692367553\n",
            "Step 527800, Loss: 9.462609252929688\n",
            "Step 527900, Loss: 9.326422262191773\n",
            "Step 528000, Loss: 9.376150312423706\n",
            "Step 528100, Loss: 9.539546756744384\n",
            "Step 528200, Loss: 9.423539113998412\n",
            "Step 528300, Loss: 9.544705896377563\n",
            "Step 528400, Loss: 9.76311559677124\n",
            "Step 528500, Loss: 9.893627262115478\n",
            "Step 528600, Loss: 9.513885946273804\n",
            "Step 528700, Loss: 9.458285808563232\n",
            "Step 528800, Loss: 9.329138746261597\n",
            "Step 528900, Loss: 9.440989646911621\n",
            "Step 529000, Loss: 9.62402717590332\n",
            "Step 529100, Loss: 9.871767110824585\n",
            "Step 529200, Loss: 9.494044380187988\n",
            "Step 529300, Loss: 9.848320121765136\n",
            "Step 529400, Loss: 9.952940826416016\n",
            "Step 529500, Loss: 10.201012506484986\n",
            "Step 529600, Loss: 9.6344735622406\n",
            "Step 529700, Loss: 9.502476654052735\n",
            "Step 529800, Loss: 9.61146198272705\n",
            "Step 529900, Loss: 9.733707599639892\n",
            "Step 530000, Loss: 9.434768123626709\n",
            "Step 530100, Loss: 9.539008684158325\n",
            "Step 530200, Loss: 9.44563549041748\n",
            "Step 530300, Loss: 9.649994926452637\n",
            "Step 530400, Loss: 9.5380890083313\n",
            "Step 530500, Loss: 9.68440083503723\n",
            "Step 530600, Loss: 9.51552453994751\n",
            "Step 530700, Loss: 9.609431648254395\n",
            "Step 530800, Loss: 9.787079496383667\n",
            "Step 530900, Loss: 9.437620983123779\n",
            "Step 531000, Loss: 9.827425050735474\n",
            "Step 531100, Loss: 9.672021493911743\n",
            "Step 531200, Loss: 10.043011331558228\n",
            "Step 531300, Loss: 9.541349325180054\n",
            "Step 531400, Loss: 9.694438648223876\n",
            "Step 531500, Loss: 9.636120128631593\n",
            "Step 531600, Loss: 9.517224063873291\n",
            "Step 531700, Loss: 9.596350746154785\n",
            "Step 531800, Loss: 9.222344722747803\n",
            "Step 531900, Loss: 9.575335721969605\n",
            "Step 532000, Loss: 9.667815093994141\n",
            "Step 532100, Loss: 9.674687242507934\n",
            "Step 532200, Loss: 9.405166702270508\n",
            "Step 532300, Loss: 9.766206283569336\n",
            "Step 532400, Loss: 9.551779956817628\n",
            "Step 532500, Loss: 9.762537832260131\n",
            "Step 532600, Loss: 9.761525678634644\n",
            "Step 532700, Loss: 9.562730836868287\n",
            "Step 532800, Loss: 9.827146339416505\n",
            "Step 532900, Loss: 9.639124040603638\n",
            "Step 533000, Loss: 9.593621072769166\n",
            "Step 533100, Loss: 10.102811765670776\n",
            "Step 533200, Loss: 10.065550708770752\n",
            "Step 533300, Loss: 9.827482776641846\n",
            "Step 533400, Loss: 9.990622663497925\n",
            "Step 533500, Loss: 9.72892183303833\n",
            "Step 533600, Loss: 9.840203485488892\n",
            "Step 533700, Loss: 9.789974613189697\n",
            "Step 533800, Loss: 10.280345735549927\n",
            "Step 533900, Loss: 10.479216051101684\n",
            "Step 534000, Loss: 10.158746938705445\n",
            "Step 534100, Loss: 9.497791967391969\n",
            "Step 534200, Loss: 9.492598648071288\n",
            "Step 534300, Loss: 9.648223190307617\n",
            "Step 534400, Loss: 9.360066719055176\n",
            "Step 534500, Loss: 9.568471717834473\n",
            "Step 534600, Loss: 9.742546548843384\n",
            "Step 534700, Loss: 9.717302856445313\n",
            "Step 534800, Loss: 9.416905736923217\n",
            "Step 534900, Loss: 9.44438244819641\n",
            "Step 535000, Loss: 9.63519052505493\n",
            "Step 535100, Loss: 9.472230014801026\n",
            "Step 535200, Loss: 9.416459131240845\n",
            "Step 535300, Loss: 9.238428754806518\n",
            "Step 535400, Loss: 9.71492142677307\n",
            "Step 535500, Loss: 9.668402318954469\n",
            "Step 535600, Loss: 9.527123470306396\n",
            "Step 535700, Loss: 9.492011795043945\n",
            "Step 535800, Loss: 9.36043755531311\n",
            "Step 535900, Loss: 9.48215573310852\n",
            "Step 536000, Loss: 9.540628547668456\n",
            "Step 536100, Loss: 9.681798515319825\n",
            "Step 536200, Loss: 9.680425291061402\n",
            "Step 536300, Loss: 9.474853773117065\n",
            "Step 536400, Loss: 9.54317400932312\n",
            "Step 536500, Loss: 9.42120506286621\n",
            "Step 536600, Loss: 9.4972696018219\n",
            "Step 536700, Loss: 9.352688808441162\n",
            "Step 536800, Loss: 9.421485776901244\n",
            "Step 536900, Loss: 9.882927980422973\n",
            "Step 537000, Loss: 9.902407712936402\n",
            "Step 537100, Loss: 9.695081062316895\n",
            "Step 537200, Loss: 9.69629726409912\n",
            "Step 537300, Loss: 9.850447769165038\n",
            "Step 537400, Loss: 9.671802082061767\n",
            "Step 537500, Loss: 9.451995639801025\n",
            "Step 537600, Loss: 9.767946662902832\n",
            "Step 537700, Loss: 9.707607774734496\n",
            "Step 537800, Loss: 9.95839066505432\n",
            "Step 537900, Loss: 9.994729557037353\n",
            "Step 538000, Loss: 10.090711851119995\n",
            "Step 538100, Loss: 10.141786422729492\n",
            "Step 538200, Loss: 9.855809736251832\n",
            "Step 538300, Loss: 10.27249701499939\n",
            "Step 538400, Loss: 10.037627992630005\n",
            "Step 538500, Loss: 9.624036521911622\n",
            "Step 538600, Loss: 10.228847036361694\n",
            "Step 538700, Loss: 10.307670783996581\n",
            "Step 538800, Loss: 9.598364295959472\n",
            "Step 538900, Loss: 9.705560998916626\n",
            "Step 539000, Loss: 9.502466478347777\n",
            "Step 539100, Loss: 9.768811140060425\n",
            "Step 539200, Loss: 9.422799711227418\n",
            "Step 539300, Loss: 9.575740547180176\n",
            "Step 539400, Loss: 10.027847528457642\n",
            "Step 539500, Loss: 9.661835327148438\n",
            "Step 539600, Loss: 9.548638639450074\n",
            "Step 539700, Loss: 9.840267295837402\n",
            "Step 539800, Loss: 9.980377025604248\n",
            "Step 539900, Loss: 9.817311267852784\n",
            "Step 540000, Loss: 9.52605061531067\n",
            "Step 540100, Loss: 9.773367166519165\n",
            "Step 540200, Loss: 10.038659315109253\n",
            "Step 540300, Loss: 9.548729476928711\n",
            "Step 540400, Loss: 9.355913171768188\n",
            "Step 540500, Loss: 9.812199649810792\n",
            "Step 540600, Loss: 9.795297393798828\n",
            "Step 540700, Loss: 9.552534837722778\n",
            "Step 540800, Loss: 9.405444707870483\n",
            "Step 540900, Loss: 9.480488729476928\n",
            "Step 541000, Loss: 9.531026458740234\n",
            "Step 541100, Loss: 9.417024240493774\n",
            "Step 541200, Loss: 9.60464768409729\n",
            "Step 541300, Loss: 9.16719183921814\n",
            "Step 541400, Loss: 9.411704959869384\n",
            "Step 541500, Loss: 9.494750270843506\n",
            "Step 541600, Loss: 9.576244869232177\n",
            "Step 541700, Loss: 9.48594225883484\n",
            "Step 541800, Loss: 9.588623056411743\n",
            "Step 541900, Loss: 9.468535299301147\n",
            "Step 542000, Loss: 9.31798797607422\n",
            "Step 542100, Loss: 9.61003186225891\n",
            "Step 542200, Loss: 9.523286972045899\n",
            "Step 542300, Loss: 9.667496004104613\n",
            "Step 542400, Loss: 9.604054775238037\n",
            "Step 542500, Loss: 9.614129753112794\n",
            "Step 542600, Loss: 9.528737831115723\n",
            "Step 542700, Loss: 9.552882661819458\n",
            "Step 542800, Loss: 9.520539846420288\n",
            "Step 542900, Loss: 9.70072117805481\n",
            "Step 543000, Loss: 9.60147437095642\n",
            "Step 543100, Loss: 9.386509733200073\n",
            "Step 543200, Loss: 9.656763582229614\n",
            "Step 543300, Loss: 9.656926584243774\n",
            "Step 543400, Loss: 9.521058158874512\n",
            "Step 543500, Loss: 9.701942119598389\n",
            "Step 543600, Loss: 9.550534477233887\n",
            "Step 543700, Loss: 9.227556571960449\n",
            "Step 543800, Loss: 9.572082653045655\n",
            "Step 543900, Loss: 9.709336032867432\n",
            "Step 544000, Loss: 9.408390798568725\n",
            "Step 544100, Loss: 9.440243215560914\n",
            "Step 544200, Loss: 9.776800947189331\n",
            "Step 544300, Loss: 9.765313682556153\n",
            "Step 544400, Loss: 9.578799657821655\n",
            "Step 544500, Loss: 9.73663787841797\n",
            "Step 544600, Loss: 9.59521206855774\n",
            "Step 544700, Loss: 9.936276559829713\n",
            "Step 544800, Loss: 9.631443109512329\n",
            "Step 544900, Loss: 10.058026533126831\n",
            "Step 545000, Loss: 9.460910911560058\n",
            "Step 545100, Loss: 9.353791522979737\n",
            "Step 545200, Loss: 9.651254720687866\n",
            "Step 545300, Loss: 9.961212673187255\n",
            "Step 545400, Loss: 9.545450325012206\n",
            "Step 545500, Loss: 9.684516534805297\n",
            "Step 545600, Loss: 9.772395486831664\n",
            "Step 545700, Loss: 9.864017210006715\n",
            "Step 545800, Loss: 9.511067295074463\n",
            "Step 545900, Loss: 10.13836564064026\n",
            "Step 546000, Loss: 9.768756580352782\n",
            "Step 546100, Loss: 9.533942213058472\n",
            "Step 546200, Loss: 9.715420503616333\n",
            "Step 546300, Loss: 9.691181964874268\n",
            "Step 546400, Loss: 9.566259021759032\n",
            "Step 546500, Loss: 9.665048179626465\n",
            "Step 546600, Loss: 9.343578977584839\n",
            "Step 546700, Loss: 9.437833948135376\n",
            "Step 546800, Loss: 9.338113069534302\n",
            "Step 546900, Loss: 9.263622961044312\n",
            "Step 547000, Loss: 9.697745485305786\n",
            "Step 547100, Loss: 9.320118217468261\n",
            "Step 547200, Loss: 9.641288232803344\n",
            "Step 547300, Loss: 9.615454254150391\n",
            "Step 547400, Loss: 9.304094638824463\n",
            "Step 547500, Loss: 9.357483367919922\n",
            "Step 547600, Loss: 9.64109697341919\n",
            "Step 547700, Loss: 9.402280836105346\n",
            "Step 547800, Loss: 9.628069581985473\n",
            "Step 547900, Loss: 9.413219680786133\n",
            "Step 548000, Loss: 9.369950513839722\n",
            "Step 548100, Loss: 9.367144107818604\n",
            "Step 548200, Loss: 9.452706623077393\n",
            "Step 548300, Loss: 9.737358570098877\n",
            "Step 548400, Loss: 9.716787033081054\n",
            "Step 548500, Loss: 9.808203468322754\n",
            "Step 548600, Loss: 9.639525566101074\n",
            "Step 548700, Loss: 9.649105949401855\n",
            "Step 548800, Loss: 9.70570993423462\n",
            "Step 548900, Loss: 9.764562129974365\n",
            "Step 549000, Loss: 10.367920608520508\n",
            "epoch no  14\n",
            "Step 549100, Loss: 9.76964472770691\n",
            "Step 549200, Loss: 9.759932060241699\n",
            "Step 549300, Loss: 9.629029569625855\n",
            "Step 549400, Loss: 9.503428001403808\n",
            "Step 549500, Loss: 9.524968852996826\n",
            "Step 549600, Loss: 9.726200065612794\n",
            "Step 549700, Loss: 9.650433206558228\n",
            "Step 549800, Loss: 9.663333463668824\n",
            "Step 549900, Loss: 10.042272233963013\n",
            "Step 550000, Loss: 9.958679132461548\n",
            "Step 550100, Loss: 9.97731882095337\n",
            "Step 550200, Loss: 9.816436595916748\n",
            "Step 550300, Loss: 10.013157539367675\n",
            "Step 550400, Loss: 9.844723796844482\n",
            "Step 550500, Loss: 9.946172037124633\n",
            "Step 550600, Loss: 9.834637126922608\n",
            "Step 550700, Loss: 9.771004848480224\n",
            "Step 550800, Loss: 9.774565324783325\n",
            "Step 550900, Loss: 10.01480089187622\n",
            "Step 551000, Loss: 9.895548009872437\n",
            "Step 551100, Loss: 9.636779870986938\n",
            "Step 551200, Loss: 9.648729934692383\n",
            "Step 551300, Loss: 10.054212226867676\n",
            "Step 551400, Loss: 9.892759475708008\n",
            "Step 551500, Loss: 9.937763414382935\n",
            "Step 551600, Loss: 9.851288747787475\n",
            "Step 551700, Loss: 9.443604850769043\n",
            "Step 551800, Loss: 9.431428308486938\n",
            "Step 551900, Loss: 9.71170907020569\n",
            "Step 552000, Loss: 9.754534177780151\n",
            "Step 552100, Loss: 9.592208185195922\n",
            "Step 552200, Loss: 9.7838871383667\n",
            "Step 552300, Loss: 9.67705533027649\n",
            "Step 552400, Loss: 9.691771430969238\n",
            "Step 552500, Loss: 9.459392900466918\n",
            "Step 552600, Loss: 9.857549219131469\n",
            "Step 552700, Loss: 9.490338525772096\n",
            "Step 552800, Loss: 9.64978524208069\n",
            "Step 552900, Loss: 9.602367238998413\n",
            "Step 553000, Loss: 9.555348176956176\n",
            "Step 553100, Loss: 9.458972539901733\n",
            "Step 553200, Loss: 9.417322826385497\n",
            "Step 553300, Loss: 9.568060102462768\n",
            "Step 553400, Loss: 9.711876888275146\n",
            "Step 553500, Loss: 9.883271036148072\n",
            "Step 553600, Loss: 9.424437713623046\n",
            "Step 553700, Loss: 9.336105823516846\n",
            "Step 553800, Loss: 9.595851049423217\n",
            "Step 553900, Loss: 9.681631984710693\n",
            "Step 554000, Loss: 9.631522188186645\n",
            "Step 554100, Loss: 9.584270849227906\n",
            "Step 554200, Loss: 9.49433048248291\n",
            "Step 554300, Loss: 9.771541118621826\n",
            "Step 554400, Loss: 9.581238737106323\n",
            "Step 554500, Loss: 9.603135175704956\n",
            "Step 554600, Loss: 9.622490587234497\n",
            "Step 554700, Loss: 10.01461298942566\n",
            "Step 554800, Loss: 9.723911590576172\n",
            "Step 554900, Loss: 9.737828626632691\n",
            "Step 555000, Loss: 9.772551498413087\n",
            "Step 555100, Loss: 9.888995752334594\n",
            "Step 555200, Loss: 10.006430540084839\n",
            "Step 555300, Loss: 9.837083864212037\n",
            "Step 555400, Loss: 9.967481536865234\n",
            "Step 555500, Loss: 9.709872283935546\n",
            "Step 555600, Loss: 9.707497224807739\n",
            "Step 555700, Loss: 9.87989315032959\n",
            "Step 555800, Loss: 10.351630163192748\n",
            "Step 555900, Loss: 9.745908613204955\n",
            "Step 556000, Loss: 9.705638856887818\n",
            "Step 556100, Loss: 9.407491798400878\n",
            "Step 556200, Loss: 9.50905330657959\n",
            "Step 556300, Loss: 9.556358461380006\n",
            "Step 556400, Loss: 9.477617559432984\n",
            "Step 556500, Loss: 9.274317750930786\n",
            "Step 556600, Loss: 9.622632102966309\n",
            "Step 556700, Loss: 9.591797685623169\n",
            "Step 556800, Loss: 9.36480164527893\n",
            "Step 556900, Loss: 9.434757890701293\n",
            "Step 557000, Loss: 9.389914875030518\n",
            "Step 557100, Loss: 9.93026177406311\n",
            "Step 557200, Loss: 9.592219343185425\n",
            "Step 557300, Loss: 9.557018146514892\n",
            "Step 557400, Loss: 9.616074647903442\n",
            "Step 557500, Loss: 9.690127487182616\n",
            "Step 557600, Loss: 9.786785411834718\n",
            "Step 557700, Loss: 9.677087154388428\n",
            "Step 557800, Loss: 9.549866418838501\n",
            "Step 557900, Loss: 9.550486373901368\n",
            "Step 558000, Loss: 10.227737064361571\n",
            "Step 558100, Loss: 10.181106300354005\n",
            "Step 558200, Loss: 9.994599027633667\n",
            "Step 558300, Loss: 9.853160362243653\n",
            "Step 558400, Loss: 10.208437032699585\n",
            "Step 558500, Loss: 10.118006782531738\n",
            "Step 558600, Loss: 9.736585788726806\n",
            "Step 558700, Loss: 9.7509051322937\n",
            "Step 558800, Loss: 9.549952564239502\n",
            "Step 558900, Loss: 9.377072658538818\n",
            "Step 559000, Loss: 9.786557159423829\n",
            "Step 559100, Loss: 9.507776527404785\n",
            "Step 559200, Loss: 9.73509937286377\n",
            "Step 559300, Loss: 9.737604732513427\n",
            "Step 559400, Loss: 9.986261405944823\n",
            "Step 559500, Loss: 9.453166227340699\n",
            "Step 559600, Loss: 9.642894287109375\n",
            "Step 559700, Loss: 9.548561496734619\n",
            "Step 559800, Loss: 9.548432168960572\n",
            "Step 559900, Loss: 9.902354154586792\n",
            "Step 560000, Loss: 9.64634153366089\n",
            "Step 560100, Loss: 9.573746194839478\n",
            "Step 560200, Loss: 9.769951915740966\n",
            "Step 560300, Loss: 9.651335897445678\n",
            "Step 560400, Loss: 9.860933856964111\n",
            "Step 560500, Loss: 9.46312243461609\n",
            "Step 560600, Loss: 9.598073587417602\n",
            "Step 560700, Loss: 9.55365719795227\n",
            "Step 560800, Loss: 9.738303909301758\n",
            "Step 560900, Loss: 9.564499206542969\n",
            "Step 561000, Loss: 9.3592746925354\n",
            "Step 561100, Loss: 9.432549924850465\n",
            "Step 561200, Loss: 9.523649835586548\n",
            "Step 561300, Loss: 9.7745778465271\n",
            "Step 561400, Loss: 9.715258455276489\n",
            "Step 561500, Loss: 9.725142555236816\n",
            "Step 561600, Loss: 9.930356140136718\n",
            "Step 561700, Loss: 9.655146627426147\n",
            "Step 561800, Loss: 9.379979677200318\n",
            "Step 561900, Loss: 9.843916988372802\n",
            "Step 562000, Loss: 9.691440858840942\n",
            "Step 562100, Loss: 9.826456699371338\n",
            "Step 562200, Loss: 9.484693269729615\n",
            "Step 562300, Loss: 9.853102407455445\n",
            "Step 562400, Loss: 9.593964929580688\n",
            "Step 562500, Loss: 9.303054838180541\n",
            "Step 562600, Loss: 9.240858221054078\n",
            "Step 562700, Loss: 10.004963035583495\n",
            "Step 562800, Loss: 9.696058378219604\n",
            "Step 562900, Loss: 9.405051698684693\n",
            "Step 563000, Loss: 9.626739883422852\n",
            "Step 563100, Loss: 9.542666463851928\n",
            "Step 563200, Loss: 9.872421550750733\n",
            "Step 563300, Loss: 9.457912139892578\n",
            "Step 563400, Loss: 9.495695400238038\n",
            "Step 563500, Loss: 9.662024374008178\n",
            "Step 563600, Loss: 9.69675745010376\n",
            "Step 563700, Loss: 9.37270902633667\n",
            "Step 563800, Loss: 9.500130615234376\n",
            "Step 563900, Loss: 9.475004568099976\n",
            "Step 564000, Loss: 9.781832294464111\n",
            "Step 564100, Loss: 9.66083601951599\n",
            "Step 564200, Loss: 9.869358558654785\n",
            "Step 564300, Loss: 9.625494194030761\n",
            "Step 564400, Loss: 9.799231634140014\n",
            "Step 564500, Loss: 9.576539325714112\n",
            "Step 564600, Loss: 9.362819719314576\n",
            "Step 564700, Loss: 9.362333917617798\n",
            "Step 564800, Loss: 9.834403924942016\n",
            "Step 564900, Loss: 9.617273788452149\n",
            "Step 565000, Loss: 9.649816284179687\n",
            "Step 565100, Loss: 9.59431287765503\n",
            "Step 565200, Loss: 9.645626173019409\n",
            "Step 565300, Loss: 9.474169521331786\n",
            "Step 565400, Loss: 9.504424123764037\n",
            "Step 565500, Loss: 9.50675178527832\n",
            "Step 565600, Loss: 9.540576105117799\n",
            "Step 565700, Loss: 9.574170427322388\n",
            "Step 565800, Loss: 9.69473521232605\n",
            "Step 565900, Loss: 9.704334115982055\n",
            "Step 566000, Loss: 9.534430198669433\n",
            "Step 566100, Loss: 9.703441028594971\n",
            "Step 566200, Loss: 9.531760845184326\n",
            "Step 566300, Loss: 9.440928802490234\n",
            "Step 566400, Loss: 9.522166328430176\n",
            "Step 566500, Loss: 9.323938598632813\n",
            "Step 566600, Loss: 9.428063287734986\n",
            "Step 566700, Loss: 9.558608179092408\n",
            "Step 566800, Loss: 9.604195766448974\n",
            "Step 566900, Loss: 9.415116100311279\n",
            "Step 567000, Loss: 9.402771492004394\n",
            "Step 567100, Loss: 9.337377395629883\n",
            "Step 567200, Loss: 9.283694696426391\n",
            "Step 567300, Loss: 9.465690078735351\n",
            "Step 567400, Loss: 9.392446708679199\n",
            "Step 567500, Loss: 9.56636622428894\n",
            "Step 567600, Loss: 9.649066877365112\n",
            "Step 567700, Loss: 9.83960949897766\n",
            "Step 567800, Loss: 9.532531299591064\n",
            "Step 567900, Loss: 9.501646413803101\n",
            "Step 568000, Loss: 9.212495908737182\n",
            "Step 568100, Loss: 9.40960090637207\n",
            "Step 568200, Loss: 9.56482850074768\n",
            "Step 568300, Loss: 9.721993560791015\n",
            "Step 568400, Loss: 9.51335955619812\n",
            "Step 568500, Loss: 9.787679719924927\n",
            "Step 568600, Loss: 9.845148591995239\n",
            "Step 568700, Loss: 10.089868640899658\n",
            "Step 568800, Loss: 9.76656491279602\n",
            "Step 568900, Loss: 9.430181055068969\n",
            "Step 569000, Loss: 9.512737054824829\n",
            "Step 569100, Loss: 9.731716384887696\n",
            "Step 569200, Loss: 9.338324422836303\n",
            "Step 569300, Loss: 9.564297609329223\n",
            "Step 569400, Loss: 9.339030323028565\n",
            "Step 569500, Loss: 9.579727697372437\n",
            "Step 569600, Loss: 9.419582805633546\n",
            "Step 569700, Loss: 9.684809198379517\n",
            "Step 569800, Loss: 9.427251644134522\n",
            "Step 569900, Loss: 9.618602933883666\n",
            "Step 570000, Loss: 9.702938299179078\n",
            "Step 570100, Loss: 9.495045299530029\n",
            "Step 570200, Loss: 9.735377607345582\n",
            "Step 570300, Loss: 9.49788275718689\n",
            "Step 570400, Loss: 9.983713703155518\n",
            "Step 570500, Loss: 9.6596226978302\n",
            "Step 570600, Loss: 9.58208077430725\n",
            "Step 570700, Loss: 9.652726459503175\n",
            "Step 570800, Loss: 9.413288097381592\n",
            "Step 570900, Loss: 9.597150764465333\n",
            "Step 571000, Loss: 9.197882041931152\n",
            "Step 571100, Loss: 9.405285778045654\n",
            "Step 571200, Loss: 9.741347513198853\n",
            "Step 571300, Loss: 9.578145389556886\n",
            "Step 571400, Loss: 9.403522138595582\n",
            "Step 571500, Loss: 9.636090669631958\n",
            "Step 571600, Loss: 9.524075298309326\n",
            "Step 571700, Loss: 9.705307426452636\n",
            "Step 571800, Loss: 9.781216831207276\n",
            "Step 571900, Loss: 9.529017343521119\n",
            "Step 572000, Loss: 9.708966426849365\n",
            "Step 572100, Loss: 9.662464027404786\n",
            "Step 572200, Loss: 9.532666368484497\n",
            "Step 572300, Loss: 9.993514547348022\n",
            "Step 572400, Loss: 10.075105285644531\n",
            "Step 572500, Loss: 9.803397512435913\n",
            "Step 572600, Loss: 10.003979787826538\n",
            "Step 572700, Loss: 9.639793558120727\n",
            "Step 572800, Loss: 9.792022218704224\n",
            "Step 572900, Loss: 9.81059739112854\n",
            "Step 573000, Loss: 10.183674058914185\n",
            "Step 573100, Loss: 10.4230299949646\n",
            "Step 573200, Loss: 10.102342596054077\n",
            "Step 573300, Loss: 9.589136638641357\n",
            "Step 573400, Loss: 9.458097305297851\n",
            "Step 573500, Loss: 9.535206851959229\n",
            "Step 573600, Loss: 9.344240779876708\n",
            "Step 573700, Loss: 9.479884672164918\n",
            "Step 573800, Loss: 9.684124670028687\n",
            "Step 573900, Loss: 9.612708520889282\n",
            "Step 574000, Loss: 9.451017084121704\n",
            "Step 574100, Loss: 9.316241664886475\n",
            "Step 574200, Loss: 9.629978590011596\n",
            "Step 574300, Loss: 9.363426637649535\n",
            "Step 574400, Loss: 9.390997915267944\n",
            "Step 574500, Loss: 9.09853262901306\n",
            "Step 574600, Loss: 9.68492223739624\n",
            "Step 574700, Loss: 9.573683137893676\n",
            "Step 574800, Loss: 9.535245685577392\n",
            "Step 574900, Loss: 9.394001054763795\n",
            "Step 575000, Loss: 9.423086290359498\n",
            "Step 575100, Loss: 9.424699325561523\n",
            "Step 575200, Loss: 9.450611925125122\n",
            "Step 575300, Loss: 9.650309972763061\n",
            "Step 575400, Loss: 9.642875003814698\n",
            "Step 575500, Loss: 9.427681112289429\n",
            "Step 575600, Loss: 9.48231957435608\n",
            "Step 575700, Loss: 9.316101188659667\n",
            "Step 575800, Loss: 9.377862787246704\n",
            "Step 575900, Loss: 9.460292959213257\n",
            "Step 576000, Loss: 9.277315521240235\n",
            "Step 576100, Loss: 9.746465015411378\n",
            "Step 576200, Loss: 9.858520927429199\n",
            "Step 576300, Loss: 9.709853353500366\n",
            "Step 576400, Loss: 9.600567121505737\n",
            "Step 576500, Loss: 9.793495483398438\n",
            "Step 576600, Loss: 9.660207796096802\n",
            "Step 576700, Loss: 9.455000352859496\n",
            "Step 576800, Loss: 9.648267641067505\n",
            "Step 576900, Loss: 9.719126405715942\n",
            "Step 577000, Loss: 9.797313547134399\n",
            "Step 577100, Loss: 10.00620677947998\n",
            "Step 577200, Loss: 10.025955295562744\n",
            "Step 577300, Loss: 10.158982315063476\n",
            "Step 577400, Loss: 9.688360395431518\n",
            "Step 577500, Loss: 10.304707164764404\n",
            "Step 577600, Loss: 10.00445647239685\n",
            "Step 577700, Loss: 9.61366560935974\n",
            "Step 577800, Loss: 10.093693571090698\n",
            "Step 577900, Loss: 10.276088018417358\n",
            "Step 578000, Loss: 9.644776163101197\n",
            "Step 578100, Loss: 9.574703140258789\n",
            "Step 578200, Loss: 9.498059482574464\n",
            "Step 578300, Loss: 9.67515386581421\n",
            "Step 578400, Loss: 9.532967977523803\n",
            "Step 578500, Loss: 9.337346057891846\n",
            "Step 578600, Loss: 10.032564430236816\n",
            "Step 578700, Loss: 9.648996362686157\n",
            "Step 578800, Loss: 9.461999711990357\n",
            "Step 578900, Loss: 9.75646604537964\n",
            "Step 579000, Loss: 9.90786024093628\n",
            "Step 579100, Loss: 9.810668535232544\n",
            "Step 579200, Loss: 9.557676811218261\n",
            "Step 579300, Loss: 9.675323438644408\n",
            "Step 579400, Loss: 9.979935092926025\n",
            "Step 579500, Loss: 9.584944581985473\n",
            "Step 579600, Loss: 9.369555110931396\n",
            "Step 579700, Loss: 9.674164876937866\n",
            "Step 579800, Loss: 9.757803583145142\n",
            "Step 579900, Loss: 9.507299509048462\n",
            "Step 580000, Loss: 9.449067306518554\n",
            "Step 580100, Loss: 9.409619398117066\n",
            "Step 580200, Loss: 9.398146572113037\n",
            "Step 580300, Loss: 9.401868705749513\n",
            "Step 580400, Loss: 9.507689085006714\n",
            "Step 580500, Loss: 9.20536334991455\n",
            "Step 580600, Loss: 9.277177963256836\n",
            "Step 580700, Loss: 9.467105598449708\n",
            "Step 580800, Loss: 9.517795534133912\n",
            "Step 580900, Loss: 9.472756814956664\n",
            "Step 581000, Loss: 9.538763761520386\n",
            "Step 581100, Loss: 9.391377143859863\n",
            "Step 581200, Loss: 9.318360757827758\n",
            "Step 581300, Loss: 9.466649045944214\n",
            "Step 581400, Loss: 9.570937337875366\n",
            "Step 581500, Loss: 9.570688610076905\n",
            "Step 581600, Loss: 9.586572523117065\n",
            "Step 581700, Loss: 9.532993469238281\n",
            "Step 581800, Loss: 9.522389497756958\n",
            "Step 581900, Loss: 9.489258899688721\n",
            "Step 582000, Loss: 9.46676516532898\n",
            "Step 582100, Loss: 9.687195739746095\n",
            "Step 582200, Loss: 9.490818872451783\n",
            "Step 582300, Loss: 9.353642435073853\n",
            "Step 582400, Loss: 9.569455757141114\n",
            "Step 582500, Loss: 9.637863245010376\n",
            "Step 582600, Loss: 9.434273557662964\n",
            "Step 582700, Loss: 9.64817964553833\n",
            "Step 582800, Loss: 9.54860866546631\n",
            "Step 582900, Loss: 9.21912459373474\n",
            "Step 583000, Loss: 9.405078973770141\n",
            "Step 583100, Loss: 9.633142366409302\n",
            "Step 583200, Loss: 9.440220203399658\n",
            "Step 583300, Loss: 9.376231393814088\n",
            "Step 583400, Loss: 9.672805099487304\n",
            "Step 583500, Loss: 9.78269187927246\n",
            "Step 583600, Loss: 9.604016189575196\n",
            "Step 583700, Loss: 9.633843660354614\n",
            "Step 583800, Loss: 9.57333535194397\n",
            "Step 583900, Loss: 9.842563409805297\n",
            "Step 584000, Loss: 9.666507148742676\n",
            "Step 584100, Loss: 10.025642642974853\n",
            "Step 584200, Loss: 9.446285457611085\n",
            "Step 584300, Loss: 9.267794609069824\n",
            "Step 584400, Loss: 9.514036111831665\n",
            "Step 584500, Loss: 9.956791276931762\n",
            "Step 584600, Loss: 9.484508142471313\n",
            "Step 584700, Loss: 9.699957180023194\n",
            "Step 584800, Loss: 9.683589744567872\n",
            "Step 584900, Loss: 9.85954153060913\n",
            "Step 585000, Loss: 9.313813667297364\n",
            "Step 585100, Loss: 10.205738754272462\n",
            "Step 585200, Loss: 9.727385358810425\n",
            "Step 585300, Loss: 9.4738973903656\n",
            "Step 585400, Loss: 9.632829418182373\n",
            "Step 585500, Loss: 9.662907857894897\n",
            "Step 585600, Loss: 9.525904693603515\n",
            "Step 585700, Loss: 9.63113733291626\n",
            "Step 585800, Loss: 9.366717824935913\n",
            "Step 585900, Loss: 9.397076768875122\n",
            "Step 586000, Loss: 9.31939787864685\n",
            "Step 586100, Loss: 9.15670298576355\n",
            "Step 586200, Loss: 9.636848955154418\n",
            "Step 586300, Loss: 9.301826305389405\n",
            "Step 586400, Loss: 9.511038627624512\n",
            "Step 586500, Loss: 9.639849681854248\n",
            "Step 586600, Loss: 9.283961267471314\n",
            "Step 586700, Loss: 9.183603076934814\n",
            "Step 586800, Loss: 9.54605881690979\n",
            "Step 586900, Loss: 9.437798023223877\n",
            "Step 587000, Loss: 9.536202430725098\n",
            "Step 587100, Loss: 9.456237821578979\n",
            "Step 587200, Loss: 9.27781735420227\n",
            "Step 587300, Loss: 9.317491788864135\n",
            "Step 587400, Loss: 9.40992525100708\n",
            "Step 587500, Loss: 9.667131099700928\n",
            "Step 587600, Loss: 9.669375267028808\n",
            "Step 587700, Loss: 9.716799325942993\n",
            "Step 587800, Loss: 9.659263458251953\n",
            "Step 587900, Loss: 9.622837371826172\n",
            "Step 588000, Loss: 9.58374493598938\n",
            "Step 588100, Loss: 9.601292695999145\n",
            "Step 588200, Loss: 10.306453437805176\n",
            "epoch no  15\n",
            "Step 588300, Loss: 9.904611463546752\n",
            "Step 588400, Loss: 9.624763803482056\n",
            "Step 588500, Loss: 9.637928247451782\n",
            "Step 588600, Loss: 9.516813516616821\n",
            "Step 588700, Loss: 9.418552503585815\n",
            "Step 588800, Loss: 9.637634000778199\n",
            "Step 588900, Loss: 9.566231117248535\n",
            "Step 589000, Loss: 9.672686805725098\n",
            "Step 589100, Loss: 10.040423889160156\n",
            "Step 589200, Loss: 9.855211143493653\n",
            "Step 589300, Loss: 9.950109367370606\n",
            "Step 589400, Loss: 9.725206899642945\n",
            "Step 589500, Loss: 9.985627393722535\n",
            "Step 589600, Loss: 9.821859169006348\n",
            "Step 589700, Loss: 9.803801383972168\n",
            "Step 589800, Loss: 9.813391275405884\n",
            "Step 589900, Loss: 9.87432755470276\n",
            "Step 590000, Loss: 9.663509826660157\n",
            "Step 590100, Loss: 9.913621320724488\n",
            "Step 590200, Loss: 9.929609422683717\n",
            "Step 590300, Loss: 9.648078145980834\n",
            "Step 590400, Loss: 9.571620283126832\n",
            "Step 590500, Loss: 9.84092137336731\n",
            "Step 590600, Loss: 9.901389350891113\n",
            "Step 590700, Loss: 9.934720010757447\n",
            "Step 590800, Loss: 9.876636209487915\n",
            "Step 590900, Loss: 9.459465112686157\n",
            "Step 591000, Loss: 9.369407787322999\n",
            "Step 591100, Loss: 9.639296598434449\n",
            "Step 591200, Loss: 9.697947874069214\n",
            "Step 591300, Loss: 9.591291179656983\n",
            "Step 591400, Loss: 9.785373182296754\n",
            "Step 591500, Loss: 9.613842935562134\n",
            "Step 591600, Loss: 9.66870771408081\n",
            "Step 591700, Loss: 9.342553033828736\n",
            "Step 591800, Loss: 9.749545211791991\n",
            "Step 591900, Loss: 9.518444290161133\n",
            "Step 592000, Loss: 9.597085628509522\n",
            "Step 592100, Loss: 9.522689533233642\n",
            "Step 592200, Loss: 9.565744724273681\n",
            "Step 592300, Loss: 9.432961492538452\n",
            "Step 592400, Loss: 9.40124656677246\n",
            "Step 592500, Loss: 9.463253383636475\n",
            "Step 592600, Loss: 9.661469974517821\n",
            "Step 592700, Loss: 9.802319898605347\n",
            "Step 592800, Loss: 9.403724870681764\n",
            "Step 592900, Loss: 9.252644710540771\n",
            "Step 593000, Loss: 9.553340606689453\n",
            "Step 593100, Loss: 9.596775159835815\n",
            "Step 593200, Loss: 9.606268472671509\n",
            "Step 593300, Loss: 9.541939945220948\n",
            "Step 593400, Loss: 9.476104793548584\n",
            "Step 593500, Loss: 9.599841117858887\n",
            "Step 593600, Loss: 9.659108800888061\n",
            "Step 593700, Loss: 9.464117164611816\n",
            "Step 593800, Loss: 9.513711156845092\n",
            "Step 593900, Loss: 9.996432361602784\n",
            "Step 594000, Loss: 9.714108180999755\n",
            "Step 594100, Loss: 9.715530757904054\n",
            "Step 594200, Loss: 9.701455783843993\n",
            "Step 594300, Loss: 9.76024269104004\n",
            "Step 594400, Loss: 10.023341941833497\n",
            "Step 594500, Loss: 9.775477743148803\n",
            "Step 594600, Loss: 9.890411052703858\n",
            "Step 594700, Loss: 9.78930341720581\n",
            "Step 594800, Loss: 9.634912252426147\n",
            "Step 594900, Loss: 9.839825086593628\n",
            "Step 595000, Loss: 10.168158712387084\n",
            "Step 595100, Loss: 9.740247621536255\n",
            "Step 595200, Loss: 9.817769451141357\n",
            "Step 595300, Loss: 9.3464430809021\n",
            "Step 595400, Loss: 9.497128582000732\n",
            "Step 595500, Loss: 9.387820739746093\n",
            "Step 595600, Loss: 9.51484248161316\n",
            "Step 595700, Loss: 9.254954137802123\n",
            "Step 595800, Loss: 9.605639095306396\n",
            "Step 595900, Loss: 9.484354238510132\n",
            "Step 596000, Loss: 9.39409680366516\n",
            "Step 596100, Loss: 9.430425119400024\n",
            "Step 596200, Loss: 9.253955879211425\n",
            "Step 596300, Loss: 9.906661415100098\n",
            "Step 596400, Loss: 9.649898681640625\n",
            "Step 596500, Loss: 9.391954784393311\n",
            "Step 596600, Loss: 9.594872617721558\n",
            "Step 596700, Loss: 9.686765966415406\n",
            "Step 596800, Loss: 9.727028875350952\n",
            "Step 596900, Loss: 9.617077808380127\n",
            "Step 597000, Loss: 9.567474060058593\n",
            "Step 597100, Loss: 9.433332223892211\n",
            "Step 597200, Loss: 10.235628290176392\n",
            "Step 597300, Loss: 10.062634477615356\n",
            "Step 597400, Loss: 9.968483114242554\n",
            "Step 597500, Loss: 9.83871922492981\n",
            "Step 597600, Loss: 10.254039096832276\n",
            "Step 597700, Loss: 10.061175003051758\n",
            "Step 597800, Loss: 9.697653131484985\n",
            "Step 597900, Loss: 9.690208559036256\n",
            "Step 598000, Loss: 9.544060211181641\n",
            "Step 598100, Loss: 9.311427536010742\n",
            "Step 598200, Loss: 9.765679407119752\n",
            "Step 598300, Loss: 9.506785984039306\n",
            "Step 598400, Loss: 9.565493764877319\n",
            "Step 598500, Loss: 9.725106401443481\n",
            "Step 598600, Loss: 9.980445146560669\n",
            "Step 598700, Loss: 9.500714836120606\n",
            "Step 598800, Loss: 9.510177011489867\n",
            "Step 598900, Loss: 9.587532119750977\n",
            "Step 599000, Loss: 9.37197286605835\n",
            "Step 599100, Loss: 9.82683780670166\n",
            "Step 599200, Loss: 9.703075742721557\n",
            "Step 599300, Loss: 9.493749361038208\n",
            "Step 599400, Loss: 9.640713310241699\n",
            "Step 599500, Loss: 9.614756288528442\n",
            "Step 599600, Loss: 9.785462169647216\n",
            "Step 599700, Loss: 9.473854732513427\n",
            "Step 599800, Loss: 9.50001914024353\n",
            "Step 599900, Loss: 9.509834833145142\n",
            "Step 600000, Loss: 9.663336353302002\n",
            "Step 600100, Loss: 9.570093755722047\n",
            "Step 600200, Loss: 9.344581394195556\n",
            "Step 600300, Loss: 9.355480394363404\n",
            "Step 600400, Loss: 9.59361915588379\n",
            "Step 600500, Loss: 9.610143938064574\n",
            "Step 600600, Loss: 9.692933683395387\n",
            "Step 600700, Loss: 9.714751577377319\n",
            "Step 600800, Loss: 9.8111767578125\n",
            "Step 600900, Loss: 9.647034893035888\n",
            "Step 601000, Loss: 9.370931644439697\n",
            "Step 601100, Loss: 9.671156997680663\n",
            "Step 601200, Loss: 9.68158432006836\n",
            "Step 601300, Loss: 9.862974214553834\n",
            "Step 601400, Loss: 9.414997787475587\n",
            "Step 601500, Loss: 9.701881484985352\n",
            "Step 601600, Loss: 9.697402200698853\n",
            "Step 601700, Loss: 9.203067541122437\n",
            "Step 601800, Loss: 9.139641103744507\n",
            "Step 601900, Loss: 9.99009017944336\n",
            "Step 602000, Loss: 9.729864101409913\n",
            "Step 602100, Loss: 9.414394493103027\n",
            "Step 602200, Loss: 9.521431341171265\n",
            "Step 602300, Loss: 9.55641305923462\n",
            "Step 602400, Loss: 9.724438753128052\n",
            "Step 602500, Loss: 9.533599767684937\n",
            "Step 602600, Loss: 9.448840284347535\n",
            "Step 602700, Loss: 9.561126880645752\n",
            "Step 602800, Loss: 9.63477644920349\n",
            "Step 602900, Loss: 9.403591661453246\n",
            "Step 603000, Loss: 9.37514591217041\n",
            "Step 603100, Loss: 9.478628578186035\n",
            "Step 603200, Loss: 9.68744878768921\n",
            "Step 603300, Loss: 9.600047464370727\n",
            "Step 603400, Loss: 9.775889091491699\n",
            "Step 603500, Loss: 9.581680316925048\n",
            "Step 603600, Loss: 9.853743677139283\n",
            "Step 603700, Loss: 9.509255743026733\n",
            "Step 603800, Loss: 9.338880701065063\n",
            "Step 603900, Loss: 9.26907880783081\n",
            "Step 604000, Loss: 9.703769245147704\n",
            "Step 604100, Loss: 9.700738582611084\n",
            "Step 604200, Loss: 9.648734674453735\n",
            "Step 604300, Loss: 9.51019694328308\n",
            "Step 604400, Loss: 9.630651607513428\n",
            "Step 604500, Loss: 9.485672121047974\n",
            "Step 604600, Loss: 9.45046727180481\n",
            "Step 604700, Loss: 9.398549690246583\n",
            "Step 604800, Loss: 9.552429332733155\n",
            "Step 604900, Loss: 9.495548362731933\n",
            "Step 605000, Loss: 9.683935089111328\n",
            "Step 605100, Loss: 9.605385971069335\n",
            "Step 605200, Loss: 9.492795705795288\n",
            "Step 605300, Loss: 9.71784252166748\n",
            "Step 605400, Loss: 9.51560941696167\n",
            "Step 605500, Loss: 9.359664611816406\n",
            "Step 605600, Loss: 9.56465612411499\n",
            "Step 605700, Loss: 9.287829151153565\n",
            "Step 605800, Loss: 9.325402841567993\n",
            "Step 605900, Loss: 9.514732847213745\n",
            "Step 606000, Loss: 9.524225444793702\n",
            "Step 606100, Loss: 9.44446891784668\n",
            "Step 606200, Loss: 9.32617868423462\n",
            "Step 606300, Loss: 9.363994197845459\n",
            "Step 606400, Loss: 9.184215726852416\n",
            "Step 606500, Loss: 9.429754028320312\n",
            "Step 606600, Loss: 9.32095869064331\n",
            "Step 606700, Loss: 9.53842357635498\n",
            "Step 606800, Loss: 9.509745121002197\n",
            "Step 606900, Loss: 9.805436868667602\n",
            "Step 607000, Loss: 9.520957345962524\n",
            "Step 607100, Loss: 9.497963790893555\n",
            "Step 607200, Loss: 9.161006469726562\n",
            "Step 607300, Loss: 9.289623174667359\n",
            "Step 607400, Loss: 9.568508195877076\n",
            "Step 607500, Loss: 9.510689306259156\n",
            "Step 607600, Loss: 9.524617729187012\n",
            "Step 607700, Loss: 9.67595272064209\n",
            "Step 607800, Loss: 9.83854944229126\n",
            "Step 607900, Loss: 9.949712200164795\n",
            "Step 608000, Loss: 9.70981442451477\n",
            "Step 608100, Loss: 9.513570032119752\n",
            "Step 608200, Loss: 9.419224090576172\n",
            "Step 608300, Loss: 9.716740188598633\n",
            "Step 608400, Loss: 9.269874467849732\n",
            "Step 608500, Loss: 9.536180047988891\n",
            "Step 608600, Loss: 9.366019296646119\n",
            "Step 608700, Loss: 9.479243021011353\n",
            "Step 608800, Loss: 9.36604437828064\n",
            "Step 608900, Loss: 9.620258655548096\n",
            "Step 609000, Loss: 9.427200708389282\n",
            "Step 609100, Loss: 9.587885084152221\n",
            "Step 609200, Loss: 9.529665517807008\n",
            "Step 609300, Loss: 9.572681007385254\n",
            "Step 609400, Loss: 9.668840417861938\n",
            "Step 609500, Loss: 9.412025508880616\n",
            "Step 609600, Loss: 9.800135374069214\n",
            "Step 609700, Loss: 9.850694770812987\n",
            "Step 609800, Loss: 9.41393931388855\n",
            "Step 609900, Loss: 9.68817684173584\n",
            "Step 610000, Loss: 9.427337989807128\n",
            "Step 610100, Loss: 9.506647291183471\n",
            "Step 610200, Loss: 9.220395879745483\n",
            "Step 610300, Loss: 9.232888374328613\n",
            "Step 610400, Loss: 9.781269607543946\n",
            "Step 610500, Loss: 9.514891967773437\n",
            "Step 610600, Loss: 9.432765026092529\n",
            "Step 610700, Loss: 9.561778059005738\n",
            "Step 610800, Loss: 9.454725122451782\n",
            "Step 610900, Loss: 9.701984910964965\n",
            "Step 611000, Loss: 9.776010961532593\n",
            "Step 611100, Loss: 9.507504501342773\n",
            "Step 611200, Loss: 9.604517230987549\n",
            "Step 611300, Loss: 9.65869709968567\n",
            "Step 611400, Loss: 9.452136144638061\n",
            "Step 611500, Loss: 9.824167766571044\n",
            "Step 611600, Loss: 10.048938512802124\n",
            "Step 611700, Loss: 9.770418577194214\n",
            "Step 611800, Loss: 9.90785490989685\n",
            "Step 611900, Loss: 9.739863691329957\n",
            "Step 612000, Loss: 9.681247081756592\n",
            "Step 612100, Loss: 9.89523045539856\n",
            "Step 612200, Loss: 10.0440065574646\n",
            "Step 612300, Loss: 10.369528579711915\n",
            "Step 612400, Loss: 10.095394010543822\n",
            "Step 612500, Loss: 9.710084419250489\n",
            "Step 612600, Loss: 9.413053026199341\n",
            "Step 612700, Loss: 9.434144592285156\n",
            "Step 612800, Loss: 9.351851091384887\n",
            "Step 612900, Loss: 9.395035409927369\n",
            "Step 613000, Loss: 9.701258897781372\n",
            "Step 613100, Loss: 9.545167894363404\n",
            "Step 613200, Loss: 9.501670446395874\n",
            "Step 613300, Loss: 9.309101047515869\n",
            "Step 613400, Loss: 9.536961708068848\n",
            "Step 613500, Loss: 9.287115879058838\n",
            "Step 613600, Loss: 9.358501796722413\n",
            "Step 613700, Loss: 9.028590154647826\n",
            "Step 613800, Loss: 9.604422645568848\n",
            "Step 613900, Loss: 9.623200359344482\n",
            "Step 614000, Loss: 9.417545280456544\n",
            "Step 614100, Loss: 9.343658218383789\n",
            "Step 614200, Loss: 9.494073820114135\n",
            "Step 614300, Loss: 9.265498104095458\n",
            "Step 614400, Loss: 9.401579456329346\n",
            "Step 614500, Loss: 9.616648988723755\n",
            "Step 614600, Loss: 9.598267002105713\n",
            "Step 614700, Loss: 9.47466402053833\n",
            "Step 614800, Loss: 9.502102661132813\n",
            "Step 614900, Loss: 9.252316579818725\n",
            "Step 615000, Loss: 9.285166397094727\n",
            "Step 615100, Loss: 9.540499029159546\n",
            "Step 615200, Loss: 9.22928189277649\n",
            "Step 615300, Loss: 9.596894960403443\n",
            "Step 615400, Loss: 9.752347793579101\n",
            "Step 615500, Loss: 9.756046962738036\n",
            "Step 615600, Loss: 9.564231691360474\n",
            "Step 615700, Loss: 9.753413343429566\n",
            "Step 615800, Loss: 9.63845248222351\n",
            "Step 615900, Loss: 9.540245637893676\n",
            "Step 616000, Loss: 9.448731718063355\n",
            "Step 616100, Loss: 9.747547082901\n",
            "Step 616200, Loss: 9.656957912445069\n",
            "Step 616300, Loss: 9.994552402496337\n",
            "Step 616400, Loss: 10.000767831802369\n",
            "Step 616500, Loss: 9.961325426101684\n",
            "Step 616600, Loss: 9.7438640499115\n",
            "Step 616700, Loss: 10.267636404037475\n",
            "Step 616800, Loss: 9.905942173004151\n",
            "Step 616900, Loss: 9.749450454711914\n",
            "Step 617000, Loss: 9.891863565444947\n",
            "Step 617100, Loss: 10.27114294052124\n",
            "Step 617200, Loss: 9.71625576019287\n",
            "Step 617300, Loss: 9.440570583343506\n",
            "Step 617400, Loss: 9.540944204330444\n",
            "Step 617500, Loss: 9.589252882003784\n",
            "Step 617600, Loss: 9.496999034881592\n",
            "Step 617700, Loss: 9.270847253799438\n",
            "Step 617800, Loss: 9.887725925445556\n",
            "Step 617900, Loss: 9.715720500946045\n",
            "Step 618000, Loss: 9.404868631362914\n",
            "Step 618100, Loss: 9.661576738357544\n",
            "Step 618200, Loss: 9.735560264587402\n",
            "Step 618300, Loss: 9.916499662399293\n",
            "Step 618400, Loss: 9.511536893844605\n",
            "Step 618500, Loss: 9.670355634689331\n",
            "Step 618600, Loss: 9.880115671157837\n",
            "Step 618700, Loss: 9.566969413757324\n",
            "Step 618800, Loss: 9.380475063323974\n",
            "Step 618900, Loss: 9.561699419021606\n",
            "Step 619000, Loss: 9.665275974273682\n",
            "Step 619100, Loss: 9.460380992889405\n",
            "Step 619200, Loss: 9.51885895729065\n",
            "Step 619300, Loss: 9.287470092773438\n",
            "Step 619400, Loss: 9.290487365722656\n",
            "Step 619500, Loss: 9.389535999298095\n",
            "Step 619600, Loss: 9.402170553207398\n",
            "Step 619700, Loss: 9.250847053527831\n",
            "Step 619800, Loss: 9.198132181167603\n",
            "Step 619900, Loss: 9.40147250175476\n",
            "Step 620000, Loss: 9.4434024810791\n",
            "Step 620100, Loss: 9.407336072921753\n",
            "Step 620200, Loss: 9.51754207611084\n",
            "Step 620300, Loss: 9.39510898590088\n",
            "Step 620400, Loss: 9.298603162765502\n",
            "Step 620500, Loss: 9.315934953689576\n",
            "Step 620600, Loss: 9.595253429412843\n",
            "Step 620700, Loss: 9.374422349929809\n",
            "Step 620800, Loss: 9.61059676170349\n",
            "Step 620900, Loss: 9.436307229995727\n",
            "Step 621000, Loss: 9.560422477722168\n",
            "Step 621100, Loss: 9.225557804107666\n",
            "Step 621200, Loss: 9.54883360862732\n",
            "Step 621300, Loss: 9.564312763214112\n",
            "Step 621400, Loss: 9.484227933883666\n",
            "Step 621500, Loss: 9.306766595840454\n",
            "Step 621600, Loss: 9.324012746810913\n",
            "Step 621700, Loss: 9.660418338775635\n",
            "Step 621800, Loss: 9.370958786010743\n",
            "Step 621900, Loss: 9.64589207649231\n",
            "Step 622000, Loss: 9.606303443908692\n",
            "Step 622100, Loss: 9.180109977722168\n",
            "Step 622200, Loss: 9.252248153686523\n",
            "Step 622300, Loss: 9.608372793197631\n",
            "Step 622400, Loss: 9.487949209213257\n",
            "Step 622500, Loss: 9.24836365699768\n",
            "Step 622600, Loss: 9.55584168434143\n",
            "Step 622700, Loss: 9.753439998626709\n",
            "Step 622800, Loss: 9.457466611862182\n",
            "Step 622900, Loss: 9.5394095993042\n",
            "Step 623000, Loss: 9.512197608947753\n",
            "Step 623100, Loss: 9.77905294418335\n",
            "Step 623200, Loss: 9.702498579025269\n",
            "Step 623300, Loss: 9.882509479522705\n",
            "Step 623400, Loss: 9.441566848754883\n",
            "Step 623500, Loss: 9.236150970458985\n",
            "Step 623600, Loss: 9.431214408874512\n",
            "Step 623700, Loss: 9.928986892700195\n",
            "Step 623800, Loss: 9.434637031555177\n",
            "Step 623900, Loss: 9.676571865081787\n",
            "Step 624000, Loss: 9.572393302917481\n",
            "Step 624100, Loss: 9.707735347747803\n",
            "Step 624200, Loss: 9.40450779914856\n",
            "Step 624300, Loss: 10.073461952209472\n",
            "Step 624400, Loss: 9.629010248184205\n",
            "Step 624500, Loss: 9.50015284538269\n",
            "Step 624600, Loss: 9.559862365722656\n",
            "Step 624700, Loss: 9.577908153533935\n",
            "Step 624800, Loss: 9.612618799209594\n",
            "Step 624900, Loss: 9.565372772216797\n",
            "Step 625000, Loss: 9.294118175506592\n",
            "Step 625100, Loss: 9.273303213119506\n",
            "Step 625200, Loss: 9.3036998462677\n",
            "Step 625300, Loss: 9.141540031433106\n",
            "Step 625400, Loss: 9.4447061252594\n",
            "Step 625500, Loss: 9.383464279174804\n",
            "Step 625600, Loss: 9.411666593551637\n",
            "Step 625700, Loss: 9.656853971481324\n",
            "Step 625800, Loss: 9.251242513656615\n",
            "Step 625900, Loss: 9.160537405014038\n",
            "Step 626000, Loss: 9.34355544090271\n",
            "Step 626100, Loss: 9.534999322891235\n",
            "Step 626200, Loss: 9.410325765609741\n",
            "Step 626300, Loss: 9.518108034133911\n",
            "Step 626400, Loss: 9.171269006729126\n",
            "Step 626500, Loss: 9.285211448669434\n",
            "Step 626600, Loss: 9.329413795471192\n",
            "Step 626700, Loss: 9.591557397842408\n",
            "Step 626800, Loss: 9.617078332901\n",
            "Step 626900, Loss: 9.598995628356933\n",
            "Step 627000, Loss: 9.669387950897217\n",
            "Step 627100, Loss: 9.556080570220947\n",
            "Step 627200, Loss: 9.500948877334595\n",
            "Step 627300, Loss: 9.472808122634888\n",
            "Step 627400, Loss: 10.290795612335206\n",
            "epoch no  16\n",
            "Step 627500, Loss: 9.997292547225952\n",
            "Step 627600, Loss: 9.55289821624756\n",
            "Step 627700, Loss: 9.614761724472046\n",
            "Step 627800, Loss: 9.52334234237671\n",
            "Step 627900, Loss: 9.379580354690551\n",
            "Step 628000, Loss: 9.480620107650758\n",
            "Step 628100, Loss: 9.556008634567261\n",
            "Step 628200, Loss: 9.584773359298707\n",
            "Step 628300, Loss: 9.9720334815979\n",
            "Step 628400, Loss: 9.747047328948975\n",
            "Step 628500, Loss: 9.953185157775879\n",
            "Step 628600, Loss: 9.749392976760864\n",
            "Step 628700, Loss: 9.897199831008912\n",
            "Step 628800, Loss: 9.926333208084106\n",
            "Step 628900, Loss: 9.784039669036865\n",
            "Step 629000, Loss: 9.67679204940796\n",
            "Step 629100, Loss: 9.791693258285523\n",
            "Step 629200, Loss: 9.718727598190307\n",
            "Step 629300, Loss: 9.766934251785278\n",
            "Step 629400, Loss: 9.89862398147583\n",
            "Step 629500, Loss: 9.539185953140258\n",
            "Step 629600, Loss: 9.65989909172058\n",
            "Step 629700, Loss: 9.67553207397461\n",
            "Step 629800, Loss: 9.866853494644165\n",
            "Step 629900, Loss: 9.933737668991089\n",
            "Step 630000, Loss: 9.876149673461914\n",
            "Step 630100, Loss: 9.468843107223512\n",
            "Step 630200, Loss: 9.306083364486694\n",
            "Step 630300, Loss: 9.471874599456788\n",
            "Step 630400, Loss: 9.689725580215454\n",
            "Step 630500, Loss: 9.584460716247559\n",
            "Step 630600, Loss: 9.644248781204224\n",
            "Step 630700, Loss: 9.590915699005127\n",
            "Step 630800, Loss: 9.659276533126832\n",
            "Step 630900, Loss: 9.367453155517579\n",
            "Step 631000, Loss: 9.618264102935791\n",
            "Step 631100, Loss: 9.516338167190552\n",
            "Step 631200, Loss: 9.5436612033844\n",
            "Step 631300, Loss: 9.49875452041626\n",
            "Step 631400, Loss: 9.42818278312683\n",
            "Step 631500, Loss: 9.446398983001709\n",
            "Step 631600, Loss: 9.326282577514649\n",
            "Step 631700, Loss: 9.344160575866699\n",
            "Step 631800, Loss: 9.624596719741822\n",
            "Step 631900, Loss: 9.80815830230713\n",
            "Step 632000, Loss: 9.398957757949828\n",
            "Step 632100, Loss: 9.199231605529786\n",
            "Step 632200, Loss: 9.504834117889404\n",
            "Step 632300, Loss: 9.51676404953003\n",
            "Step 632400, Loss: 9.588998365402222\n",
            "Step 632500, Loss: 9.49958878517151\n",
            "Step 632600, Loss: 9.460863800048829\n",
            "Step 632700, Loss: 9.451374950408935\n",
            "Step 632800, Loss: 9.719488582611085\n",
            "Step 632900, Loss: 9.385804510116577\n",
            "Step 633000, Loss: 9.431757755279541\n",
            "Step 633100, Loss: 9.961716365814208\n",
            "Step 633200, Loss: 9.670452661514283\n",
            "Step 633300, Loss: 9.674711227416992\n",
            "Step 633400, Loss: 9.674366998672486\n",
            "Step 633500, Loss: 9.72435422897339\n",
            "Step 633600, Loss: 9.978818540573121\n",
            "Step 633700, Loss: 9.691722993850709\n",
            "Step 633800, Loss: 9.811637935638428\n",
            "Step 633900, Loss: 9.881199989318848\n",
            "Step 634000, Loss: 9.57630841255188\n",
            "Step 634100, Loss: 9.711626482009887\n",
            "Step 634200, Loss: 10.040390405654907\n",
            "Step 634300, Loss: 9.857399864196777\n",
            "Step 634400, Loss: 9.815733709335326\n",
            "Step 634500, Loss: 9.28914547920227\n",
            "Step 634600, Loss: 9.464548597335815\n",
            "Step 634700, Loss: 9.327747640609742\n",
            "Step 634800, Loss: 9.576230812072755\n",
            "Step 634900, Loss: 9.239781665802003\n",
            "Step 635000, Loss: 9.39590524673462\n",
            "Step 635100, Loss: 9.475258264541626\n",
            "Step 635200, Loss: 9.345558252334595\n",
            "Step 635300, Loss: 9.399905500411988\n",
            "Step 635400, Loss: 9.224026498794556\n",
            "Step 635500, Loss: 9.739529962539672\n",
            "Step 635600, Loss: 9.651724128723144\n",
            "Step 635700, Loss: 9.331535482406617\n",
            "Step 635800, Loss: 9.541565284729003\n",
            "Step 635900, Loss: 9.589198989868164\n",
            "Step 636000, Loss: 9.711355085372924\n",
            "Step 636100, Loss: 9.622608528137206\n",
            "Step 636200, Loss: 9.472259254455567\n",
            "Step 636300, Loss: 9.388627347946167\n",
            "Step 636400, Loss: 10.204744501113892\n",
            "Step 636500, Loss: 9.90045835494995\n",
            "Step 636600, Loss: 9.961583318710327\n",
            "Step 636700, Loss: 9.813331232070922\n",
            "Step 636800, Loss: 10.191395816802979\n",
            "Step 636900, Loss: 10.012001361846924\n",
            "Step 637000, Loss: 9.602938041687011\n",
            "Step 637100, Loss: 9.642740182876587\n",
            "Step 637200, Loss: 9.551175756454468\n",
            "Step 637300, Loss: 9.335896949768067\n",
            "Step 637400, Loss: 9.686273355484008\n",
            "Step 637500, Loss: 9.527717800140381\n",
            "Step 637600, Loss: 9.412054271697999\n",
            "Step 637700, Loss: 9.633589429855347\n",
            "Step 637800, Loss: 9.977145519256592\n",
            "Step 637900, Loss: 9.521352138519287\n",
            "Step 638000, Loss: 9.450358228683472\n",
            "Step 638100, Loss: 9.473571310043335\n",
            "Step 638200, Loss: 9.38035545349121\n",
            "Step 638300, Loss: 9.683908739089965\n",
            "Step 638400, Loss: 9.74456916809082\n",
            "Step 638500, Loss: 9.399069023132324\n",
            "Step 638600, Loss: 9.566852731704712\n",
            "Step 638700, Loss: 9.631165428161621\n",
            "Step 638800, Loss: 9.710128841400147\n",
            "Step 638900, Loss: 9.487987384796142\n",
            "Step 639000, Loss: 9.425108718872071\n",
            "Step 639100, Loss: 9.4854691696167\n",
            "Step 639200, Loss: 9.578518419265746\n",
            "Step 639300, Loss: 9.626046342849731\n",
            "Step 639400, Loss: 9.289944829940795\n",
            "Step 639500, Loss: 9.289252281188965\n",
            "Step 639600, Loss: 9.520823135375977\n",
            "Step 639700, Loss: 9.516697645187378\n",
            "Step 639800, Loss: 9.67690176963806\n",
            "Step 639900, Loss: 9.692780799865723\n",
            "Step 640000, Loss: 9.71855902671814\n",
            "Step 640100, Loss: 9.65286449432373\n",
            "Step 640200, Loss: 9.356740570068359\n",
            "Step 640300, Loss: 9.466662664413452\n",
            "Step 640400, Loss: 9.617521800994872\n",
            "Step 640500, Loss: 9.883545618057251\n",
            "Step 640600, Loss: 9.44141222000122\n",
            "Step 640700, Loss: 9.657311906814575\n",
            "Step 640800, Loss: 9.694158172607422\n",
            "Step 640900, Loss: 9.182193593978882\n",
            "Step 641000, Loss: 9.137311983108521\n",
            "Step 641100, Loss: 9.915763072967529\n",
            "Step 641200, Loss: 9.663235559463502\n",
            "Step 641300, Loss: 9.332696857452392\n",
            "Step 641400, Loss: 9.457736167907715\n",
            "Step 641500, Loss: 9.486473121643066\n",
            "Step 641600, Loss: 9.70195761680603\n",
            "Step 641700, Loss: 9.511587829589844\n",
            "Step 641800, Loss: 9.455482091903686\n",
            "Step 641900, Loss: 9.457505807876586\n",
            "Step 642000, Loss: 9.526462879180908\n",
            "Step 642100, Loss: 9.466447257995606\n",
            "Step 642200, Loss: 9.322144079208375\n",
            "Step 642300, Loss: 9.440134963989259\n",
            "Step 642400, Loss: 9.641181201934815\n",
            "Step 642500, Loss: 9.574884433746337\n",
            "Step 642600, Loss: 9.64707116127014\n",
            "Step 642700, Loss: 9.642279672622681\n",
            "Step 642800, Loss: 9.723434495925904\n",
            "Step 642900, Loss: 9.506700782775878\n",
            "Step 643000, Loss: 9.446857347488404\n",
            "Step 643100, Loss: 9.1842910861969\n",
            "Step 643200, Loss: 9.57185899734497\n",
            "Step 643300, Loss: 9.728309202194215\n",
            "Step 643500, Loss: 9.40954083442688\n",
            "Step 643600, Loss: 9.589330158233643\n",
            "Step 643700, Loss: 9.492833757400513\n",
            "Step 643800, Loss: 9.386562604904174\n",
            "Step 643900, Loss: 9.3883882522583\n",
            "Step 644000, Loss: 9.500783281326294\n",
            "Step 644100, Loss: 9.394770565032958\n",
            "Step 644200, Loss: 9.58445026397705\n",
            "Step 644300, Loss: 9.543065299987793\n",
            "Step 644400, Loss: 9.490875272750854\n",
            "Step 644500, Loss: 9.630264377593994\n",
            "Step 644600, Loss: 9.50905385017395\n",
            "Step 644700, Loss: 9.282223072052002\n",
            "Step 644800, Loss: 9.499086284637452\n",
            "Step 644900, Loss: 9.29878620147705\n",
            "Step 645000, Loss: 9.250538110733032\n",
            "Step 645100, Loss: 9.487628498077392\n",
            "Step 645200, Loss: 9.449759159088135\n",
            "Step 645300, Loss: 9.437927646636963\n",
            "Step 645400, Loss: 9.269237661361695\n",
            "Step 645500, Loss: 9.327749652862549\n",
            "Step 645600, Loss: 9.175934677124024\n",
            "Step 645700, Loss: 9.312657270431519\n",
            "Step 645800, Loss: 9.316800956726075\n",
            "Step 645900, Loss: 9.542958221435548\n",
            "Step 646000, Loss: 9.438716602325439\n",
            "Step 646100, Loss: 9.781731595993042\n",
            "Step 646200, Loss: 9.531890258789062\n",
            "Step 646300, Loss: 9.467146940231324\n",
            "Step 646400, Loss: 9.072441682815551\n",
            "Step 646500, Loss: 9.187523746490479\n",
            "Step 646600, Loss: 9.464195528030395\n",
            "Step 646700, Loss: 9.440741634368896\n",
            "Step 646800, Loss: 9.574965829849242\n",
            "Step 646900, Loss: 9.623003883361816\n",
            "Step 647000, Loss: 9.644132633209228\n",
            "Step 647100, Loss: 9.940657224655151\n",
            "Step 647200, Loss: 9.784011764526367\n",
            "Step 647300, Loss: 9.41940185546875\n",
            "Step 647400, Loss: 9.37551157951355\n",
            "Step 647500, Loss: 9.674138832092286\n",
            "Step 647600, Loss: 9.29825361251831\n",
            "Step 647700, Loss: 9.437924947738647\n",
            "Step 647800, Loss: 9.32060516357422\n",
            "Step 647900, Loss: 9.349507369995116\n",
            "Step 648000, Loss: 9.362392835617065\n",
            "Step 648100, Loss: 9.571361780166626\n",
            "Step 648200, Loss: 9.378742322921752\n",
            "Step 648300, Loss: 9.526687488555908\n",
            "Step 648400, Loss: 9.379213180541992\n",
            "Step 648500, Loss: 9.610016746520996\n",
            "Step 648600, Loss: 9.609342441558837\n",
            "Step 648700, Loss: 9.465331707000733\n",
            "Step 648800, Loss: 9.646279678344726\n",
            "Step 648900, Loss: 9.85370174407959\n",
            "Step 649000, Loss: 9.39288703918457\n",
            "Step 649100, Loss: 9.653962059020996\n",
            "Step 649200, Loss: 9.376260643005372\n",
            "Step 649300, Loss: 9.404265804290771\n",
            "Step 649400, Loss: 9.321856889724732\n",
            "Step 649500, Loss: 9.12855878829956\n",
            "Step 649600, Loss: 9.77346076965332\n",
            "Step 649700, Loss: 9.405432558059692\n",
            "Step 649800, Loss: 9.43260362625122\n",
            "Step 649900, Loss: 9.349911069869995\n",
            "Step 650000, Loss: 9.59808650970459\n",
            "Step 650100, Loss: 9.555629596710205\n",
            "Step 650200, Loss: 9.72670672416687\n",
            "Step 650300, Loss: 9.509496669769288\n",
            "Step 650400, Loss: 9.41264428138733\n",
            "Step 650500, Loss: 9.713859024047851\n",
            "Step 650600, Loss: 9.331682786941528\n",
            "Step 650700, Loss: 9.75473892211914\n",
            "Step 650800, Loss: 10.007891731262207\n",
            "Step 650900, Loss: 9.74890832901001\n",
            "Step 651000, Loss: 9.755949897766113\n",
            "Step 651100, Loss: 9.773200731277466\n",
            "Step 651200, Loss: 9.61303994178772\n",
            "Step 651300, Loss: 9.875795421600342\n",
            "Step 651400, Loss: 9.86806890487671\n",
            "Step 651500, Loss: 10.343925132751465\n",
            "Step 651600, Loss: 10.002363195419312\n",
            "Step 651700, Loss: 9.914647960662842\n",
            "Step 651800, Loss: 9.27537218093872\n",
            "Step 651900, Loss: 9.328915023803711\n",
            "Step 652000, Loss: 9.33853705406189\n",
            "Step 652100, Loss: 9.291760940551757\n",
            "Step 652200, Loss: 9.645606050491333\n",
            "Step 652300, Loss: 9.472031755447388\n",
            "Step 652400, Loss: 9.531830263137817\n",
            "Step 652500, Loss: 9.243900566101074\n",
            "Step 652600, Loss: 9.456554803848267\n",
            "Step 652700, Loss: 9.28837851524353\n",
            "Step 652800, Loss: 9.352158737182616\n",
            "Step 652900, Loss: 9.01413498878479\n",
            "Step 653000, Loss: 9.429132928848267\n",
            "Step 653100, Loss: 9.642968978881836\n",
            "Step 653200, Loss: 9.431967458724976\n",
            "Step 653300, Loss: 9.22612434387207\n",
            "Step 653400, Loss: 9.453451194763183\n",
            "Step 653500, Loss: 9.231720695495605\n",
            "Step 653600, Loss: 9.333494825363159\n",
            "Step 653700, Loss: 9.514965114593506\n",
            "Step 653800, Loss: 9.631341342926026\n",
            "Step 653900, Loss: 9.482746686935425\n",
            "Step 654000, Loss: 9.310822229385376\n",
            "Step 654100, Loss: 9.22781714439392\n",
            "Step 654200, Loss: 9.341473140716552\n",
            "Step 654300, Loss: 9.513918924331666\n",
            "Step 654400, Loss: 9.113579196929932\n",
            "Step 654500, Loss: 9.429913206100464\n",
            "Step 654600, Loss: 9.730713720321654\n",
            "Step 654700, Loss: 9.757335195541382\n",
            "Step 654800, Loss: 9.56134792327881\n",
            "Step 654900, Loss: 9.678708066940308\n",
            "Step 655000, Loss: 9.624126720428468\n",
            "Step 655100, Loss: 9.484911031723023\n",
            "Step 655200, Loss: 9.361703443527222\n",
            "Step 655300, Loss: 9.736296310424805\n",
            "Step 655400, Loss: 9.491200914382935\n",
            "Step 655500, Loss: 9.99062089920044\n",
            "Step 655600, Loss: 9.957092523574829\n",
            "Step 655700, Loss: 9.91418948173523\n",
            "Step 655800, Loss: 9.767782087326049\n",
            "Step 655900, Loss: 10.112618207931519\n",
            "Step 656000, Loss: 9.899468145370484\n",
            "Step 656100, Loss: 9.794698600769044\n",
            "Step 656200, Loss: 9.800856256484986\n",
            "Step 656300, Loss: 10.287238235473632\n",
            "Step 656400, Loss: 9.766300315856933\n",
            "Step 656500, Loss: 9.237406854629517\n",
            "Step 656600, Loss: 9.559209337234497\n",
            "Step 656700, Loss: 9.495715112686158\n",
            "Step 656800, Loss: 9.422414588928223\n",
            "Step 656900, Loss: 9.314026250839234\n",
            "Step 657000, Loss: 9.73490327835083\n",
            "Step 657100, Loss: 9.725997438430786\n",
            "Step 657200, Loss: 9.33427487373352\n",
            "Step 657300, Loss: 9.543231296539307\n",
            "Step 657400, Loss: 9.728957643508911\n",
            "Step 657500, Loss: 9.8660821723938\n",
            "Step 657600, Loss: 9.442342977523804\n",
            "Step 657700, Loss: 9.597318429946899\n",
            "Step 657800, Loss: 9.795595111846923\n",
            "Step 657900, Loss: 9.72248285293579\n",
            "Step 658000, Loss: 9.285898275375366\n",
            "Step 658100, Loss: 9.4579008102417\n",
            "Step 658200, Loss: 9.649664125442506\n",
            "Step 658300, Loss: 9.430330266952515\n",
            "Step 658400, Loss: 9.508108596801758\n",
            "Step 658500, Loss: 9.243623476028443\n",
            "Step 658600, Loss: 9.25335609436035\n",
            "Step 658700, Loss: 9.356688861846925\n",
            "Step 658800, Loss: 9.32837176322937\n",
            "Step 658900, Loss: 9.179141788482665\n",
            "Step 659000, Loss: 9.191022205352784\n",
            "Step 659100, Loss: 9.289559116363526\n",
            "Step 659200, Loss: 9.389855813980102\n",
            "Step 659300, Loss: 9.37661413192749\n",
            "Step 659400, Loss: 9.376757097244262\n",
            "Step 659500, Loss: 9.364584474563598\n",
            "Step 659600, Loss: 9.313426780700684\n",
            "Step 659700, Loss: 9.235964307785034\n",
            "Step 659800, Loss: 9.526283664703369\n",
            "Step 659900, Loss: 9.298842363357544\n",
            "Step 660000, Loss: 9.590952262878417\n",
            "Step 660100, Loss: 9.34446354866028\n",
            "Step 660200, Loss: 9.604749994277954\n",
            "Step 660300, Loss: 9.197257738113404\n",
            "Step 660400, Loss: 9.57065793991089\n",
            "Step 660500, Loss: 9.460682067871094\n",
            "Step 660600, Loss: 9.537739410400391\n",
            "Step 660700, Loss: 9.261975364685059\n",
            "Step 660800, Loss: 9.248179426193238\n",
            "Step 660900, Loss: 9.59914164543152\n",
            "Step 661000, Loss: 9.337296810150146\n",
            "Step 661100, Loss: 9.50377016067505\n",
            "Step 661200, Loss: 9.64560691833496\n",
            "Step 661300, Loss: 9.207091026306152\n",
            "Step 661400, Loss: 9.069531660079956\n",
            "Step 661500, Loss: 9.602182931900025\n",
            "Step 661600, Loss: 9.488641595840454\n",
            "Step 661700, Loss: 9.136579761505127\n",
            "Step 661800, Loss: 9.41393123626709\n",
            "Step 661900, Loss: 9.8028622341156\n",
            "Step 662000, Loss: 9.489376029968263\n",
            "Step 662100, Loss: 9.408926858901978\n",
            "Step 662200, Loss: 9.454045391082763\n",
            "Step 662300, Loss: 9.670639114379883\n",
            "Step 662400, Loss: 9.714215450286865\n",
            "Step 662500, Loss: 9.77141194343567\n",
            "Step 662600, Loss: 9.51024200439453\n",
            "Step 662700, Loss: 9.117161321640015\n",
            "Step 662800, Loss: 9.434679079055787\n",
            "Step 662900, Loss: 9.748654317855834\n",
            "Step 663000, Loss: 9.54131757736206\n",
            "Step 663100, Loss: 9.5332257938385\n",
            "Step 663200, Loss: 9.409464387893676\n",
            "Step 663300, Loss: 9.695218725204468\n",
            "Step 663400, Loss: 9.501237697601319\n",
            "Step 663500, Loss: 9.835275287628173\n",
            "Step 663600, Loss: 9.700999202728271\n",
            "Step 663700, Loss: 9.48109185218811\n",
            "Step 663800, Loss: 9.50397292137146\n",
            "Step 663900, Loss: 9.500940866470337\n",
            "Step 664000, Loss: 9.548303003311156\n",
            "Step 664100, Loss: 9.500285549163818\n",
            "Step 664200, Loss: 9.347902116775513\n",
            "Step 664300, Loss: 9.100964345932006\n",
            "Step 664400, Loss: 9.34008041381836\n",
            "Step 664500, Loss: 9.079534158706664\n",
            "Step 664600, Loss: 9.261437196731567\n",
            "Step 664700, Loss: 9.44405080795288\n",
            "Step 664800, Loss: 9.277111978530884\n",
            "Step 664900, Loss: 9.592541341781617\n",
            "Step 665000, Loss: 9.2687593460083\n",
            "Step 665100, Loss: 9.172920665740968\n",
            "Step 665200, Loss: 9.232019271850586\n",
            "Step 665300, Loss: 9.518932657241821\n",
            "Step 665400, Loss: 9.232381744384766\n",
            "Step 665500, Loss: 9.554758853912354\n",
            "Step 665600, Loss: 9.096254291534423\n",
            "Step 665700, Loss: 9.22472882270813\n",
            "Step 665800, Loss: 9.328621835708619\n",
            "Step 665900, Loss: 9.452150659561157\n",
            "Step 666000, Loss: 9.605091781616212\n",
            "Step 666100, Loss: 9.50252058982849\n",
            "Step 666200, Loss: 9.698833303451538\n",
            "Step 666300, Loss: 9.362818241119385\n",
            "Step 666400, Loss: 9.51957631111145\n",
            "Step 666500, Loss: 9.38669669151306\n",
            "Step 666600, Loss: 10.12930983543396\n",
            "epoch no  17\n",
            "Step 666700, Loss: 10.008739995956422\n",
            "Step 666800, Loss: 9.522508373260498\n",
            "Step 666900, Loss: 9.562282476425171\n",
            "Step 667000, Loss: 9.520591650009155\n",
            "Step 667100, Loss: 9.213256149291992\n",
            "Step 667200, Loss: 9.4793971824646\n",
            "Step 667300, Loss: 9.510296058654784\n",
            "Step 667400, Loss: 9.60742826461792\n",
            "Step 667500, Loss: 9.812903871536255\n",
            "Step 667600, Loss: 9.700757322311402\n",
            "Step 667700, Loss: 9.948075981140137\n",
            "Step 667800, Loss: 9.759398231506347\n",
            "Step 667900, Loss: 9.677824983596802\n",
            "Step 668000, Loss: 9.834864702224731\n",
            "Step 668100, Loss: 9.855052270889281\n",
            "Step 668200, Loss: 9.641138429641723\n",
            "Step 668300, Loss: 9.74635066986084\n",
            "Step 668400, Loss: 9.595852270126343\n",
            "Step 668500, Loss: 9.826478033065795\n",
            "Step 668600, Loss: 9.810758390426635\n",
            "Step 668700, Loss: 9.504081449508668\n",
            "Step 668800, Loss: 9.6806245803833\n",
            "Step 668900, Loss: 9.529279365539551\n",
            "Step 669000, Loss: 9.89080945968628\n",
            "Step 669100, Loss: 9.840551414489745\n",
            "Step 669200, Loss: 9.724450130462646\n",
            "Step 669300, Loss: 9.524862432479859\n",
            "Step 669400, Loss: 9.289949741363525\n",
            "Step 669500, Loss: 9.35921986579895\n",
            "Step 669600, Loss: 9.68929326057434\n",
            "Step 669700, Loss: 9.552022905349732\n",
            "Step 669800, Loss: 9.498345623016357\n",
            "Step 669900, Loss: 9.538153915405273\n",
            "Step 670000, Loss: 9.607675657272338\n",
            "Step 670100, Loss: 9.363211860656738\n",
            "Step 670200, Loss: 9.541461992263795\n",
            "Step 670300, Loss: 9.49817837715149\n",
            "Step 670400, Loss: 9.414943561553955\n",
            "Step 670500, Loss: 9.46745400428772\n",
            "Step 670600, Loss: 9.492341165542603\n",
            "Step 670700, Loss: 9.373701276779174\n",
            "Step 670800, Loss: 9.257390089035034\n",
            "Step 670900, Loss: 9.176902446746826\n",
            "Step 671000, Loss: 9.62653447151184\n",
            "Step 671100, Loss: 9.746406421661376\n",
            "Step 671200, Loss: 9.41092206954956\n",
            "Step 671300, Loss: 9.205995855331421\n",
            "Step 671400, Loss: 9.353127708435059\n",
            "Step 671500, Loss: 9.502906169891357\n",
            "Step 671600, Loss: 9.52225757598877\n",
            "Step 671700, Loss: 9.429258604049682\n",
            "Step 671800, Loss: 9.378966264724731\n",
            "Step 671900, Loss: 9.397673864364624\n",
            "Step 672000, Loss: 9.785322971343994\n",
            "Step 672100, Loss: 9.320489234924317\n",
            "Step 672200, Loss: 9.414846181869507\n",
            "Step 672300, Loss: 9.786903953552246\n",
            "Step 672400, Loss: 9.651916418075562\n",
            "Step 672500, Loss: 9.633481740951538\n",
            "Step 672600, Loss: 9.664819526672364\n",
            "Step 672700, Loss: 9.63949818611145\n",
            "Step 672800, Loss: 9.96790472984314\n",
            "Step 672900, Loss: 9.663037567138671\n",
            "Step 673000, Loss: 9.705429124832154\n",
            "Step 673100, Loss: 9.881843013763428\n",
            "Step 673200, Loss: 9.537669076919556\n",
            "Step 673300, Loss: 9.682885150909424\n",
            "Step 673400, Loss: 9.902265214920044\n",
            "Step 673500, Loss: 9.90396466255188\n",
            "Step 673600, Loss: 9.810574111938477\n",
            "Step 673700, Loss: 9.250832204818726\n",
            "Step 673800, Loss: 9.35115252494812\n",
            "Step 673900, Loss: 9.2836421585083\n",
            "Step 674000, Loss: 9.57092303276062\n",
            "Step 674100, Loss: 9.214051399230957\n",
            "Step 674200, Loss: 9.244808177947998\n",
            "Step 674300, Loss: 9.450847415924072\n",
            "Step 674400, Loss: 9.343850803375243\n",
            "Step 674500, Loss: 9.320549449920655\n",
            "Step 674600, Loss: 9.20337282180786\n",
            "Step 674700, Loss: 9.551236114501954\n",
            "Step 674800, Loss: 9.696599750518798\n",
            "Step 674900, Loss: 9.307037630081176\n",
            "Step 675000, Loss: 9.48985899925232\n",
            "Step 675100, Loss: 9.523742189407349\n",
            "Step 675200, Loss: 9.644788942337037\n",
            "Step 675300, Loss: 9.515618534088135\n",
            "Step 675400, Loss: 9.478343496322632\n",
            "Step 675500, Loss: 9.365608863830566\n",
            "Step 675600, Loss: 9.911093654632568\n",
            "Step 675700, Loss: 9.961022233963012\n",
            "Step 675800, Loss: 9.942338047027588\n",
            "Step 675900, Loss: 9.82365909576416\n",
            "Step 676000, Loss: 9.975190486907959\n",
            "Step 676100, Loss: 10.003836221694947\n",
            "Step 676200, Loss: 9.673206968307495\n",
            "Step 676300, Loss: 9.567221450805665\n",
            "Step 676400, Loss: 9.483863706588744\n",
            "Step 676500, Loss: 9.33545521736145\n",
            "Step 676600, Loss: 9.521448383331299\n",
            "Step 676700, Loss: 9.601618700027466\n",
            "Step 676800, Loss: 9.26383201599121\n",
            "Step 676900, Loss: 9.620114336013794\n",
            "Step 677000, Loss: 9.899766693115234\n",
            "Step 677100, Loss: 9.507247323989867\n",
            "Step 677200, Loss: 9.284992179870606\n",
            "Step 677300, Loss: 9.528309154510499\n",
            "Step 677400, Loss: 9.334331483840943\n",
            "Step 677500, Loss: 9.562423267364501\n",
            "Step 677600, Loss: 9.752746267318726\n",
            "Step 677700, Loss: 9.392301721572876\n",
            "Step 677800, Loss: 9.505897617340088\n",
            "Step 677900, Loss: 9.536485080718995\n",
            "Step 678000, Loss: 9.656612939834595\n",
            "Step 678100, Loss: 9.499040279388428\n",
            "Step 678200, Loss: 9.303196506500244\n",
            "Step 678300, Loss: 9.458867912292481\n",
            "Step 678400, Loss: 9.47927345275879\n",
            "Step 678500, Loss: 9.593479099273681\n",
            "Step 678600, Loss: 9.326236000061035\n",
            "Step 678700, Loss: 9.165107870101929\n",
            "Step 678800, Loss: 9.513221969604492\n",
            "Step 678900, Loss: 9.42466272354126\n",
            "Step 679000, Loss: 9.65372543334961\n",
            "Step 679100, Loss: 9.601955347061157\n",
            "Step 679200, Loss: 9.655568046569824\n",
            "Step 679300, Loss: 9.697033309936524\n",
            "Step 679400, Loss: 9.36250244140625\n",
            "Step 679500, Loss: 9.368700428009033\n",
            "Step 679600, Loss: 9.604945983886719\n",
            "Step 679700, Loss: 9.804852018356323\n",
            "Step 679800, Loss: 9.445446681976318\n",
            "Step 679900, Loss: 9.560677471160888\n",
            "Step 680000, Loss: 9.664770193099976\n",
            "Step 680100, Loss: 9.151086559295655\n",
            "Step 680200, Loss: 9.150318746566773\n",
            "Step 680300, Loss: 9.730616703033448\n",
            "Step 680400, Loss: 9.688270301818848\n",
            "Step 680500, Loss: 9.310398321151734\n",
            "Step 680600, Loss: 9.340726861953735\n",
            "Step 680700, Loss: 9.438036108016968\n",
            "Step 680800, Loss: 9.562809410095214\n",
            "Step 680900, Loss: 9.570572366714478\n",
            "Step 681000, Loss: 9.295034971237182\n",
            "Step 681100, Loss: 9.42925449371338\n",
            "Step 681200, Loss: 9.492635660171509\n",
            "Step 681300, Loss: 9.470787878036498\n",
            "Step 681400, Loss: 9.227419319152832\n",
            "Step 681500, Loss: 9.380792484283447\n",
            "Step 681600, Loss: 9.518407697677612\n",
            "Step 681700, Loss: 9.52259690284729\n",
            "Step 681800, Loss: 9.62244927406311\n",
            "Step 681900, Loss: 9.584745969772339\n",
            "Step 682000, Loss: 9.651605281829834\n",
            "Step 682100, Loss: 9.494203243255615\n",
            "Step 682200, Loss: 9.430794715881348\n",
            "Step 682300, Loss: 9.215430669784546\n",
            "Step 682400, Loss: 9.403755722045899\n",
            "Step 682500, Loss: 9.689471855163575\n",
            "Step 682600, Loss: 9.489461383819581\n",
            "Step 682700, Loss: 9.328147716522217\n",
            "Step 682800, Loss: 9.552472734451294\n",
            "Step 682900, Loss: 9.471402702331543\n",
            "Step 683000, Loss: 9.30390793800354\n",
            "Step 683100, Loss: 9.383311462402343\n",
            "Step 683200, Loss: 9.422898750305176\n",
            "Step 683300, Loss: 9.305885543823242\n",
            "Step 683400, Loss: 9.512556896209716\n",
            "Step 683500, Loss: 9.616798686981202\n",
            "Step 683600, Loss: 9.419130678176879\n",
            "Step 683700, Loss: 9.57712700843811\n",
            "Step 683800, Loss: 9.490953063964843\n",
            "Step 683900, Loss: 9.275528755187988\n",
            "Step 684000, Loss: 9.363191928863525\n",
            "Step 684100, Loss: 9.285915794372558\n",
            "Step 684200, Loss: 9.30881594657898\n",
            "Step 684300, Loss: 9.389510021209716\n",
            "Step 684400, Loss: 9.417546224594116\n",
            "Step 684500, Loss: 9.428798599243164\n",
            "Step 684600, Loss: 9.172289514541626\n",
            "Step 684700, Loss: 9.249807567596436\n",
            "Step 684800, Loss: 9.1382275390625\n",
            "Step 684900, Loss: 9.23105920791626\n",
            "Step 685000, Loss: 9.286738815307617\n",
            "Step 685100, Loss: 9.367308530807495\n",
            "Step 685200, Loss: 9.360882644653321\n",
            "Step 685300, Loss: 9.70395911216736\n",
            "Step 685400, Loss: 9.61747088432312\n",
            "Step 685500, Loss: 9.368565216064454\n",
            "Step 685600, Loss: 9.038262977600098\n",
            "Step 685700, Loss: 9.183688259124756\n",
            "Step 685800, Loss: 9.384216709136963\n",
            "Step 685900, Loss: 9.318002519607544\n",
            "Step 686000, Loss: 9.564146041870117\n",
            "Step 686100, Loss: 9.553315362930299\n",
            "Step 686200, Loss: 9.537296438217163\n",
            "Step 686300, Loss: 9.837578172683715\n",
            "Step 686400, Loss: 9.859365797042846\n",
            "Step 686500, Loss: 9.396177473068237\n",
            "Step 686600, Loss: 9.240407485961914\n",
            "Step 686700, Loss: 9.549762048721313\n",
            "Step 686800, Loss: 9.436193170547485\n",
            "Step 686900, Loss: 9.258151330947875\n",
            "Step 687000, Loss: 9.32429705619812\n",
            "Step 687100, Loss: 9.253997812271118\n",
            "Step 687200, Loss: 9.369777727127076\n",
            "Step 687300, Loss: 9.480601358413697\n",
            "Step 687400, Loss: 9.352865495681762\n",
            "Step 687500, Loss: 9.449853162765503\n",
            "Step 687600, Loss: 9.303608665466308\n",
            "Step 687700, Loss: 9.564519357681274\n",
            "Step 687800, Loss: 9.454593992233276\n",
            "Step 687900, Loss: 9.48710615158081\n",
            "Step 688000, Loss: 9.542171325683594\n",
            "Step 688100, Loss: 9.902649679183959\n",
            "Step 688200, Loss: 9.267779731750489\n",
            "Step 688300, Loss: 9.582571725845337\n",
            "Step 688400, Loss: 9.348239259719849\n",
            "Step 688500, Loss: 9.367805414199829\n",
            "Step 688600, Loss: 9.303078746795654\n",
            "Step 688700, Loss: 9.045419397354125\n",
            "Step 688800, Loss: 9.56555281639099\n",
            "Step 688900, Loss: 9.443748188018798\n",
            "Step 689000, Loss: 9.453586473464966\n",
            "Step 689100, Loss: 9.264455938339234\n",
            "Step 689200, Loss: 9.582014656066894\n",
            "Step 689300, Loss: 9.423939008712768\n",
            "Step 689400, Loss: 9.699524917602538\n",
            "Step 689500, Loss: 9.366359004974365\n",
            "Step 689600, Loss: 9.453346757888793\n",
            "Step 689700, Loss: 9.697270612716675\n",
            "Step 689800, Loss: 9.305187044143677\n",
            "Step 689900, Loss: 9.62583758354187\n",
            "Step 690000, Loss: 9.885584592819214\n",
            "Step 690100, Loss: 9.804541234970094\n",
            "Step 690200, Loss: 9.624214458465577\n",
            "Step 690300, Loss: 9.750205898284912\n",
            "Step 690400, Loss: 9.503152513504029\n",
            "Step 690500, Loss: 9.87249074935913\n",
            "Step 690600, Loss: 9.659877882003784\n",
            "Step 690700, Loss: 10.294009437561035\n",
            "Step 690800, Loss: 10.102342929840088\n",
            "Step 690900, Loss: 9.925326652526856\n",
            "Step 691000, Loss: 9.244995260238648\n",
            "Step 691100, Loss: 9.276272859573364\n",
            "Step 691200, Loss: 9.351894683837891\n",
            "Step 691300, Loss: 9.198041143417358\n",
            "Step 691400, Loss: 9.484393472671508\n",
            "Step 691500, Loss: 9.506517143249512\n",
            "Step 691600, Loss: 9.529252119064331\n",
            "Step 691700, Loss: 9.174495153427124\n",
            "Step 691800, Loss: 9.363252563476562\n",
            "Step 691900, Loss: 9.307071695327759\n",
            "Step 692000, Loss: 9.407862682342529\n",
            "Step 692100, Loss: 8.97165780067444\n",
            "Step 692200, Loss: 9.271577196121216\n",
            "Step 692300, Loss: 9.528302030563355\n",
            "Step 692400, Loss: 9.389835624694824\n",
            "Step 692500, Loss: 9.2612677192688\n",
            "Step 692600, Loss: 9.38126522064209\n",
            "Step 692700, Loss: 9.15124578475952\n",
            "Step 692800, Loss: 9.25301097869873\n",
            "Step 692900, Loss: 9.4599742603302\n",
            "Step 693000, Loss: 9.493598413467407\n",
            "Step 693100, Loss: 9.485127792358398\n",
            "Step 693200, Loss: 9.211920824050903\n",
            "Step 693300, Loss: 9.284710302352906\n",
            "Step 693400, Loss: 9.247668190002441\n",
            "Step 693500, Loss: 9.462882890701295\n",
            "Step 693600, Loss: 9.019709119796753\n",
            "Step 693700, Loss: 9.315456762313843\n",
            "Step 693800, Loss: 9.664387893676757\n",
            "Step 693900, Loss: 9.766240224838256\n",
            "Step 694000, Loss: 9.480256195068359\n",
            "Step 694100, Loss: 9.582722568511963\n",
            "Step 694200, Loss: 9.561243648529052\n",
            "Step 694300, Loss: 9.453548069000243\n",
            "Step 694400, Loss: 9.247698373794556\n",
            "Step 694500, Loss: 9.70133918762207\n",
            "Step 694600, Loss: 9.448666229248047\n",
            "Step 694700, Loss: 9.883924160003662\n",
            "Step 694800, Loss: 9.880508842468261\n",
            "Step 694900, Loss: 9.841481494903565\n",
            "Step 695000, Loss: 9.875593242645264\n",
            "Step 695100, Loss: 9.916274814605712\n",
            "Step 695200, Loss: 9.898219347000122\n",
            "Step 695300, Loss: 9.80915065765381\n",
            "Step 695400, Loss: 9.520420036315919\n",
            "Step 695500, Loss: 10.227065925598145\n",
            "Step 695600, Loss: 9.764019508361816\n",
            "Step 695700, Loss: 9.214330196380615\n",
            "Step 695800, Loss: 9.552703018188476\n",
            "Step 695900, Loss: 9.326202421188354\n",
            "Step 696000, Loss: 9.414326887130738\n",
            "Step 696100, Loss: 9.290243940353394\n",
            "Step 696200, Loss: 9.565835485458374\n",
            "Step 696300, Loss: 9.710593671798707\n",
            "Step 696400, Loss: 9.371712369918823\n",
            "Step 696500, Loss: 9.418408288955689\n",
            "Step 696600, Loss: 9.686873846054077\n",
            "Step 696700, Loss: 9.788480577468873\n",
            "Step 696800, Loss: 9.462307262420655\n",
            "Step 696900, Loss: 9.477623281478882\n",
            "Step 697000, Loss: 9.69111762046814\n",
            "Step 697100, Loss: 9.811414299011231\n",
            "Step 697200, Loss: 9.243745174407959\n",
            "Step 697300, Loss: 9.364330949783325\n",
            "Step 697400, Loss: 9.599404106140137\n",
            "Step 697500, Loss: 9.413977193832398\n",
            "Step 697600, Loss: 9.487895469665528\n",
            "Step 697700, Loss: 9.18710373878479\n",
            "Step 697800, Loss: 9.235501022338868\n",
            "Step 697900, Loss: 9.355651168823242\n",
            "Step 698000, Loss: 9.192805700302124\n",
            "Step 698100, Loss: 9.290123739242553\n",
            "Step 698200, Loss: 9.028428649902343\n",
            "Step 698300, Loss: 9.206345081329346\n",
            "Step 698400, Loss: 9.382308406829834\n",
            "Step 698500, Loss: 9.38357635498047\n",
            "Step 698600, Loss: 9.236811189651489\n",
            "Step 698700, Loss: 9.382223796844482\n",
            "Step 698800, Loss: 9.243823175430299\n",
            "Step 698900, Loss: 9.201681270599366\n",
            "Step 699000, Loss: 9.403107147216797\n",
            "Step 699100, Loss: 9.238979463577271\n",
            "Step 699200, Loss: 9.491750917434693\n",
            "Step 699300, Loss: 9.381624269485474\n",
            "Step 699400, Loss: 9.510399961471558\n",
            "Step 699500, Loss: 9.170620317459106\n",
            "Step 699600, Loss: 9.485918083190917\n",
            "Step 699700, Loss: 9.33692708015442\n",
            "Step 699800, Loss: 9.442498683929443\n",
            "Step 699900, Loss: 9.326702575683594\n",
            "Step 700000, Loss: 9.150912046432495\n",
            "Step 700100, Loss: 9.498084335327148\n",
            "Step 700200, Loss: 9.377183809280396\n",
            "Step 700300, Loss: 9.452007265090943\n",
            "Step 700400, Loss: 9.542218656539918\n",
            "Step 700500, Loss: 9.195844211578368\n",
            "Step 700600, Loss: 8.959882469177247\n",
            "Step 700700, Loss: 9.493609943389892\n",
            "Step 700800, Loss: 9.585849199295044\n",
            "Step 700900, Loss: 9.076971893310548\n",
            "Step 701000, Loss: 9.287699594497681\n",
            "Step 701100, Loss: 9.656852779388428\n",
            "Step 701200, Loss: 9.511312036514282\n",
            "Step 701300, Loss: 9.35170060157776\n",
            "Step 701400, Loss: 9.436873455047607\n",
            "Step 701500, Loss: 9.50869849205017\n",
            "Step 701600, Loss: 9.699066658020019\n",
            "Step 701700, Loss: 9.649756555557252\n",
            "Step 701800, Loss: 9.580590543746949\n",
            "Step 701900, Loss: 9.143689079284668\n",
            "Step 702000, Loss: 9.242732028961182\n",
            "Step 702100, Loss: 9.66190315246582\n",
            "Step 702200, Loss: 9.594543409347533\n",
            "Step 702300, Loss: 9.418642320632934\n",
            "Step 702400, Loss: 9.385480794906616\n",
            "Step 702500, Loss: 9.631950759887696\n",
            "Step 702600, Loss: 9.514372539520263\n",
            "Step 702700, Loss: 9.66437346458435\n",
            "Step 702800, Loss: 9.69688780784607\n",
            "Step 702900, Loss: 9.540743026733399\n",
            "Step 703000, Loss: 9.312512702941895\n",
            "Step 703100, Loss: 9.522481670379639\n",
            "Step 703200, Loss: 9.39582064628601\n",
            "Step 703300, Loss: 9.454764289855957\n",
            "Step 703400, Loss: 9.374863157272339\n",
            "Step 703500, Loss: 9.067213916778565\n",
            "Step 703600, Loss: 9.28769826889038\n",
            "Step 703700, Loss: 9.101417455673218\n",
            "Step 703800, Loss: 9.111539936065673\n",
            "Step 703900, Loss: 9.436123781204223\n",
            "Step 704000, Loss: 9.20043134689331\n",
            "Step 704100, Loss: 9.53822787284851\n",
            "Step 704200, Loss: 9.248767871856689\n",
            "Step 704300, Loss: 9.08102819442749\n",
            "Step 704400, Loss: 9.135076608657837\n",
            "Step 704500, Loss: 9.437085552215576\n",
            "Step 704600, Loss: 9.115901021957397\n",
            "Step 704700, Loss: 9.54389015197754\n",
            "Step 704800, Loss: 9.130320682525635\n",
            "Step 704900, Loss: 9.195635280609132\n",
            "Step 705000, Loss: 9.221753768920898\n",
            "Step 705100, Loss: 9.288635549545289\n",
            "Step 705200, Loss: 9.57826343536377\n",
            "Step 705300, Loss: 9.454821691513061\n",
            "Step 705400, Loss: 9.66650915145874\n",
            "Step 705500, Loss: 9.287268171310425\n",
            "Step 705600, Loss: 9.508711585998535\n",
            "Step 705700, Loss: 9.372739219665528\n",
            "Step 705800, Loss: 9.818610401153565\n",
            "Step 705900, Loss: 10.138284225463867\n",
            "epoch no  18\n",
            "Step 706000, Loss: 9.467064199447632\n",
            "Step 706100, Loss: 9.550287818908691\n",
            "Step 706200, Loss: 9.430275030136109\n",
            "Step 706300, Loss: 9.200969467163086\n",
            "Step 706400, Loss: 9.45992413520813\n",
            "Step 706500, Loss: 9.487613668441773\n",
            "Step 706600, Loss: 9.588248929977418\n",
            "Step 706700, Loss: 9.578308982849121\n",
            "Step 706800, Loss: 9.723814897537231\n",
            "Step 706900, Loss: 9.862210893630982\n",
            "Step 707000, Loss: 9.741975870132446\n",
            "Step 707100, Loss: 9.626844129562379\n",
            "Step 707200, Loss: 9.800214557647704\n",
            "Step 707300, Loss: 9.804959020614625\n",
            "Step 707400, Loss: 9.711533803939819\n",
            "Step 707500, Loss: 9.687446603775024\n",
            "Step 707600, Loss: 9.593759679794312\n",
            "Step 707700, Loss: 9.66630012512207\n",
            "Step 707800, Loss: 9.843219919204712\n",
            "Step 707900, Loss: 9.477356510162354\n",
            "Step 708000, Loss: 9.676566181182862\n",
            "Step 708100, Loss: 9.388628311157227\n",
            "Step 708200, Loss: 10.008504467010498\n",
            "Step 708300, Loss: 9.745462207794189\n",
            "Step 708400, Loss: 9.596341753005982\n",
            "Step 708500, Loss: 9.543022241592407\n",
            "Step 708600, Loss: 9.22576003074646\n",
            "Step 708700, Loss: 9.24353624343872\n",
            "Step 708800, Loss: 9.64134552001953\n",
            "Step 708900, Loss: 9.54083620071411\n",
            "Step 709000, Loss: 9.44174404144287\n",
            "Step 709100, Loss: 9.570607986450195\n",
            "Step 709200, Loss: 9.525939588546754\n",
            "Step 709300, Loss: 9.374305486679077\n",
            "Step 709400, Loss: 9.410888929367065\n",
            "Step 709500, Loss: 9.485258293151855\n",
            "Step 709600, Loss: 9.380403461456298\n",
            "Step 709700, Loss: 9.369934453964234\n",
            "Step 709800, Loss: 9.47493782043457\n",
            "Step 709900, Loss: 9.34730541229248\n",
            "Step 710000, Loss: 9.2073495388031\n",
            "Step 710100, Loss: 9.124166536331177\n",
            "Step 710200, Loss: 9.547725667953491\n",
            "Step 710300, Loss: 9.661925344467162\n",
            "Step 710400, Loss: 9.46155743598938\n",
            "Step 710500, Loss: 9.180178413391113\n",
            "Step 710600, Loss: 9.24286021232605\n",
            "Step 710700, Loss: 9.420368213653564\n",
            "Step 710800, Loss: 9.514435863494873\n",
            "Step 710900, Loss: 9.403332328796386\n",
            "Step 711000, Loss: 9.36595890045166\n",
            "Step 711100, Loss: 9.337463035583497\n",
            "Step 711200, Loss: 9.666798229217529\n",
            "Step 711300, Loss: 9.317148418426514\n",
            "Step 711400, Loss: 9.423191833496094\n",
            "Step 711500, Loss: 9.570318937301636\n",
            "Step 711600, Loss: 9.682318744659424\n",
            "Step 711700, Loss: 9.59727110862732\n",
            "Step 711800, Loss: 9.584510164260864\n",
            "Step 711900, Loss: 9.53293532371521\n",
            "Step 712000, Loss: 9.882640295028686\n",
            "Step 712100, Loss: 9.668907861709595\n",
            "Step 712200, Loss: 9.633210134506225\n",
            "Step 712300, Loss: 9.811002883911133\n",
            "Step 712400, Loss: 9.47486273765564\n",
            "Step 712500, Loss: 9.582548332214355\n",
            "Step 712600, Loss: 9.848227672576904\n",
            "Step 712700, Loss: 9.979407930374146\n",
            "Step 712800, Loss: 9.67377721786499\n",
            "Step 712900, Loss: 9.31149172782898\n",
            "Step 713000, Loss: 9.243922929763794\n",
            "Step 713100, Loss: 9.292010517120362\n",
            "Step 713200, Loss: 9.508466968536377\n",
            "Step 713300, Loss: 9.169786529541016\n",
            "Step 713400, Loss: 9.104943141937255\n",
            "Step 713500, Loss: 9.427225141525268\n",
            "Step 713600, Loss: 9.372792978286743\n",
            "Step 713700, Loss: 9.212531204223632\n",
            "Step 713800, Loss: 9.139011631011963\n",
            "Step 713900, Loss: 9.434124937057495\n",
            "Step 714000, Loss: 9.645334300994874\n",
            "Step 714100, Loss: 9.335694351196288\n",
            "Step 714200, Loss: 9.422197580337524\n",
            "Step 714300, Loss: 9.421031246185303\n",
            "Step 714400, Loss: 9.54959789276123\n",
            "Step 714500, Loss: 9.51249773979187\n",
            "Step 714600, Loss: 9.419867811203003\n",
            "Step 714700, Loss: 9.372391061782837\n",
            "Step 714800, Loss: 9.65469072341919\n",
            "Step 714900, Loss: 10.017136268615722\n",
            "Step 715000, Loss: 9.964570512771607\n",
            "Step 715100, Loss: 9.716186485290528\n",
            "Step 715200, Loss: 9.753878774642944\n",
            "Step 715300, Loss: 10.046410036087035\n",
            "Step 715400, Loss: 9.745819368362426\n",
            "Step 715500, Loss: 9.554816474914551\n",
            "Step 715600, Loss: 9.469214153289794\n",
            "Step 715700, Loss: 9.318497467041016\n",
            "Step 715800, Loss: 9.345282068252564\n",
            "Step 715900, Loss: 9.545726346969605\n",
            "Step 716000, Loss: 9.24255241394043\n",
            "Step 716100, Loss: 9.58325930595398\n",
            "Step 716200, Loss: 9.78513123512268\n",
            "Step 716300, Loss: 9.537347068786621\n",
            "Step 716400, Loss: 9.283806581497192\n",
            "Step 716500, Loss: 9.505431518554687\n",
            "Step 716600, Loss: 9.23028615951538\n",
            "Step 716700, Loss: 9.4050847530365\n",
            "Step 716800, Loss: 9.737855529785156\n",
            "Step 716900, Loss: 9.35202938079834\n",
            "Step 717000, Loss: 9.520980148315429\n",
            "Step 717100, Loss: 9.526512546539307\n",
            "Step 717200, Loss: 9.554282302856445\n",
            "Step 717300, Loss: 9.547992734909057\n",
            "Step 717400, Loss: 9.1985493850708\n",
            "Step 717500, Loss: 9.437955923080445\n",
            "Step 717600, Loss: 9.378978843688964\n",
            "Step 717700, Loss: 9.54531488418579\n",
            "Step 717800, Loss: 9.268819007873535\n",
            "Step 717900, Loss: 9.156853427886963\n",
            "Step 718000, Loss: 9.441369066238403\n",
            "Step 718100, Loss: 9.34233717918396\n",
            "Step 718200, Loss: 9.621332120895385\n",
            "Step 718300, Loss: 9.536620769500733\n",
            "Step 718400, Loss: 9.540506267547608\n",
            "Step 718500, Loss: 9.695933418273926\n",
            "Step 718600, Loss: 9.320611515045165\n",
            "Step 718700, Loss: 9.25495059967041\n",
            "Step 718800, Loss: 9.578530664443969\n",
            "Step 718900, Loss: 9.6550284576416\n",
            "Step 719000, Loss: 9.50748917579651\n",
            "Step 719100, Loss: 9.38145094871521\n",
            "Step 719200, Loss: 9.632180700302124\n",
            "Step 719300, Loss: 9.219272155761718\n",
            "Step 719400, Loss: 9.102367849349976\n",
            "Step 719500, Loss: 9.474452228546143\n",
            "Step 719600, Loss: 9.66351453781128\n",
            "Step 719700, Loss: 9.343180904388428\n",
            "Step 719800, Loss: 9.253723955154419\n",
            "Step 719900, Loss: 9.387367868423462\n",
            "Step 720000, Loss: 9.466525964736938\n",
            "Step 720100, Loss: 9.615976104736328\n",
            "Step 720200, Loss: 9.230634326934814\n",
            "Step 720300, Loss: 9.344918613433839\n",
            "Step 720400, Loss: 9.4831316280365\n",
            "Step 720500, Loss: 9.469809217453003\n",
            "Step 720600, Loss: 9.192799987792968\n",
            "Step 720700, Loss: 9.29121777534485\n",
            "Step 720800, Loss: 9.436577939987183\n",
            "Step 720900, Loss: 9.5391366481781\n",
            "Step 721000, Loss: 9.483155765533446\n",
            "Step 721100, Loss: 9.53401623725891\n",
            "Step 721200, Loss: 9.585287036895751\n",
            "Step 721300, Loss: 9.508273658752442\n",
            "Step 721400, Loss: 9.348462476730347\n",
            "Step 721500, Loss: 9.155748634338378\n",
            "Step 721600, Loss: 9.290915746688842\n",
            "Step 721700, Loss: 9.629645738601685\n",
            "Step 721800, Loss: 9.462329530715943\n",
            "Step 721900, Loss: 9.344362592697143\n",
            "Step 722000, Loss: 9.4340953540802\n",
            "Step 722100, Loss: 9.442389087677002\n",
            "Step 722200, Loss: 9.211765480041503\n",
            "Step 722300, Loss: 9.366661701202393\n",
            "Step 722400, Loss: 9.38332992553711\n",
            "Step 722500, Loss: 9.197498931884766\n",
            "Step 722600, Loss: 9.39511731147766\n",
            "Step 722700, Loss: 9.551092748641969\n",
            "Step 722800, Loss: 9.399709911346436\n",
            "Step 722900, Loss: 9.470766582489013\n",
            "Step 723000, Loss: 9.457037763595581\n",
            "Step 723100, Loss: 9.212194442749023\n",
            "Step 723200, Loss: 9.351278247833251\n",
            "Step 723300, Loss: 9.210351543426514\n",
            "Step 723400, Loss: 9.245066061019898\n",
            "Step 723500, Loss: 9.35305235862732\n",
            "Step 723600, Loss: 9.288353033065796\n",
            "Step 723700, Loss: 9.42155436515808\n",
            "Step 723800, Loss: 9.110353260040283\n",
            "Step 723900, Loss: 9.230998439788818\n",
            "Step 724000, Loss: 9.089168310165405\n",
            "Step 724100, Loss: 9.096708393096923\n",
            "Step 724200, Loss: 9.261376495361327\n",
            "Step 724300, Loss: 9.224447736740112\n",
            "Step 724400, Loss: 9.378132648468018\n",
            "Step 724500, Loss: 9.65140793800354\n",
            "Step 724600, Loss: 9.61315453529358\n",
            "Step 724700, Loss: 9.252726707458496\n",
            "Step 724800, Loss: 9.08857837677002\n",
            "Step 724900, Loss: 9.116342210769654\n",
            "Step 725000, Loss: 9.216797037124634\n",
            "Step 725100, Loss: 9.290596475601197\n",
            "Step 725200, Loss: 9.551030797958374\n",
            "Step 725300, Loss: 9.346947040557861\n",
            "Step 725400, Loss: 9.544277076721192\n",
            "Step 725500, Loss: 9.723093919754028\n",
            "Step 725600, Loss: 9.864263906478882\n",
            "Step 725700, Loss: 9.381477813720704\n",
            "Step 725800, Loss: 9.22101146697998\n",
            "Step 725900, Loss: 9.42564148902893\n",
            "Step 726000, Loss: 9.411025915145874\n",
            "Step 726100, Loss: 9.255300064086914\n",
            "Step 726200, Loss: 9.25440414428711\n",
            "Step 726300, Loss: 9.199867877960205\n",
            "Step 726400, Loss: 9.40645791053772\n",
            "Step 726500, Loss: 9.304537086486816\n",
            "Step 726600, Loss: 9.467644882202148\n",
            "Step 726700, Loss: 9.33406611442566\n",
            "Step 726800, Loss: 9.341103000640869\n",
            "Step 726900, Loss: 9.487865657806397\n",
            "Step 727000, Loss: 9.384328260421753\n",
            "Step 727100, Loss: 9.52582815170288\n",
            "Step 727200, Loss: 9.465200386047364\n",
            "Step 727300, Loss: 9.84447096824646\n",
            "Step 727400, Loss: 9.2190966796875\n",
            "Step 727500, Loss: 9.506966142654418\n",
            "Step 727600, Loss: 9.362320232391358\n",
            "Step 727700, Loss: 9.29431721687317\n",
            "Step 727800, Loss: 9.2889515209198\n",
            "Step 727900, Loss: 8.955341539382935\n",
            "Step 728000, Loss: 9.494047622680664\n",
            "Step 728100, Loss: 9.385036220550537\n",
            "Step 728200, Loss: 9.414068899154664\n",
            "Step 728300, Loss: 9.173792152404785\n",
            "Step 728400, Loss: 9.62343108177185\n",
            "Step 728500, Loss: 9.239153900146484\n",
            "Step 728600, Loss: 9.559211683273315\n",
            "Step 728700, Loss: 9.494655838012696\n",
            "Step 728800, Loss: 9.328483743667602\n",
            "Step 728900, Loss: 9.702717571258544\n",
            "Step 729000, Loss: 9.271279315948487\n",
            "Step 729100, Loss: 9.54356367111206\n",
            "Step 729200, Loss: 9.777146921157836\n",
            "Step 729300, Loss: 9.862110652923583\n",
            "Step 729400, Loss: 9.580097084045411\n",
            "Step 729500, Loss: 9.730733156204224\n",
            "Step 729600, Loss: 9.51264458656311\n",
            "Step 729700, Loss: 9.738284788131715\n",
            "Step 729800, Loss: 9.49738576889038\n",
            "Step 729900, Loss: 10.204621686935425\n",
            "Step 730000, Loss: 10.17619592666626\n",
            "Step 730100, Loss: 9.984128589630126\n",
            "Step 730200, Loss: 9.211327257156372\n",
            "Step 730300, Loss: 9.199761514663697\n",
            "Step 730400, Loss: 9.37249361038208\n",
            "Step 730500, Loss: 9.118651494979858\n",
            "Step 730600, Loss: 9.353144397735596\n",
            "Step 730700, Loss: 9.499254808425903\n",
            "Step 730800, Loss: 9.474689273834228\n",
            "Step 730900, Loss: 9.049098854064942\n",
            "Step 731000, Loss: 9.272612905502319\n",
            "Step 731100, Loss: 9.359140462875366\n",
            "Step 731200, Loss: 9.273834371566773\n",
            "Step 731300, Loss: 9.02899453163147\n",
            "Step 731400, Loss: 9.116859254837037\n",
            "Step 731500, Loss: 9.437716102600097\n",
            "Step 731600, Loss: 9.420884733200074\n",
            "Step 731700, Loss: 9.215858526229859\n",
            "Step 731800, Loss: 9.30640604019165\n",
            "Step 731900, Loss: 9.099084253311156\n",
            "Step 732000, Loss: 9.197237510681152\n",
            "Step 732100, Loss: 9.403727626800537\n",
            "Step 732200, Loss: 9.490806550979615\n",
            "Step 732300, Loss: 9.397510099411011\n",
            "Step 732400, Loss: 9.163138818740844\n",
            "Step 732500, Loss: 9.320762243270874\n",
            "Step 732600, Loss: 9.185093622207642\n",
            "Step 732700, Loss: 9.31009446144104\n",
            "Step 732800, Loss: 8.999302263259887\n",
            "Step 732900, Loss: 9.220526504516602\n",
            "Step 733000, Loss: 9.638238859176635\n",
            "Step 733100, Loss: 9.625624380111695\n",
            "Step 733200, Loss: 9.463611898422242\n",
            "Step 733300, Loss: 9.44784068107605\n",
            "Step 733400, Loss: 9.558435077667236\n",
            "Step 733500, Loss: 9.437673816680908\n",
            "Step 733600, Loss: 9.222775535583496\n",
            "Step 733700, Loss: 9.57307918548584\n",
            "Step 733800, Loss: 9.40987247467041\n",
            "Step 733900, Loss: 9.803797197341918\n",
            "Step 734000, Loss: 9.769339723587036\n",
            "Step 734100, Loss: 9.79136842727661\n",
            "Step 734200, Loss: 9.981899604797363\n",
            "Step 734300, Loss: 9.712206211090088\n",
            "Step 734400, Loss: 9.9764821434021\n",
            "Step 734500, Loss: 9.784301080703734\n",
            "Step 734600, Loss: 9.383320131301879\n",
            "Step 734700, Loss: 10.07644268989563\n",
            "Step 734800, Loss: 9.892064723968506\n",
            "Step 734900, Loss: 9.157311916351318\n",
            "Step 735000, Loss: 9.575588579177856\n",
            "Step 735100, Loss: 9.189064664840698\n",
            "Step 735200, Loss: 9.457907047271728\n",
            "Step 735300, Loss: 9.228969039916992\n",
            "Step 735400, Loss: 9.443044748306274\n",
            "Step 735500, Loss: 9.69891004562378\n",
            "Step 735600, Loss: 9.381398677825928\n",
            "Step 735700, Loss: 9.280861854553223\n",
            "Step 735800, Loss: 9.60470350265503\n",
            "Step 735900, Loss: 9.71969066619873\n",
            "Step 736000, Loss: 9.523008871078492\n",
            "Step 736100, Loss: 9.352798185348512\n",
            "Step 736200, Loss: 9.525580263137817\n",
            "Step 736300, Loss: 9.922992610931397\n",
            "Step 736400, Loss: 9.180282402038575\n",
            "Step 736500, Loss: 9.128975820541381\n",
            "Step 736600, Loss: 9.61971088409424\n",
            "Step 736700, Loss: 9.547454214096069\n",
            "Step 736800, Loss: 9.35884087562561\n",
            "Step 736900, Loss: 9.077626552581787\n",
            "Step 737000, Loss: 9.250367527008057\n",
            "Step 737100, Loss: 9.325389127731324\n",
            "Step 737200, Loss: 9.150210027694703\n",
            "Step 737300, Loss: 9.33002872467041\n",
            "Step 737400, Loss: 8.919823083877564\n",
            "Step 737500, Loss: 9.14466835975647\n",
            "Step 737600, Loss: 9.253902368545532\n",
            "Step 737700, Loss: 9.352731685638428\n",
            "Step 737800, Loss: 9.256318035125732\n",
            "Step 737900, Loss: 9.28101432800293\n",
            "Step 738000, Loss: 9.28173454284668\n",
            "Step 738100, Loss: 9.090404644012452\n",
            "Step 738200, Loss: 9.309043464660645\n",
            "Step 738300, Loss: 9.232316856384278\n",
            "Step 738400, Loss: 9.436450452804566\n",
            "Step 738500, Loss: 9.341203451156616\n",
            "Step 738600, Loss: 9.336684160232544\n",
            "Step 738700, Loss: 9.222479591369629\n",
            "Step 738800, Loss: 9.324762296676635\n",
            "Step 738900, Loss: 9.22161759376526\n",
            "Step 739000, Loss: 9.457148876190185\n",
            "Step 739100, Loss: 9.318123655319214\n",
            "Step 739200, Loss: 9.045991735458374\n",
            "Step 739300, Loss: 9.468253431320191\n",
            "Step 739400, Loss: 9.296571836471557\n",
            "Step 739500, Loss: 9.335196495056152\n",
            "Step 739600, Loss: 9.497094898223876\n",
            "Step 739700, Loss: 9.207141237258911\n",
            "Step 739800, Loss: 8.873989686965942\n",
            "Step 739900, Loss: 9.36789384841919\n",
            "Step 740000, Loss: 9.551285848617553\n",
            "Step 740100, Loss: 9.04716326713562\n",
            "Step 740200, Loss: 9.213618421554566\n",
            "Step 740300, Loss: 9.567478866577149\n",
            "Step 740400, Loss: 9.463944816589356\n",
            "Step 740500, Loss: 9.300576829910279\n",
            "Step 740600, Loss: 9.397544584274293\n",
            "Step 740700, Loss: 9.428284835815429\n",
            "Step 740800, Loss: 9.58214991569519\n",
            "Step 740900, Loss: 9.454689617156982\n",
            "Step 741000, Loss: 9.737859535217286\n",
            "Step 741100, Loss: 9.095204572677613\n",
            "Step 741200, Loss: 9.130654668807983\n",
            "Step 741300, Loss: 9.48372465133667\n",
            "Step 741400, Loss: 9.649258451461792\n",
            "Step 741500, Loss: 9.283533411026001\n",
            "Step 741600, Loss: 9.393784627914428\n",
            "Step 741700, Loss: 9.597171611785889\n",
            "Step 741800, Loss: 9.537378377914429\n",
            "Step 741900, Loss: 9.423592376708985\n",
            "Step 742000, Loss: 9.760277605056762\n",
            "Step 742100, Loss: 9.447018251419067\n",
            "Step 742200, Loss: 9.245417976379395\n",
            "Step 742300, Loss: 9.522166910171508\n",
            "Step 742400, Loss: 9.337781629562379\n",
            "Step 742500, Loss: 9.324757976531982\n",
            "Step 742600, Loss: 9.447932786941529\n",
            "Step 742700, Loss: 9.071427755355835\n",
            "Step 742800, Loss: 9.154151277542114\n",
            "Step 742900, Loss: 9.088027858734131\n",
            "Step 743000, Loss: 8.998394222259522\n",
            "Step 743100, Loss: 9.466562414169312\n",
            "Step 743200, Loss: 9.091391944885254\n",
            "Step 743300, Loss: 9.415383634567261\n",
            "Step 743400, Loss: 9.39166955947876\n",
            "Step 743500, Loss: 8.983003988265992\n",
            "Step 743600, Loss: 9.11006181716919\n",
            "Step 743700, Loss: 9.377939109802247\n",
            "Step 743800, Loss: 9.086223649978638\n",
            "Step 743900, Loss: 9.397838373184204\n",
            "Step 744000, Loss: 9.14342056274414\n",
            "Step 744100, Loss: 9.122966117858887\n",
            "Step 744200, Loss: 9.12628541946411\n",
            "Step 744300, Loss: 9.222957315444946\n",
            "Step 744400, Loss: 9.539382362365723\n",
            "Step 744500, Loss: 9.356108360290527\n",
            "Step 744600, Loss: 9.623131437301636\n",
            "Step 744700, Loss: 9.310893859863281\n",
            "Step 744800, Loss: 9.4578865814209\n",
            "Step 744900, Loss: 9.399713249206544\n",
            "Step 745000, Loss: 9.620182428359985\n",
            "Step 745100, Loss: 10.123797435760498\n",
            "epoch no  19\n",
            "Step 745200, Loss: 9.461779451370239\n",
            "Step 745300, Loss: 9.485801000595092\n",
            "Step 745400, Loss: 9.412625522613526\n",
            "Step 745500, Loss: 9.181188659667969\n",
            "Step 745600, Loss: 9.343722915649414\n",
            "Step 745700, Loss: 9.446939878463745\n",
            "Step 745800, Loss: 9.512236166000367\n",
            "Step 745900, Loss: 9.43571807861328\n",
            "Step 746000, Loss: 9.747729959487915\n",
            "Step 746100, Loss: 9.757429513931275\n",
            "Step 746200, Loss: 9.689993925094605\n",
            "Step 746300, Loss: 9.635582513809204\n",
            "Step 746400, Loss: 9.801577091217041\n",
            "Step 746500, Loss: 9.66732346534729\n",
            "Step 746600, Loss: 9.606640901565552\n",
            "Step 746700, Loss: 9.675373830795287\n",
            "Step 746800, Loss: 9.634431848526\n",
            "Step 746900, Loss: 9.60612551689148\n",
            "Step 747000, Loss: 9.751802978515625\n",
            "Step 747100, Loss: 9.616326417922973\n",
            "Step 747200, Loss: 9.476950702667237\n",
            "Step 747300, Loss: 9.426850509643554\n",
            "Step 747400, Loss: 9.885218505859376\n",
            "Step 747500, Loss: 9.666151847839355\n",
            "Step 747600, Loss: 9.639859476089477\n",
            "Step 747700, Loss: 9.58796992301941\n",
            "Step 747800, Loss: 9.154043865203857\n",
            "Step 747900, Loss: 9.21534083366394\n",
            "Step 748000, Loss: 9.512377271652221\n",
            "Step 748100, Loss: 9.526723899841308\n",
            "Step 748200, Loss: 9.415589323043823\n",
            "Step 748300, Loss: 9.5114253616333\n",
            "Step 748400, Loss: 9.44392110824585\n",
            "Step 748500, Loss: 9.373544664382935\n",
            "Step 748600, Loss: 9.30845564842224\n",
            "Step 748700, Loss: 9.491583385467528\n",
            "Step 748800, Loss: 9.316328201293945\n",
            "Step 748900, Loss: 9.345417861938477\n",
            "Step 749000, Loss: 9.4119033908844\n",
            "Step 749100, Loss: 9.303654832839966\n",
            "Step 749200, Loss: 9.17864107131958\n",
            "Step 749300, Loss: 9.065827112197876\n",
            "Step 749400, Loss: 9.419603128433227\n",
            "Step 749500, Loss: 9.482078752517701\n",
            "Step 749600, Loss: 9.581644611358643\n",
            "Step 749700, Loss: 9.135694179534912\n",
            "Step 749800, Loss: 9.165212602615357\n",
            "Step 749900, Loss: 9.37494493484497\n",
            "Step 750000, Loss: 9.42267939567566\n",
            "Step 750100, Loss: 9.331399030685425\n",
            "Step 750200, Loss: 9.383858327865601\n",
            "Step 750300, Loss: 9.249962110519409\n",
            "Step 750400, Loss: 9.57170425415039\n",
            "Step 750500, Loss: 9.309160032272338\n",
            "Step 750600, Loss: 9.335867538452149\n",
            "Step 750700, Loss: 9.371715049743653\n",
            "Step 750800, Loss: 9.749133214950561\n",
            "Step 750900, Loss: 9.50014546394348\n",
            "Step 751000, Loss: 9.501370849609375\n",
            "Step 751100, Loss: 9.528714332580567\n",
            "Step 751200, Loss: 9.74500964164734\n",
            "Step 751300, Loss: 9.706579875946044\n",
            "Step 751400, Loss: 9.620476179122925\n",
            "Step 751500, Loss: 9.7186763381958\n",
            "Step 751600, Loss: 9.484604740142823\n",
            "Step 751700, Loss: 9.454722032546997\n",
            "Step 751800, Loss: 9.653806743621827\n",
            "Step 751900, Loss: 10.074542551040649\n",
            "Step 752000, Loss: 9.575144090652465\n",
            "Step 752100, Loss: 9.39395393371582\n",
            "Step 752200, Loss: 9.132424612045288\n",
            "Step 752300, Loss: 9.255983686447143\n",
            "Step 752400, Loss: 9.410793743133546\n",
            "Step 752500, Loss: 9.200953760147094\n",
            "Step 752600, Loss: 8.996104822158813\n",
            "Step 752700, Loss: 9.409327583312988\n",
            "Step 752800, Loss: 9.339238300323487\n",
            "Step 752900, Loss: 9.122787990570068\n",
            "Step 753000, Loss: 9.101340312957763\n",
            "Step 753100, Loss: 9.296127786636353\n",
            "Step 753200, Loss: 9.670795001983642\n",
            "Step 753300, Loss: 9.297903995513916\n",
            "Step 753400, Loss: 9.33467324256897\n",
            "Step 753500, Loss: 9.349230489730836\n",
            "Step 753600, Loss: 9.508585386276245\n",
            "Step 753700, Loss: 9.441824378967285\n",
            "Step 753800, Loss: 9.384250259399414\n",
            "Step 753900, Loss: 9.420531873703004\n",
            "Step 754000, Loss: 9.407271938323975\n",
            "Step 754100, Loss: 10.067546472549438\n",
            "Step 754200, Loss: 9.88679160118103\n",
            "Step 754300, Loss: 9.710631504058838\n",
            "Step 754400, Loss: 9.575455408096314\n",
            "Step 754500, Loss: 10.069443445205689\n",
            "Step 754600, Loss: 9.800645112991333\n",
            "Step 754700, Loss: 9.451064739227295\n",
            "Step 754800, Loss: 9.450605249404907\n",
            "Step 754900, Loss: 9.332123613357544\n",
            "Step 755000, Loss: 9.196330919265748\n",
            "Step 755100, Loss: 9.474819440841674\n",
            "Step 755200, Loss: 9.236201667785645\n",
            "Step 755300, Loss: 9.439956140518188\n",
            "Step 755400, Loss: 9.667480831146241\n",
            "Step 755500, Loss: 9.635962524414062\n",
            "Step 755600, Loss: 9.141892614364624\n",
            "Step 755700, Loss: 9.438283967971802\n",
            "Step 755800, Loss: 9.289778900146484\n",
            "Step 755900, Loss: 9.259783697128295\n",
            "Step 756000, Loss: 9.66792420387268\n",
            "Step 756100, Loss: 9.40582260131836\n",
            "Step 756200, Loss: 9.341745471954345\n",
            "Step 756300, Loss: 9.576516895294189\n",
            "Step 756400, Loss: 9.395228958129882\n",
            "Step 756500, Loss: 9.59597417831421\n",
            "Step 756600, Loss: 9.126969366073608\n",
            "Step 756700, Loss: 9.341618003845214\n",
            "Step 756800, Loss: 9.334024887084961\n",
            "Step 756900, Loss: 9.522140064239501\n",
            "Step 757000, Loss: 9.294658107757568\n",
            "Step 757100, Loss: 9.084715585708619\n",
            "Step 757200, Loss: 9.353968544006348\n",
            "Step 757300, Loss: 9.291783742904663\n",
            "Step 757400, Loss: 9.546076183319093\n",
            "Step 757500, Loss: 9.53051215171814\n",
            "Step 757600, Loss: 9.408824167251588\n",
            "Step 757700, Loss: 9.705895175933838\n",
            "Step 757800, Loss: 9.376514511108399\n",
            "Step 757900, Loss: 9.211612396240234\n",
            "Step 758000, Loss: 9.525783071517944\n",
            "Step 758100, Loss: 9.497522068023681\n",
            "Step 758200, Loss: 9.558416738510132\n",
            "Step 758300, Loss: 9.320682125091553\n",
            "Step 758400, Loss: 9.56920084953308\n",
            "Step 758500, Loss: 9.259846563339233\n",
            "Step 758600, Loss: 9.091649885177612\n",
            "Step 758700, Loss: 9.175021047592162\n",
            "Step 758800, Loss: 9.753400659561157\n",
            "Step 758900, Loss: 9.378809289932251\n",
            "Step 759000, Loss: 9.16192398071289\n",
            "Step 759100, Loss: 9.37918529510498\n",
            "Step 759200, Loss: 9.292556705474853\n",
            "Step 759300, Loss: 9.630788078308106\n",
            "Step 759400, Loss: 9.203686676025391\n",
            "Step 759500, Loss: 9.203625326156617\n",
            "Step 759600, Loss: 9.432055158615112\n",
            "Step 759700, Loss: 9.418298797607422\n",
            "Step 759800, Loss: 9.13723572731018\n",
            "Step 759900, Loss: 9.276424837112426\n",
            "Step 760000, Loss: 9.290463285446167\n",
            "Step 760100, Loss: 9.562835454940796\n",
            "Step 760200, Loss: 9.378304252624512\n",
            "Step 760300, Loss: 9.52857967376709\n",
            "Step 760400, Loss: 9.50845769882202\n",
            "Step 760500, Loss: 9.501099367141723\n",
            "Step 760600, Loss: 9.29191819190979\n",
            "Step 760700, Loss: 9.11138671875\n",
            "Step 760800, Loss: 9.158561811447143\n",
            "Step 760900, Loss: 9.541171627044678\n",
            "Step 761000, Loss: 9.487064266204834\n",
            "Step 761100, Loss: 9.33842179298401\n",
            "Step 761200, Loss: 9.344507865905761\n",
            "Step 761300, Loss: 9.39271562576294\n",
            "Step 761400, Loss: 9.140715351104737\n",
            "Step 761500, Loss: 9.341361179351807\n",
            "Step 761600, Loss: 9.301045093536377\n",
            "Step 761700, Loss: 9.163198823928832\n",
            "Step 761800, Loss: 9.356396703720092\n",
            "Step 761900, Loss: 9.472477083206178\n",
            "Step 762000, Loss: 9.411661262512206\n",
            "Step 762100, Loss: 9.27251678466797\n",
            "Step 762200, Loss: 9.473497695922852\n",
            "Step 762300, Loss: 9.179968490600587\n",
            "Step 762400, Loss: 9.269051446914673\n",
            "Step 762500, Loss: 9.201434688568115\n",
            "Step 762600, Loss: 9.192676286697388\n",
            "Step 762700, Loss: 9.246666221618652\n",
            "Step 762800, Loss: 9.293178634643555\n",
            "Step 762900, Loss: 9.382927808761597\n",
            "Step 763000, Loss: 9.150250005722047\n",
            "Step 763100, Loss: 9.170167789459228\n",
            "Step 763200, Loss: 9.06676510810852\n",
            "Step 763300, Loss: 9.055123834609985\n",
            "Step 763400, Loss: 9.260264549255371\n",
            "Step 763500, Loss: 9.114041395187378\n",
            "Step 763600, Loss: 9.334545288085938\n",
            "Step 763700, Loss: 9.4779017162323\n",
            "Step 763800, Loss: 9.615997171401977\n",
            "Step 763900, Loss: 9.274404668807984\n",
            "Step 764000, Loss: 9.205244016647338\n",
            "Step 764100, Loss: 8.99082356452942\n",
            "Step 764200, Loss: 9.149709005355835\n",
            "Step 764300, Loss: 9.25542935371399\n",
            "Step 764400, Loss: 9.451673517227173\n",
            "Step 764500, Loss: 9.196673231124878\n",
            "Step 764600, Loss: 9.546609897613525\n",
            "Step 764700, Loss: 9.690686826705933\n",
            "Step 764800, Loss: 9.889321813583374\n",
            "Step 764900, Loss: 9.343423671722412\n",
            "Step 765000, Loss: 9.175608463287354\n",
            "Step 765100, Loss: 9.328851909637452\n",
            "Step 765200, Loss: 9.421019229888916\n",
            "Step 765300, Loss: 9.11894416809082\n",
            "Step 765400, Loss: 9.263160190582276\n",
            "Step 765500, Loss: 9.161610670089722\n",
            "Step 765600, Loss: 9.365341796875\n",
            "Step 765700, Loss: 9.243258724212646\n",
            "Step 765800, Loss: 9.436880111694336\n",
            "Step 765900, Loss: 9.201590213775635\n",
            "Step 766000, Loss: 9.332890996932983\n",
            "Step 766100, Loss: 9.483307428359986\n",
            "Step 766200, Loss: 9.180226564407349\n",
            "Step 766300, Loss: 9.524627342224122\n",
            "Step 766400, Loss: 9.318881530761718\n",
            "Step 766500, Loss: 9.751403837203979\n",
            "Step 766600, Loss: 9.256614036560059\n",
            "Step 766700, Loss: 9.41141872406006\n",
            "Step 766800, Loss: 9.33286467552185\n",
            "Step 766900, Loss: 9.24738374710083\n",
            "Step 767000, Loss: 9.257243099212646\n",
            "Step 767100, Loss: 8.86290470123291\n",
            "Step 767200, Loss: 9.281291007995605\n",
            "Step 767300, Loss: 9.45738639831543\n",
            "Step 767400, Loss: 9.334782524108887\n",
            "Step 767500, Loss: 9.137633838653564\n",
            "Step 767600, Loss: 9.508625144958495\n",
            "Step 767700, Loss: 9.270702219009399\n",
            "Step 767800, Loss: 9.480978775024415\n",
            "Step 767900, Loss: 9.516890268325806\n",
            "Step 768000, Loss: 9.24440118789673\n",
            "Step 768100, Loss: 9.590741233825684\n",
            "Step 768200, Loss: 9.31427348136902\n",
            "Step 768300, Loss: 9.389284467697143\n",
            "Step 768400, Loss: 9.815561256408692\n",
            "Step 768500, Loss: 9.821200571060182\n",
            "Step 768600, Loss: 9.530591745376586\n",
            "Step 768700, Loss: 9.712802896499634\n",
            "Step 768800, Loss: 9.50482084274292\n",
            "Step 768900, Loss: 9.566615982055664\n",
            "Step 769000, Loss: 9.52444896697998\n",
            "Step 769100, Loss: 9.976844825744628\n",
            "Step 769200, Loss: 10.20222939491272\n",
            "Step 769300, Loss: 9.984306659698486\n",
            "Step 769400, Loss: 9.232512531280518\n",
            "Step 769500, Loss: 9.182906618118286\n",
            "Step 769600, Loss: 9.312286005020141\n",
            "Step 769700, Loss: 9.139276704788209\n",
            "Step 769800, Loss: 9.252445211410523\n",
            "Step 769900, Loss: 9.425184812545776\n",
            "Step 770000, Loss: 9.39442632675171\n",
            "Step 770100, Loss: 9.094056587219239\n",
            "Step 770200, Loss: 9.147946462631225\n",
            "Step 770300, Loss: 9.42101330757141\n",
            "Step 770400, Loss: 9.161565017700195\n",
            "Step 770500, Loss: 9.148875226974488\n",
            "Step 770600, Loss: 8.92203893661499\n",
            "Step 770700, Loss: 9.420823163986206\n",
            "Step 770800, Loss: 9.357795810699463\n",
            "Step 770900, Loss: 9.248462896347046\n",
            "Step 771000, Loss: 9.220323028564453\n",
            "Step 771100, Loss: 9.02511526107788\n",
            "Step 771200, Loss: 9.178610010147095\n",
            "Step 771300, Loss: 9.22459137916565\n",
            "Step 771400, Loss: 9.388636169433594\n",
            "Step 771500, Loss: 9.375446157455444\n",
            "Step 771600, Loss: 9.173165140151978\n",
            "Step 771700, Loss: 9.235155210494995\n",
            "Step 771800, Loss: 9.109010610580444\n",
            "Step 771900, Loss: 9.168678255081177\n",
            "Step 772000, Loss: 9.080842180252075\n",
            "Step 772100, Loss: 9.063830881118774\n",
            "Step 772200, Loss: 9.63447060585022\n",
            "Step 772300, Loss: 9.596309394836426\n",
            "Step 772400, Loss: 9.404294509887695\n",
            "Step 772500, Loss: 9.423297023773193\n",
            "Step 772600, Loss: 9.532206888198852\n",
            "Step 772700, Loss: 9.401429042816162\n",
            "Step 772800, Loss: 9.14716558456421\n",
            "Step 772900, Loss: 9.465201196670533\n",
            "Step 773000, Loss: 9.415583324432372\n",
            "Step 773100, Loss: 9.653601665496826\n",
            "Step 773200, Loss: 9.734461908340455\n",
            "Step 773300, Loss: 9.763708066940307\n",
            "Step 773400, Loss: 9.898303775787353\n",
            "Step 773500, Loss: 9.576251411437989\n",
            "Step 773600, Loss: 9.980281553268433\n",
            "Step 773700, Loss: 9.789798564910889\n",
            "Step 773800, Loss: 9.307464179992676\n",
            "Step 773900, Loss: 9.960222072601319\n",
            "Step 774000, Loss: 9.985194520950317\n",
            "Step 774100, Loss: 9.16224033355713\n",
            "Step 774200, Loss: 9.407919054031373\n",
            "Step 774300, Loss: 9.1332985496521\n",
            "Step 774400, Loss: 9.469856672286987\n",
            "Step 774500, Loss: 9.191818256378173\n",
            "Step 774600, Loss: 9.20506142616272\n",
            "Step 774700, Loss: 9.767436256408692\n",
            "Step 774800, Loss: 9.314065189361573\n",
            "Step 774900, Loss: 9.24556933403015\n",
            "Step 775000, Loss: 9.480181245803832\n",
            "Step 775100, Loss: 9.669727010726929\n",
            "Step 775200, Loss: 9.550695600509643\n",
            "Step 775300, Loss: 9.261973867416382\n",
            "Step 775400, Loss: 9.44116798400879\n",
            "Step 775500, Loss: 9.82484395980835\n",
            "Step 775600, Loss: 9.301086444854736\n",
            "Step 775700, Loss: 9.068204612731934\n",
            "Step 775800, Loss: 9.544551105499268\n",
            "Step 775900, Loss: 9.500674390792847\n",
            "Step 776000, Loss: 9.277098569869995\n",
            "Step 776100, Loss: 9.068028030395508\n",
            "Step 776200, Loss: 9.147743282318116\n",
            "Step 776300, Loss: 9.294641952514649\n",
            "Step 776400, Loss: 9.103569650650025\n",
            "Step 776500, Loss: 9.304561080932617\n",
            "Step 776600, Loss: 8.854029960632325\n",
            "Step 776700, Loss: 9.052843389511109\n",
            "Step 776800, Loss: 9.190007390975952\n",
            "Step 776900, Loss: 9.253584814071655\n",
            "Step 777000, Loss: 9.193029680252074\n",
            "Step 777100, Loss: 9.270138339996338\n",
            "Step 777200, Loss: 9.148342933654785\n",
            "Step 777300, Loss: 8.989599370956421\n",
            "Step 777400, Loss: 9.284593467712403\n",
            "Step 777500, Loss: 9.208835029602051\n",
            "Step 777600, Loss: 9.337049522399901\n",
            "Step 777700, Loss: 9.350740518569946\n",
            "Step 777800, Loss: 9.289210739135742\n",
            "Step 777900, Loss: 9.273128423690796\n",
            "Step 778000, Loss: 9.201199722290038\n",
            "Step 778100, Loss: 9.176753454208374\n",
            "Step 778200, Loss: 9.389393863677979\n",
            "Step 778300, Loss: 9.284250631332398\n",
            "Step 778400, Loss: 9.077299919128418\n",
            "Step 778500, Loss: 9.316624326705933\n",
            "Step 778600, Loss: 9.360036859512329\n",
            "Step 778700, Loss: 9.21200174331665\n",
            "Step 778800, Loss: 9.395670509338379\n",
            "Step 778900, Loss: 9.269311532974243\n",
            "Step 779000, Loss: 8.797940607070922\n",
            "Step 779100, Loss: 9.25309302330017\n",
            "Step 779200, Loss: 9.459000549316407\n",
            "Step 779300, Loss: 9.075512561798096\n",
            "Step 779400, Loss: 9.098540811538696\n",
            "Step 779500, Loss: 9.423549575805664\n",
            "Step 779600, Loss: 9.458011293411255\n",
            "Step 779700, Loss: 9.230971336364746\n",
            "Step 779800, Loss: 9.41618779182434\n",
            "Step 779900, Loss: 9.289286432266236\n",
            "Step 780000, Loss: 9.623100719451905\n",
            "Step 780100, Loss: 9.346261548995972\n",
            "Step 780200, Loss: 9.759656209945678\n",
            "Step 780300, Loss: 9.085781049728393\n",
            "Step 780400, Loss: 9.052542982101441\n",
            "Step 780500, Loss: 9.314764938354493\n",
            "Step 780600, Loss: 9.676193294525147\n",
            "Step 780700, Loss: 9.184442501068116\n",
            "Step 780800, Loss: 9.415896224975587\n",
            "Step 780900, Loss: 9.467031717300415\n",
            "Step 781000, Loss: 9.557469701766967\n",
            "Step 781100, Loss: 9.155909357070923\n",
            "Step 781200, Loss: 9.794250202178954\n",
            "Step 781300, Loss: 9.435938158035277\n",
            "Step 781400, Loss: 9.226161184310913\n",
            "Step 781500, Loss: 9.386812353134156\n",
            "Step 781600, Loss: 9.34879147529602\n",
            "Step 781700, Loss: 9.247963352203369\n",
            "Step 781800, Loss: 9.39836314201355\n",
            "Step 781900, Loss: 9.05218825340271\n",
            "Step 782000, Loss: 9.08013726234436\n",
            "Step 782100, Loss: 9.05370325088501\n",
            "Step 782200, Loss: 8.929265174865723\n",
            "Step 782300, Loss: 9.408192167282104\n",
            "Step 782400, Loss: 8.991124687194825\n",
            "Step 782500, Loss: 9.351842823028564\n",
            "Step 782600, Loss: 9.345231838226319\n",
            "Step 782700, Loss: 8.991605892181397\n",
            "Step 782800, Loss: 9.001329784393311\n",
            "Step 782900, Loss: 9.300944108963012\n",
            "Step 783000, Loss: 9.084784908294678\n",
            "Step 783100, Loss: 9.314475622177124\n",
            "Step 783200, Loss: 9.122323379516601\n",
            "Step 783300, Loss: 9.062790393829346\n",
            "Step 783400, Loss: 9.045860843658447\n",
            "Step 783500, Loss: 9.126855154037475\n",
            "Step 783600, Loss: 9.42699715614319\n",
            "Step 783700, Loss: 9.394144792556762\n",
            "Step 783800, Loss: 9.512779922485352\n",
            "Step 783900, Loss: 9.361940984725953\n",
            "Step 784000, Loss: 9.321299629211426\n",
            "Step 784100, Loss: 9.365210990905762\n",
            "Step 784200, Loss: 9.350526313781739\n",
            "Step 784300, Loss: 10.063168048858643\n",
            "Training Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEY_7Zk8SIxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: # y are our logits from the Bengio equation. those logits we must then convert to pseudo probabilities, then normalize via softmax to produce our y_hat.\n",
        "#       y_hat should be (vocabulary length) X (1) in size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MADe0WWDSJBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: calculate perplexity "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z6-hs5MJVaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "def cosine_similarity(wordvec1, wordvec2):\n",
        "    return np.dot(wordvec1, wordvec2) #/ (np.linalg.norm(wordvec1) * np.linalg.norm(wordvec2))\n",
        "\n",
        "def distance(wordvec1, wordvec2):\n",
        "    return (np.linalg.norm(wordvec1 - wordvec2))\n",
        "    \n",
        "def most_similar(word_embedding, word):\n",
        "    \n",
        "    target = word2id[word]\n",
        "    n = 10\n",
        "    top10 = [('', 100000) for i in range(n)]\n",
        "    \n",
        "    \n",
        "    for i in range(len(word_embedding)):\n",
        "        if i == target:\n",
        "            continue\n",
        "        \n",
        "        sim = distance(word_embedding[target], word_embedding[i])\n",
        "        for j in range(n):\n",
        "            if sim <= top10[j][1]:\n",
        "                top10[j+1:] = top10[j:-1]\n",
        "                top10[j] = (vocab[i], sim)\n",
        "                break        \n",
        "    pprint(top10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHn6yfYGM0YI",
        "colab_type": "code",
        "outputId": "b730c987-09dd-44bf-d358-12e2c5d064e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "most_similar(word_embedding, 'the')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('The', 4.3601418),\n",
            " ('of', 5.0125465),\n",
            " (\"'s\", 5.1363564),\n",
            " ('his', 5.5005894),\n",
            " ('with', 5.5602474),\n",
            " ('our', 5.573979),\n",
            " ('great', 5.723778),\n",
            " ('this', 5.7692237),\n",
            " ('their', 6.054073),\n",
            " ('in', 6.118718)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w__jdburJVaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "wordemb_2D = pca.fit_transform(word_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dkNqbb-JVaF",
        "colab_type": "code",
        "outputId": "d1b754df-fd22-4a8c-c41b-d058f6b34dfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "n = 100\n",
        "x_coords = wordemb_2D[:n, 0]\n",
        "y_coords = wordemb_2D[:n, 1]\n",
        "\n",
        "plt.scatter(x_coords, y_coords, c='b')\n",
        "for label, x, y in zip(vocab[:n], x_coords, y_coords):\n",
        "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nySullAfTJK+UUh7s/wND8HAkZGAWmAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqbhPkj_JVaI",
        "colab_type": "code",
        "outputId": "f45a8524-34dd-4408-f3e2-ea9523c04024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "sentences = word2vec.LineSentence(filepath)\n",
        "model = word2vec.Word2Vec(sentences, size=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Nh_SwlJVaK",
        "colab_type": "code",
        "outputId": "4805b042-b831-45e1-da61-5d6360a9a662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "model.wv.most_similar(positive='the')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('every', 0.6385283470153809),\n",
              " ('eternity', 0.6074800491333008),\n",
              " ('its', 0.5911273956298828),\n",
              " ('masonry', 0.5906268358230591),\n",
              " ('entire', 0.5666809678077698),\n",
              " ('America', 0.5587772130966187),\n",
              " ('proceeding', 0.5579907298088074),\n",
              " ('intersection', 0.5565259456634521),\n",
              " ('validity', 0.5466636419296265),\n",
              " ('free', 0.5426400303840637)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}