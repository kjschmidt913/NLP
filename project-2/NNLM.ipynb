{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Copy of NNLM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4aQIHYYJVZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'brown_train.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq9WIOPwAnKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no228r1oJVZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "    \n",
        "def load(filepath, window_size, vocab_size=None):\n",
        "\n",
        "    words = []\n",
        "    with open(filepath, 'r', encoding='utf8') as file:\n",
        "        words = word_tokenize(file.readline())    \n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(len(words) - window_size + 1):\n",
        "        x_train.append(words[i: i + window_size - 1])\n",
        "        y_train.append(words[i +  window_size - 1])\n",
        "    \n",
        "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
        "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
        "    \n",
        "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
        "\n",
        "def load_zh(filepath, window_size, vocab_size=None):\n",
        "\n",
        "    words = []\n",
        "    with open(filepath, 'r', encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            words += word_tokenize(line.strip())\n",
        "        \n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(len(words) - window_size + 1):\n",
        "        x_train.append(words[i: i + window_size - 1])\n",
        "        y_train.append(words[i +  window_size - 1])\n",
        "    \n",
        "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
        "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
        "    \n",
        "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
        "            \n",
        "def convert_to_id(x_train, y_train, vocab):\n",
        "    \n",
        "    word_to_id = {}\n",
        "    for i, vocab in enumerate(vocab):\n",
        "        word_to_id[vocab] = i\n",
        "        \n",
        "    for i in range(len(x_train)):\n",
        "        x_train[i] = [word_to_id[word] for word in x_train[i]]\n",
        "        y_train[i] = word_to_id[y_train[i][0]]\n",
        "        \n",
        "    return x_train.astype(int), y_train.astype(int)\n",
        "\n",
        "\n",
        "def next_batch(x_train, y_train, batch_size):\n",
        "    \n",
        "    num_batch = len(x_train) // batch_size + 1\n",
        "    for n in range(num_batch):        \n",
        "        offset = n * batch_size\n",
        "        x_batch = x_train[offset: offset + batch_size]\n",
        "        y_batch = y_train[offset: offset + batch_size]\n",
        "        \n",
        "        yield x_batch, y_batch\n",
        "        \n",
        "# def convert_to_word(x_train, y_train, id_to_word):\n",
        "#     for i in range(len(x_train)):\n",
        "#         print(x_train[i])\n",
        "#         x_train[i] = id_to_word[x_train[i]]\n",
        "#         y_train[i] = id_to_word[y_train[i]]\n",
        "#     return x_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4WGvLpMJVZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameter\n",
        "# TODO: change to number of batches \n",
        "batch_size = 30\n",
        "# TODO: edit to be less hacky\n",
        "window_size = 6\n",
        "vocab_size = None\n",
        "hidden_size = 50\n",
        "emb_dim = 60\n",
        "learning_rate = 0.5\n",
        "epoch_size = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Had-f-TQJwvI",
        "colab_type": "code",
        "outputId": "08d96de6-2f2b-41df-cc2b-307299cba00d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weiDCYWiL2Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: split into train, valid, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEW5yqMxJVZu",
        "colab_type": "code",
        "outputId": "6e08f5ea-72eb-4aae-edcd-4d71d816dfbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_raw, y_raw, vocab, word2id = load_zh(filepath, window_size, vocab_size)\n",
        "vocab_size = len(vocab)\n",
        "print('vocab_size: {}'.format(vocab_size))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 52945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbWHEqDRJjr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_v = 'brown_valid.txt'\n",
        "x_raw_v, y_raw_v, vocab_v, word2id_v = load_zh(filepath_v, window_size, vocab_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqZ8qrbSJbon",
        "colab_type": "code",
        "outputId": "97799d2f-66ae-470c-a314-7ec061efde94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['<', '>', 'the', ..., 'lurched', 'muddied', 'dogtrot'],\n",
              "      dtype='<U38')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdynygmUJVZx",
        "colab_type": "code",
        "outputId": "211e4f5a-e4c8-4733-f10e-bc3f9c4a8135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# integer representations of vocab\n",
        "x_train, y_train = convert_to_id(x_raw, y_raw, vocab)\n",
        "print('Length: {}'.format(len(x_train)))\n",
        "print('Number of batch: {}'.format(len(x_train) / batch_size))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length: 1321189\n",
            "Number of batch: 44039.63333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfo81jKZJsSw",
        "colab_type": "code",
        "outputId": "99aad571-a5b6-45a1-e480-43a570b3ea63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_valid, y_valid = convert_to_id(x_raw_v, y_raw_v, vocab_v)\n",
        "print('Length: {}'.format(len(x_valid)))\n",
        "print('Number of batch: {}'.format(len(x_valid) / batch_size))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length: 175397\n",
            "Number of batch: 5846.566666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mP2MomyLbD0",
        "colab_type": "code",
        "outputId": "8afc9512-63cb-493b-874d-db398ca3eeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1321189, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfZ3NR_AJVZ0",
        "colab_type": "code",
        "outputId": "11bffb75-6309-4bb4-c9c7-68d49055e899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# import tensorflow as tf\n",
        "# %tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQkPaB6VJVZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Parameter Definition\n",
        "\n",
        "\n",
        "# Input && Output\n",
        "input_words = tf.placeholder(dtype=tf.int32, shape=(batch_size, window_size-1))\n",
        "output_word = tf.placeholder(dtype=tf.int32, shape=(batch_size, 1))\n",
        "\n",
        "\n",
        "# Word Features\n",
        "# word embedding matrix\n",
        "# truncated_normal randomly initializes a matrix of the given shape with values from the normal distribution\n",
        "C = tf.Variable(tf.truncated_normal(shape=(vocab_size, emb_dim), mean=-1, stddev=-1), name='word_embedding')\n",
        "\n",
        "\n",
        "# Hidden Layer Weight && Bias\n",
        "H = tf.Variable(tf.random_normal(shape=(hidden_size, (window_size - 1 ) * emb_dim)))\n",
        "d = tf.Variable(tf.random_normal(shape=(hidden_size, )))\n",
        "\n",
        "# Hidden-to-Output Weight && Bias\n",
        "U = tf.Variable(tf.random_normal(shape=(vocab_size, hidden_size)))\n",
        "b = tf.Variable(tf.random_normal(shape=(vocab_size, )))\n",
        "\n",
        "# Projection-to-Output Weight\n",
        "W = tf.Variable(tf.random_normal(shape=(vocab_size, (window_size - 1) * emb_dim)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSbKQ-12JVZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "64617f09-3b09-4a1a-b8fc-3a8583181ae7"
      },
      "source": [
        "# y = b + Wx + Utanh(d + Hx)\n",
        "\n",
        "# x = (C(w(t-1)), C(w(t-2), ..., C(w(t-n+1))), n == window_size\n",
        "with tf.name_scope('Projection_Layer'):\n",
        "  # get the actual embedding vectors from our batch inputs\n",
        "    x  = tf.nn.embedding_lookup(C, input_words) # (batch_size, window_size-1, emb_dim)\n",
        "    x  = tf.reshape(x, shape=(batch_size, (window_size - 1) * emb_dim))\n",
        "    \n",
        "with tf.name_scope('Hidden_Layer'):\n",
        "    Hx = tf.matmul(x, tf.transpose(H)) # (batch_size, hidden_size)\n",
        "    o  = tf.add(d, Hx) # (batch_size, hidden_size)\n",
        "    a  = tf.nn.tanh(o)  # (batch_size, hidden_size)\n",
        "     \n",
        "with tf.name_scope('Output_Layer'):\n",
        "    Ua = tf.matmul(a, tf.transpose(U)) # (batch_size, vocab_size)\n",
        "    Wx = tf.matmul(x, tf.transpose(W)) # (batch_size, vocab_size)\n",
        "    y  = tf.nn.softmax(tf.clip_by_value(tf.add(b, tf.add(Wx, Ua)), 0.0, 10)) # (batch_size, vocab_size)\n",
        "    #ppl = -1*tf.log(y)\n",
        "\n",
        "with tf.name_scope('Loss'):\n",
        "    onehot_tgt = tf.one_hot(tf.squeeze(output_word), vocab_size)  # (batch_size, vocab_size)\n",
        "    loss = -1 * tf.reduce_mean(tf.reduce_sum(tf.log(y) * onehot_tgt, 1)) # 乘 -1 -> maximize loss\n",
        "   \n",
        "with tf.name_scope('Perplexity'):\n",
        "    ppl = tf.math.exp(loss)\n",
        "    #print(ppl)\n",
        "    \n",
        "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss) \n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ0x6Zu7bYRe",
        "colab_type": "code",
        "outputId": "d185b37a-9808-4fcb-c992-dcf5c5e75d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "saver = tf.train.Saver()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMHizXWCJVZ-",
        "colab_type": "code",
        "outputId": "bde96b47-4c1c-406d-d28d-434065b7e1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)) as sess:\n",
        "    # initializes all of those variables we declared in earlier cells!\n",
        "    initializer = tf.global_variables_initializer()\n",
        "    initializer.run()\n",
        "    \n",
        "    step = 0\n",
        "    avg_loss_t = 0\n",
        "    avg_loss_v = 0\n",
        "    loss_t = []\n",
        "    loss_v = []\n",
        "    saver.restore(sess, \"/content/gdrive/My Drive/nnlm/final_model.ckpt\")\n",
        "    for epoch in range(epoch_size):\n",
        "        print('epoch no ', epoch)\n",
        "        save_path = saver.save(sess, \"/content/gdrive/My Drive/nnlm/final_model.ckpt\")\n",
        "\n",
        "        for x_batch, y_batch in next_batch(x_train, y_train, batch_size):\n",
        "            # if the batch is smaller than it's supposed to be (i.e. at end of vocab), skip it\n",
        "            # TODO: change this to account for num_batches, not batch_size\n",
        "\n",
        "            if len(x_batch) != batch_size:\n",
        "                continue\n",
        "            # give TF the data to use for all of the calcs in previous cells\n",
        "            feed_dict = {input_words: x_batch, output_word: y_batch}\n",
        "            # here we tell TF to return the loss to us \n",
        "            fetches = [loss, optimizer]\n",
        "            # where the magic happens \n",
        "            Loss, _ = sess.run(fetches, feed_dict)\n",
        "            avg_loss_t += Loss\n",
        "            #ppl = Perplexity\n",
        "            if step % 1000 == 0:\n",
        "                print('Step {}, Loss: {}'.format(step, avg_loss_t / 1000))\n",
        "                #print('Perplexity: {}'.format(ppl))\n",
        "\n",
        "                for valid_x, valid_y in next_batch(x_valid, y_valid, batch_size):\n",
        "                  if len(valid_x) != batch_size:\n",
        "                    continue\n",
        "                  feed_dict = {input_words: valid_x, output_word: valid_y}\n",
        "                  fetches = [loss, optimizer]\n",
        "                  Loss, _ = sess.run(fetches, feed_dict)\n",
        "                  avg_loss_v += Loss\n",
        "                print('Step {}, Loss: {}'.format(step, avg_loss_v / 1000))\n",
        "                loss_t.append(avg_loss_t)\n",
        "                loss_v.append(avg_loss_v)\n",
        "                avg_loss_t = 0\n",
        "                avg_loss_v = 0\n",
        "                \n",
        "            step += 1\n",
        "        \n",
        "    print('Training Done.')\n",
        "    word_embedding = C.eval()\n",
        "    # # TODO: this fails because it's a placeholder. Figure out how to visualize\n",
        "    # # y \n",
        "    # y_vals = y.eval()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/nnlm/final_model.ckpt\n",
            "epoch no  0\n",
            "Step 0, Loss: 0.009127057075500489\n",
            "Step 0, Loss: 16.13023182606697\n",
            "Step 1000, Loss: 6.744821178913116\n",
            "Step 1000, Loss: 16.163186735630035\n",
            "Step 2000, Loss: 6.475336982011795\n",
            "Step 2000, Loss: 16.16780705022812\n",
            "Step 3000, Loss: 6.645789207458496\n",
            "Step 3000, Loss: 16.165128007769585\n",
            "Step 4000, Loss: 6.6676059546470645\n",
            "Step 4000, Loss: 16.17021701157093\n",
            "Step 5000, Loss: 6.801721986055374\n",
            "Step 5000, Loss: 16.169664452433587\n",
            "Step 6000, Loss: 6.768621707916259\n",
            "Step 6000, Loss: 16.171452413678168\n",
            "Step 7000, Loss: 6.80507390332222\n",
            "Step 7000, Loss: 16.162838727116586\n",
            "Step 8000, Loss: 6.746948427677155\n",
            "Step 8000, Loss: 16.173994308948515\n",
            "Step 9000, Loss: 6.762878167629242\n",
            "Step 9000, Loss: 16.164661620140077\n",
            "Step 10000, Loss: 6.858805428504944\n",
            "Step 10000, Loss: 16.305911774396897\n",
            "Step 11000, Loss: 7.044774237632751\n",
            "Step 11000, Loss: 16.17952470970154\n",
            "Step 12000, Loss: 6.8330965828895565\n",
            "Step 12000, Loss: 16.160339549064638\n",
            "Step 13000, Loss: 6.553694458007812\n",
            "Step 13000, Loss: 16.154589764714242\n",
            "Step 14000, Loss: 6.8982737939357754\n",
            "Step 14000, Loss: 16.163033945918084\n",
            "Step 15000, Loss: 7.042601402282715\n",
            "Step 15000, Loss: 16.15521386563778\n",
            "Step 16000, Loss: 6.8878802103996275\n",
            "Step 16000, Loss: 16.157645325779914\n",
            "Step 17000, Loss: 6.721081310749054\n",
            "Step 17000, Loss: 16.164302348017692\n",
            "Step 18000, Loss: 6.860337968349457\n",
            "Step 18000, Loss: 16.156674344062804\n",
            "Step 19000, Loss: 6.893613390684128\n",
            "Step 19000, Loss: 16.158654071807863\n",
            "Step 20000, Loss: 6.772243626832962\n",
            "Step 20000, Loss: 16.154163561701775\n",
            "Step 21000, Loss: 6.9384261398315425\n",
            "Step 21000, Loss: 16.202108553290365\n",
            "Step 22000, Loss: 6.919728905439377\n",
            "Step 22000, Loss: 16.15060933196545\n",
            "Step 23000, Loss: 6.842321931362152\n",
            "Step 23000, Loss: 16.156825258255004\n",
            "Step 24000, Loss: 6.888046712636948\n",
            "Step 24000, Loss: 16.156617666244507\n",
            "Step 25000, Loss: 6.616916563749314\n",
            "Step 25000, Loss: 16.158445492982864\n",
            "Step 26000, Loss: 6.751133259534836\n",
            "Step 26000, Loss: 16.156183920264244\n",
            "Step 27000, Loss: 6.799762046813965\n",
            "Step 27000, Loss: 16.153536872148514\n",
            "Step 28000, Loss: 6.775058926582337\n",
            "Step 28000, Loss: 16.149905459165574\n",
            "Step 29000, Loss: 6.925055918216706\n",
            "Step 29000, Loss: 16.151950583219527\n",
            "Step 30000, Loss: 6.893237127780914\n",
            "Step 30000, Loss: 16.14565052473545\n",
            "Step 31000, Loss: 6.8123721377849575\n",
            "Step 31000, Loss: 16.14612812960148\n",
            "Step 32000, Loss: 6.866800844430924\n",
            "Step 32000, Loss: 16.148076265096666\n",
            "Step 33000, Loss: 6.690680770993232\n",
            "Step 33000, Loss: 16.142416964054107\n",
            "Step 34000, Loss: 6.441647352933884\n",
            "Step 34000, Loss: 16.13970650076866\n",
            "Step 35000, Loss: 7.07875941991806\n",
            "Step 35000, Loss: 16.147702912449837\n",
            "Step 36000, Loss: 7.061849413394928\n",
            "Step 36000, Loss: 16.14885730457306\n",
            "Step 37000, Loss: 7.05800648522377\n",
            "Step 37000, Loss: 16.14433012199402\n",
            "Step 38000, Loss: 7.069974600553513\n",
            "Step 38000, Loss: 16.144428416967394\n",
            "Step 39000, Loss: 6.743372505187988\n",
            "Step 39000, Loss: 16.124622272372246\n",
            "Step 40000, Loss: 6.6016792566776274\n",
            "Step 40000, Loss: 16.130907248020172\n",
            "Step 41000, Loss: 6.192169029712677\n",
            "Step 41000, Loss: 16.127206131696703\n",
            "Step 42000, Loss: 6.441242828130722\n",
            "Step 42000, Loss: 16.124505843997003\n",
            "Step 43000, Loss: 6.013599536180497\n",
            "Step 43000, Loss: 16.122148738265036\n",
            "Step 44000, Loss: 6.079301628351211\n",
            "Step 44000, Loss: 16.124746761083603\n",
            "epoch no  1\n",
            "Step 45000, Loss: 6.697922102212906\n",
            "Step 45000, Loss: 16.121002234339713\n",
            "Step 46000, Loss: 6.481347839593887\n",
            "Step 46000, Loss: 16.114181014060975\n",
            "Step 47000, Loss: 6.5960907652378085\n",
            "Step 47000, Loss: 16.11391811835766\n",
            "Step 48000, Loss: 6.628808364868164\n",
            "Step 48000, Loss: 16.1154876986742\n",
            "Step 49000, Loss: 6.779723555326462\n",
            "Step 49000, Loss: 16.117420941233636\n",
            "Step 50000, Loss: 6.765827137470246\n",
            "Step 50000, Loss: 16.108966046214103\n",
            "Step 51000, Loss: 6.769183081388474\n",
            "Step 51000, Loss: 16.108066057920457\n",
            "Step 52000, Loss: 6.723939723968506\n",
            "Step 52000, Loss: 16.109144565463065\n",
            "Step 53000, Loss: 6.780582386732101\n",
            "Step 53000, Loss: 16.103402595639228\n",
            "Step 54000, Loss: 6.787594652414322\n",
            "Step 54000, Loss: 16.10378213107586\n",
            "Step 55000, Loss: 6.995951647043229\n",
            "Step 55000, Loss: 16.109833483695983\n",
            "Step 56000, Loss: 6.799699215650558\n",
            "Step 56000, Loss: 16.1012762029171\n",
            "Step 57000, Loss: 6.526877851724625\n",
            "Step 57000, Loss: 16.098246416330337\n",
            "Step 58000, Loss: 6.852372839212418\n",
            "Step 58000, Loss: 16.09542694270611\n",
            "Step 59000, Loss: 7.0179859113693235\n",
            "Step 59000, Loss: 16.091619235754013\n",
            "Step 60000, Loss: 6.895607172250748\n",
            "Step 60000, Loss: 16.090925365447998\n",
            "Step 61000, Loss: 6.666436272382736\n",
            "Step 61000, Loss: 16.09658956480026\n",
            "Step 62000, Loss: 6.824426439523697\n",
            "Step 62000, Loss: 16.096522604942322\n",
            "Step 63000, Loss: 6.872827633142471\n",
            "Step 63000, Loss: 16.100784922838212\n",
            "Step 64000, Loss: 6.7504435338974\n",
            "Step 64000, Loss: 16.095645215392114\n",
            "Step 65000, Loss: 6.9027213191986085\n",
            "Step 65000, Loss: 16.096293716430665\n",
            "Step 66000, Loss: 6.890957697153091\n",
            "Step 66000, Loss: 16.09323096013069\n",
            "Step 67000, Loss: 6.820834841251373\n",
            "Step 67000, Loss: 16.096483297109604\n",
            "Step 68000, Loss: 6.868846523046494\n",
            "Step 68000, Loss: 16.095834812164306\n",
            "Step 69000, Loss: 6.584285489082337\n",
            "Step 69000, Loss: 16.0919790494442\n",
            "Step 70000, Loss: 6.721174310684204\n",
            "Step 70000, Loss: 16.0954843968153\n",
            "Step 71000, Loss: 6.750300021886826\n",
            "Step 71000, Loss: 16.09249353969097\n",
            "Step 72000, Loss: 6.7509582688808445\n",
            "Step 72000, Loss: 16.087640442967416\n",
            "Step 73000, Loss: 6.881778697252273\n",
            "Step 73000, Loss: 16.093636749625205\n",
            "Step 74000, Loss: 6.887635102748871\n",
            "Step 74000, Loss: 16.084249754667283\n",
            "Step 75000, Loss: 6.800113067388534\n",
            "Step 75000, Loss: 16.09171196269989\n",
            "Step 76000, Loss: 6.83856026673317\n",
            "Step 76000, Loss: 16.093635999560355\n",
            "Step 77000, Loss: 6.657473781108856\n",
            "Step 77000, Loss: 16.077831627130507\n",
            "Step 78000, Loss: 6.357605377316475\n",
            "Step 78000, Loss: 16.084065482735635\n",
            "Step 79000, Loss: 7.079830363750458\n",
            "Step 79000, Loss: 16.091415128827094\n",
            "Step 80000, Loss: 7.02070930147171\n",
            "Step 80000, Loss: 16.091120890617372\n",
            "Step 81000, Loss: 7.029731068849563\n",
            "Step 81000, Loss: 16.08975021696091\n",
            "Step 82000, Loss: 7.048528717041016\n",
            "Step 82000, Loss: 16.08579395198822\n",
            "Step 83000, Loss: 6.739070795059204\n",
            "Step 83000, Loss: 16.075440711140633\n",
            "Step 84000, Loss: 6.563752785682678\n",
            "Step 84000, Loss: 16.075948039650918\n",
            "Step 85000, Loss: 6.188915940284729\n",
            "Step 85000, Loss: 16.07213127720356\n",
            "Step 86000, Loss: 6.391122798204422\n",
            "Step 86000, Loss: 16.071557644724844\n",
            "Step 87000, Loss: 5.963743068933487\n",
            "Step 87000, Loss: 16.072567569971085\n",
            "Step 88000, Loss: 6.092812737464905\n",
            "Step 88000, Loss: 16.069717485904693\n",
            "epoch no  2\n",
            "Step 89000, Loss: 6.671151614904404\n",
            "Step 89000, Loss: 16.070459502339364\n",
            "Step 90000, Loss: 6.45797102355957\n",
            "Step 90000, Loss: 16.06087487053871\n",
            "Step 91000, Loss: 6.5557404532432555\n",
            "Step 91000, Loss: 16.06017788076401\n",
            "Step 92000, Loss: 6.593702584981918\n",
            "Step 92000, Loss: 16.06287095439434\n",
            "Step 93000, Loss: 6.740938929319381\n",
            "Step 93000, Loss: 16.06078374159336\n",
            "Step 94000, Loss: 6.756060312986374\n",
            "Step 94000, Loss: 16.061015745162965\n",
            "Step 95000, Loss: 6.734798509836197\n",
            "Step 95000, Loss: 16.062342395305635\n",
            "Step 96000, Loss: 6.71365609908104\n",
            "Step 96000, Loss: 16.061133784890174\n",
            "Step 97000, Loss: 6.743299967765808\n",
            "Step 97000, Loss: 16.055559084177016\n",
            "Step 98000, Loss: 6.744662841558457\n",
            "Step 98000, Loss: 16.060063952684402\n",
            "Step 99000, Loss: 6.973121821403503\n",
            "Step 99000, Loss: 16.064015428185463\n",
            "Step 100000, Loss: 6.784800794124603\n",
            "Step 100000, Loss: 16.052685926198958\n",
            "Step 101000, Loss: 6.512414669275284\n",
            "Step 101000, Loss: 16.052363456249235\n",
            "Step 102000, Loss: 6.78547322845459\n",
            "Step 102000, Loss: 16.05271261000633\n",
            "Step 103000, Loss: 7.015988809108734\n",
            "Step 103000, Loss: 16.052808673858642\n",
            "Step 104000, Loss: 6.872039162874222\n",
            "Step 104000, Loss: 16.05654995536804\n",
            "Step 105000, Loss: 6.628834412336349\n",
            "Step 105000, Loss: 16.05401385307312\n",
            "Step 106000, Loss: 6.8438001050949095\n",
            "Step 106000, Loss: 16.0486126537323\n",
            "Step 107000, Loss: 6.834986969947815\n",
            "Step 107000, Loss: 16.05362851881981\n",
            "Step 108000, Loss: 6.725160320997238\n",
            "Step 108000, Loss: 16.053057755589485\n",
            "Step 109000, Loss: 6.889819779157639\n",
            "Step 109000, Loss: 16.04864948129654\n",
            "Step 110000, Loss: 6.820652977228165\n",
            "Step 110000, Loss: 16.049691541671752\n",
            "Step 111000, Loss: 6.83341513967514\n",
            "Step 111000, Loss: 16.05150683295727\n",
            "Step 112000, Loss: 6.855229776382446\n",
            "Step 112000, Loss: 16.051612041711806\n",
            "Step 113000, Loss: 6.52164104294777\n",
            "Step 113000, Loss: 16.050819633483886\n",
            "Step 114000, Loss: 6.714142524719239\n",
            "Step 114000, Loss: 16.050747715473175\n",
            "Step 115000, Loss: 6.7329530954360965\n",
            "Step 115000, Loss: 16.052725074887277\n",
            "Step 116000, Loss: 6.72295255112648\n",
            "Step 116000, Loss: 16.047465819954873\n",
            "Step 117000, Loss: 6.8442863178253175\n",
            "Step 117000, Loss: 16.043834463834763\n",
            "Step 118000, Loss: 6.9060577964782714\n",
            "Step 118000, Loss: 16.04460899066925\n",
            "Step 119000, Loss: 6.750563071727752\n",
            "Step 119000, Loss: 16.053213411569594\n",
            "Step 120000, Loss: 6.824341821670532\n",
            "Step 120000, Loss: 16.044415076732637\n",
            "Step 121000, Loss: 6.644232612133026\n",
            "Step 121000, Loss: 16.040895393133162\n",
            "Step 122000, Loss: 6.280254372835159\n",
            "Step 122000, Loss: 16.044533410072326\n",
            "Step 123000, Loss: 7.084113224029541\n",
            "Step 123000, Loss: 16.042707537770273\n",
            "Step 124000, Loss: 6.978575005531311\n",
            "Step 124000, Loss: 16.045595075130464\n",
            "Step 125000, Loss: 7.009683002471924\n",
            "Step 125000, Loss: 16.045614695191382\n",
            "Step 126000, Loss: 7.022616841316223\n",
            "Step 126000, Loss: 16.039925057172777\n",
            "Step 127000, Loss: 6.712069801568985\n",
            "Step 127000, Loss: 16.037927428603172\n",
            "Step 128000, Loss: 6.563610214948654\n",
            "Step 128000, Loss: 16.036384551882744\n",
            "Step 129000, Loss: 6.1678480324745175\n",
            "Step 129000, Loss: 16.037840776205062\n",
            "Step 130000, Loss: 6.363668731212616\n",
            "Step 130000, Loss: 16.032775921344758\n",
            "Step 131000, Loss: 6.002836913347244\n",
            "Step 131000, Loss: 16.031619774103163\n",
            "Step 132000, Loss: 6.034895980119705\n",
            "Step 132000, Loss: 16.034155844449998\n",
            "epoch no  3\n",
            "Step 133000, Loss: 6.648936980009079\n",
            "Step 133000, Loss: 16.03005150437355\n",
            "Step 134000, Loss: 6.414387779474258\n",
            "Step 134000, Loss: 16.026686318159104\n",
            "Step 135000, Loss: 6.531021302700043\n",
            "Step 135000, Loss: 16.02507624745369\n",
            "Step 136000, Loss: 6.557195814847947\n",
            "Step 136000, Loss: 16.025085769057274\n",
            "Step 137000, Loss: 6.71520154595375\n",
            "Step 137000, Loss: 16.029436965823173\n",
            "Step 138000, Loss: 6.770644432544708\n",
            "Step 138000, Loss: 16.025263381004333\n",
            "Step 139000, Loss: 6.70441325378418\n",
            "Step 139000, Loss: 16.025146357297896\n",
            "Step 140000, Loss: 6.7293533051013945\n",
            "Step 140000, Loss: 16.027930151820183\n",
            "Step 141000, Loss: 6.672041759252548\n",
            "Step 141000, Loss: 16.021207624197007\n",
            "Step 142000, Loss: 6.724063910961151\n",
            "Step 142000, Loss: 16.01849143469334\n",
            "Step 143000, Loss: 6.947754840612411\n",
            "Step 143000, Loss: 16.02774077653885\n",
            "Step 144000, Loss: 6.796472651481628\n",
            "Step 144000, Loss: 16.019820779800416\n",
            "Step 145000, Loss: 6.481841245174408\n",
            "Step 145000, Loss: 16.01637574696541\n",
            "Step 146000, Loss: 6.7517837986946105\n",
            "Step 146000, Loss: 16.014685258626937\n",
            "Step 147000, Loss: 7.015056830406189\n",
            "Step 147000, Loss: 16.013251168608665\n",
            "Step 148000, Loss: 6.815626687526703\n",
            "Step 148000, Loss: 16.018941215753554\n",
            "Step 149000, Loss: 6.621806663274765\n",
            "Step 149000, Loss: 16.01730632984638\n",
            "Step 150000, Loss: 6.816778583049774\n",
            "Step 150000, Loss: 16.015276500225067\n",
            "Step 151000, Loss: 6.806965454816818\n",
            "Step 151000, Loss: 16.018336629271506\n",
            "Step 152000, Loss: 6.72992920923233\n",
            "Step 152000, Loss: 16.017167365550996\n",
            "Step 153000, Loss: 6.87248551774025\n",
            "Step 153000, Loss: 16.013717274188995\n",
            "Step 154000, Loss: 6.783387737035751\n",
            "Step 154000, Loss: 16.01271137201786\n",
            "Step 155000, Loss: 6.825676441669464\n",
            "Step 155000, Loss: 16.01764296579361\n",
            "Step 156000, Loss: 6.834948259353638\n",
            "Step 156000, Loss: 16.019029785394668\n",
            "Step 157000, Loss: 6.475162862062454\n",
            "Step 157000, Loss: 16.02938419806957\n",
            "Step 158000, Loss: 6.708583382844925\n",
            "Step 158000, Loss: 16.017160566806794\n",
            "Step 159000, Loss: 6.71368914437294\n",
            "Step 159000, Loss: 16.017201717615126\n",
            "Step 160000, Loss: 6.722210940361023\n",
            "Step 160000, Loss: 16.01268754708767\n",
            "Step 161000, Loss: 6.788231993675232\n",
            "Step 161000, Loss: 16.012651782393455\n",
            "Step 162000, Loss: 6.919217846632003\n",
            "Step 162000, Loss: 16.013677317619322\n",
            "Step 163000, Loss: 6.722477953433991\n",
            "Step 163000, Loss: 16.01575660967827\n",
            "Step 164000, Loss: 6.813902660131454\n",
            "Step 164000, Loss: 16.01013624417782\n",
            "Step 165000, Loss: 6.606398172020912\n",
            "Step 165000, Loss: 16.00675087571144\n",
            "Step 166000, Loss: 6.280734083652496\n",
            "Step 166000, Loss: 16.009888814210893\n",
            "Step 167000, Loss: 7.041840831041336\n",
            "Step 167000, Loss: 16.00393685913086\n",
            "Step 168000, Loss: 6.9606944816112515\n",
            "Step 168000, Loss: 16.01281196320057\n",
            "Step 169000, Loss: 6.984607413291931\n",
            "Step 169000, Loss: 16.010690670847893\n",
            "Step 170000, Loss: 7.00900390625\n",
            "Step 170000, Loss: 16.008144561052323\n",
            "Step 171000, Loss: 6.697798437833786\n",
            "Step 171000, Loss: 16.00228413414955\n",
            "Step 172000, Loss: 6.557504070520401\n",
            "Step 172000, Loss: 16.000920884609222\n",
            "Step 173000, Loss: 6.164083545923233\n",
            "Step 173000, Loss: 16.00165698981285\n",
            "Step 174000, Loss: 6.333911513566971\n",
            "Step 174000, Loss: 16.001602434039118\n",
            "Step 175000, Loss: 6.029278679847717\n",
            "Step 175000, Loss: 15.996438577890396\n",
            "Step 176000, Loss: 6.0360720262527465\n",
            "Step 176000, Loss: 16.001009156465532\n",
            "epoch no  4\n",
            "Step 177000, Loss: 6.585738531827927\n",
            "Step 177000, Loss: 16.003587265014648\n",
            "Step 178000, Loss: 6.410812156677246\n",
            "Step 178000, Loss: 15.996177107334137\n",
            "Step 179000, Loss: 6.490904349565506\n",
            "Step 179000, Loss: 15.99518787908554\n",
            "Step 180000, Loss: 6.526783092260361\n",
            "Step 180000, Loss: 15.991152102947234\n",
            "Step 181000, Loss: 6.687251528024674\n",
            "Step 181000, Loss: 15.993471138596535\n",
            "Step 182000, Loss: 6.766565435886383\n",
            "Step 182000, Loss: 15.990707085013389\n",
            "Step 183000, Loss: 6.687321691274643\n",
            "Step 183000, Loss: 15.989687497377396\n",
            "Step 184000, Loss: 6.7176298003196715\n",
            "Step 184000, Loss: 15.993037294745445\n",
            "Step 185000, Loss: 6.6664772148132325\n",
            "Step 185000, Loss: 15.987174714326859\n",
            "Step 186000, Loss: 6.699667167186737\n",
            "Step 186000, Loss: 15.985767535090446\n",
            "Step 187000, Loss: 6.924761344909668\n",
            "Step 187000, Loss: 15.997650789618492\n",
            "Step 188000, Loss: 6.772055655956268\n",
            "Step 188000, Loss: 15.995345837593078\n",
            "Step 189000, Loss: 6.467146535396576\n",
            "Step 189000, Loss: 15.98403384923935\n",
            "Step 190000, Loss: 6.759488762140274\n",
            "Step 190000, Loss: 15.989276233911514\n",
            "Step 191000, Loss: 6.981144625902176\n",
            "Step 191000, Loss: 15.982167179107666\n",
            "Step 192000, Loss: 6.8091284914016725\n",
            "Step 192000, Loss: 15.988539369106293\n",
            "Step 193000, Loss: 6.601659028291702\n",
            "Step 193000, Loss: 15.986849567890168\n",
            "Step 194000, Loss: 6.800252060174942\n",
            "Step 194000, Loss: 15.989145347118377\n",
            "Step 195000, Loss: 6.795535223007202\n",
            "Step 195000, Loss: 15.985096307635308\n",
            "Step 196000, Loss: 6.720899252653122\n",
            "Step 196000, Loss: 15.991365555763245\n",
            "Step 197000, Loss: 6.882556327581406\n",
            "Step 197000, Loss: 15.98773278951645\n",
            "Step 198000, Loss: 6.7492817902565\n",
            "Step 198000, Loss: 15.985284638881684\n",
            "Step 199000, Loss: 6.797922166109085\n",
            "Step 199000, Loss: 15.990196566104888\n",
            "Step 200000, Loss: 6.826573106527328\n",
            "Step 200000, Loss: 15.994405659437179\n",
            "Step 201000, Loss: 6.470779239177704\n",
            "Step 201000, Loss: 17.832402572989462\n",
            "Step 202000, Loss: 6.696923466444016\n",
            "Step 202000, Loss: 16.03586097609997\n",
            "Step 203000, Loss: 6.714082396745682\n",
            "Step 203000, Loss: 16.001774909853935\n",
            "Step 204000, Loss: 6.685802728891373\n",
            "Step 204000, Loss: 16.036372676491737\n",
            "Step 205000, Loss: 6.770817696332932\n",
            "Step 205000, Loss: 16.009757869005202\n",
            "Step 206000, Loss: 6.9380517964363095\n",
            "Step 206000, Loss: 15.98957560133934\n",
            "Step 207000, Loss: 6.691183189153671\n",
            "Step 207000, Loss: 15.989127887248992\n",
            "Step 208000, Loss: 6.8113398971557615\n",
            "Step 208000, Loss: 15.985038184046745\n",
            "Step 209000, Loss: 6.597427783966064\n",
            "Step 209000, Loss: 15.990529731869698\n",
            "Step 210000, Loss: 6.280699362158775\n",
            "Step 210000, Loss: 15.985435378313065\n",
            "Step 211000, Loss: 7.003317018270493\n",
            "Step 211000, Loss: 15.987647527456284\n",
            "Step 212000, Loss: 6.9589247546195985\n",
            "Step 212000, Loss: 15.989799867153168\n",
            "Step 213000, Loss: 6.978782762765884\n",
            "Step 213000, Loss: 15.991355871081351\n",
            "Step 214000, Loss: 6.984532707452774\n",
            "Step 214000, Loss: 15.984870304703712\n",
            "Step 215000, Loss: 6.70373477768898\n",
            "Step 215000, Loss: 15.981498412847518\n",
            "Step 216000, Loss: 6.536188434839248\n",
            "Step 216000, Loss: 15.979044585108756\n",
            "Step 217000, Loss: 6.188851178407669\n",
            "Step 217000, Loss: 15.982795521855355\n",
            "Step 218000, Loss: 6.291593604803086\n",
            "Step 218000, Loss: 15.982178669214248\n",
            "Step 219000, Loss: 6.077683576345444\n",
            "Step 219000, Loss: 15.978770775556564\n",
            "Step 220000, Loss: 6.003863304615021\n",
            "Step 220000, Loss: 15.976775935053825\n",
            "epoch no  5\n",
            "Step 221000, Loss: 6.517577291488648\n",
            "Step 221000, Loss: 15.975744354367256\n",
            "Step 222000, Loss: 6.436473780870438\n",
            "Step 222000, Loss: 15.970501858115195\n",
            "Step 223000, Loss: 6.440497678279876\n",
            "Step 223000, Loss: 15.964222782731056\n",
            "Step 224000, Loss: 6.505139260768891\n",
            "Step 224000, Loss: 15.964497090101242\n",
            "Step 225000, Loss: 6.689945187568664\n",
            "Step 225000, Loss: 15.964312152862549\n",
            "Step 226000, Loss: 6.787141108512879\n",
            "Step 226000, Loss: 15.964393918275833\n",
            "Step 227000, Loss: 6.645127472877502\n",
            "Step 227000, Loss: 15.964954815387726\n",
            "Step 228000, Loss: 6.710768327713013\n",
            "Step 228000, Loss: 15.967956517457962\n",
            "Step 229000, Loss: 6.663162522554398\n",
            "Step 229000, Loss: 15.96328267788887\n",
            "Step 230000, Loss: 6.656252095937729\n",
            "Step 230000, Loss: 15.960316590070725\n",
            "Step 231000, Loss: 6.914050322771073\n",
            "Step 231000, Loss: 15.971261738538741\n",
            "Step 232000, Loss: 6.766259086370468\n",
            "Step 232000, Loss: 15.963847126364708\n",
            "Step 233000, Loss: 6.455010948657989\n",
            "Step 233000, Loss: 15.960658897876739\n",
            "Step 234000, Loss: 6.737363992452622\n",
            "Step 234000, Loss: 15.961906265377998\n",
            "Step 235000, Loss: 6.9624854259490965\n",
            "Step 235000, Loss: 15.956236073613168\n",
            "Step 236000, Loss: 6.811533602952957\n",
            "Step 236000, Loss: 15.958746582627297\n",
            "Step 237000, Loss: 6.598102756738663\n",
            "Step 237000, Loss: 15.955960584402085\n",
            "Step 238000, Loss: 6.77233203291893\n",
            "Step 238000, Loss: 15.958236710071564\n",
            "Step 239000, Loss: 6.803290545225144\n",
            "Step 239000, Loss: 15.956814769268036\n",
            "Step 240000, Loss: 6.71191567659378\n",
            "Step 240000, Loss: 15.958100385785103\n",
            "Step 241000, Loss: 6.854620830535889\n",
            "Step 241000, Loss: 15.95935555624962\n",
            "Step 242000, Loss: 6.745950158596039\n",
            "Step 242000, Loss: 15.958377338409424\n",
            "Step 243000, Loss: 6.760974194288254\n",
            "Step 243000, Loss: 15.960889111757279\n",
            "Step 244000, Loss: 6.844266290426254\n",
            "Step 244000, Loss: 15.960629304409027\n",
            "Step 245000, Loss: 6.463621323108673\n",
            "Step 245000, Loss: 15.962607585072517\n",
            "Step 246000, Loss: 6.664986692905426\n",
            "Step 246000, Loss: 15.959282398819923\n",
            "Step 247000, Loss: 6.729065145015716\n",
            "Step 247000, Loss: 15.963138146519661\n",
            "Step 248000, Loss: 6.647076847553254\n",
            "Step 248000, Loss: 15.96011492908001\n",
            "Step 249000, Loss: 6.7541830940246586\n",
            "Step 249000, Loss: 15.960729241251945\n",
            "Step 250000, Loss: 6.922833335161209\n",
            "Step 250000, Loss: 15.956832128047942\n",
            "Step 251000, Loss: 6.643661636352539\n",
            "Step 251000, Loss: 15.956641966938973\n",
            "Step 252000, Loss: 6.809863831281662\n",
            "Step 252000, Loss: 15.95607038462162\n",
            "Step 253000, Loss: 6.595623045921326\n",
            "Step 253000, Loss: 15.95419982767105\n",
            "Step 254000, Loss: 6.265701488733292\n",
            "Step 254000, Loss: 15.959381312251091\n",
            "Step 255000, Loss: 6.932261352062225\n",
            "Step 255000, Loss: 15.95696606838703\n",
            "Step 256000, Loss: 6.923647157669067\n",
            "Step 256000, Loss: 15.962937011003493\n",
            "Step 257000, Loss: 6.962411343336106\n",
            "Step 257000, Loss: 15.958475235700607\n",
            "Step 258000, Loss: 6.95963455247879\n",
            "Step 258000, Loss: 15.958001039028169\n",
            "Step 259000, Loss: 6.701991705179214\n",
            "Step 259000, Loss: 15.954366350054741\n",
            "Step 260000, Loss: 6.527023343801498\n",
            "Step 260000, Loss: 15.949393442511559\n",
            "Step 261000, Loss: 6.183982115030289\n",
            "Step 261000, Loss: 15.950902085185051\n",
            "Step 262000, Loss: 6.268003468751908\n",
            "Step 262000, Loss: 15.950861212253571\n",
            "Step 263000, Loss: 6.097019207715988\n",
            "Step 263000, Loss: 15.950882026791573\n",
            "Step 264000, Loss: 5.963129004955292\n",
            "Step 264000, Loss: 15.948391573309898\n",
            "epoch no  6\n",
            "Step 265000, Loss: 6.459932181835175\n",
            "Step 265000, Loss: 15.952396379828453\n",
            "Step 266000, Loss: 6.457163370132446\n",
            "Step 266000, Loss: 15.9460532476902\n",
            "Step 267000, Loss: 6.418330614566803\n",
            "Step 267000, Loss: 15.947276700139046\n",
            "Step 268000, Loss: 6.461322633266449\n",
            "Step 268000, Loss: 15.943599108576775\n",
            "Step 269000, Loss: 6.690281949281692\n",
            "Step 269000, Loss: 15.941381591320038\n",
            "Step 270000, Loss: 6.749712594509125\n",
            "Step 270000, Loss: 15.945203716874122\n",
            "Step 271000, Loss: 6.614397808074951\n",
            "Step 271000, Loss: 15.947224163770676\n",
            "Step 272000, Loss: 6.713658164024353\n",
            "Step 272000, Loss: 15.947178170323372\n",
            "Step 273000, Loss: 6.636438102722168\n",
            "Step 273000, Loss: 15.9376971231699\n",
            "Step 274000, Loss: 6.6177515444755555\n",
            "Step 274000, Loss: 15.933412637352943\n",
            "Step 275000, Loss: 6.8991825234889985\n",
            "Step 275000, Loss: 15.945049171328545\n",
            "Step 276000, Loss: 6.755238659143448\n",
            "Step 276000, Loss: 15.937573165416717\n",
            "Step 277000, Loss: 6.449065610647201\n",
            "Step 277000, Loss: 15.935831934928894\n",
            "Step 278000, Loss: 6.720100871562957\n",
            "Step 278000, Loss: 15.935888792157174\n",
            "Step 279000, Loss: 6.951576016187667\n",
            "Step 279000, Loss: 15.94014876782894\n",
            "Step 280000, Loss: 6.795405672073365\n",
            "Step 280000, Loss: 15.939041873455048\n",
            "Step 281000, Loss: 6.57523317861557\n",
            "Step 281000, Loss: 15.940990527153016\n",
            "Step 282000, Loss: 6.754282874584198\n",
            "Step 282000, Loss: 15.939506592512132\n",
            "Step 283000, Loss: 6.792248503446579\n",
            "Step 283000, Loss: 15.9394994225502\n",
            "Step 284000, Loss: 6.701226052045822\n",
            "Step 284000, Loss: 15.939259309768676\n",
            "Step 285000, Loss: 6.824282984733582\n",
            "Step 285000, Loss: 15.940982742786407\n",
            "Step 286000, Loss: 6.752243297100067\n",
            "Step 286000, Loss: 15.93889607822895\n",
            "Step 287000, Loss: 6.71858322930336\n",
            "Step 287000, Loss: 15.94248873770237\n",
            "Step 288000, Loss: 6.852475367069244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEY_7Zk8SIxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: # y are our logits from the Bengio equation. those logits we must then convert to pseudo probabilities, then normalize via softmax to produce our y_hat.\n",
        "#       y_hat should be (vocabulary length) X (1) in size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt9utADwkSL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_t = np.array(loss_t)\n",
        "plt.plot(range(len(loss_t)), np.exp(loss_t/1000))\n",
        "loss_v = np.array(loss_v)\n",
        "plt.plot(range(len(loss_v)), np.exp(loss_v/1000))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeabkDBJiS3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_t = 'brown_test.txt'\n",
        "x_raw_t, y_raw_t, vocab_t, word2id_t = load_zh(filepath_t, window_size, vocab_size)\n",
        "x_test, y_test = convert_to_id(x_raw_t, y_raw_t, vocab_t)\n",
        "print('Length: {}'.format(len(x_test)))\n",
        "print('Number of batch: {}'.format(len(x_test) / batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riw3EQg5qt3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_loss_test = 0\n",
        "loss_test = []\n",
        "ctr = 0\n",
        "for test_x, test_y in next_batch(x_test, y_test, batch_size):\n",
        "  if len(test_x) != batch_size:\n",
        "      continue\n",
        "  feed_dict = {input_words: test_x, output_word: test_y}\n",
        "  fetches = [loss, optimizer]\n",
        "  # where the magic happens \n",
        "  Loss, _ = sess.run(fetches, feed_dict)\n",
        "  avg_loss_test += Loss\n",
        "  ctr += 1\n",
        "\n",
        "  if step % 1000 == 0:\n",
        "    loss_test.append(avg_loss_test)\n",
        "    avg_loss_test = 0\n",
        "\n",
        "#print(np.exp(avg_loss_test/ctr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld6NjpeQscOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_test = np.array(loss_test)\n",
        "plt.plot(range(len(loss_test)), np.exp(loss_test/1000))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}